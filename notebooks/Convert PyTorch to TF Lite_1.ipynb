{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Hardswish(nn.Module):  # alternative to nn.Hardswish() for export\n",
    "#     @staticmethod\n",
    "#     def forward(x):\n",
    "#         # return x * F.hardsigmoid(x)\n",
    "#         return x * F.hardtanh(x + 3, 0., 6.) / 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Net, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, params):\n",
    "    super(Net, self).__init__()\n",
    "    \n",
    "    C_in,H_in,W_in=params[\"input_shape\"] \n",
    "    self.dropout_rate=params[\"drop_rate\"] \n",
    "    self.num_fc1 = params[\"num_fc1\"]\n",
    "\n",
    "    mobile_v3 =  timm.create_model('mobilenetv3_large_100', pretrained=True, scriptable=True)\n",
    "    \n",
    "    in_features = mobile_v3.classifier.in_features\n",
    "    \n",
    "    mobile_v3.classifier = nn.Linear(in_features, self.num_fc1)\n",
    "    \n",
    "    self.backbone = nn.Sequential(\n",
    "                    mobile_v3,\n",
    "                    nn.BatchNorm1d(num_features=self.num_fc1),\n",
    "                    nn.ReLU()\n",
    "    )\n",
    "    \n",
    "    self.gender_head = nn.Sequential(\n",
    "                            nn.Dropout(p=self.dropout_rate),\n",
    "                            nn.Linear(self.num_fc1, 1),\n",
    "                            nn.Sigmoid()\n",
    "    )\n",
    "    \n",
    "    self.age_head = nn.Sequential(\n",
    "                    nn.Linear(self.num_fc1, 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x = self.backbone(x)  \n",
    "    \n",
    "    gender = self.gender_head(x)\n",
    "    age = self.age_head(x)\n",
    "    \n",
    "    return age, gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "Net.__init__= __init__\n",
    "Net.forward = forward\n",
    "\n",
    "params_model={\n",
    "        \"input_shape\": (3,224,224),\n",
    "        \"drop_rate\" : 0.2,\n",
    "        \"num_fc1\" : 512,\n",
    "    }\n",
    "\n",
    "model = Net(params_model)\n",
    "\n",
    "# def replace_nnHar_to_SiLU(model):\n",
    "#     for child_name, child in model.named_children():\n",
    "#         if isinstance(child, nn.SiLU):\n",
    "#             setattr(model, child_name, SiLU())\n",
    "#         else:\n",
    "#             replace_nnSiLU_to_SiLU(child)\n",
    "            \n",
    "# replace_nnSiLU_to_SiLU(model)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('../models/exp_2020_12_09-09_45_03/best_checkpoints.bin')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model, torch.nn.DataParallel):\n",
    "        model = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (backbone): Sequential(\n",
       "    (0): MobileNetV3(\n",
       "      (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act1): HardSwishJit()\n",
       "      (blocks): Sequential(\n",
       "        (0): Sequential(\n",
       "          (0): DepthwiseSeparableConv(\n",
       "            (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): Identity()\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_dw): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): ReLU(inplace=True)\n",
       "            (conv_pwl): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_dw): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "            (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): ReLU(inplace=True)\n",
       "            (conv_pwl): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_dw): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "            (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): ReLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_dw): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): ReLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): ReLU(inplace=True)\n",
       "            (conv_dw): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (bn2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): ReLU(inplace=True)\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "            (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (conv_pwl): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (bn2): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (conv_pwl): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (3): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (bn2): BatchNorm2d(184, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (conv_pwl): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (4): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): InvertedResidual(\n",
       "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): InvertedResidual(\n",
       "            (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): InvertedResidual(\n",
       "            (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "            (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act2): HardSwishJit()\n",
       "            (se): SqueezeExcite(\n",
       "              (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): ConvBnAct(\n",
       "            (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (act1): HardSwishJit()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=False)\n",
       "      (conv_head): Conv2d(960, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (act2): HardSwishJit()\n",
       "      (classifier): Linear(in_features=1280, out_features=512, bias=True)\n",
       "    )\n",
       "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (gender_head): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       "  (age_head): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ONNX export failed: Couldn't export operator aten::flatten\n\nDefined at:\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/mobilenetv3.py(141): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/container.py(117): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(725): _call_impl\n<ipython-input-14-b8c514028c28>(2): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/jit/_trace.py(116): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/jit/_trace.py(125): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(727): _call_impl\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/jit/_trace.py(1148): _get_trace_graph\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(342): _trace_and_get_graph_from_model\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(379): _create_jit_graph\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(409): _model_to_graph\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(632): _export\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(85): export\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/__init__.py(225): export\n<ipython-input-21-f07c5c3db454>(10): <module>\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3418): run_code\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3338): run_ast_nodes\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3146): run_cell_async\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2923): _run_cell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2877): run_cell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/zmqshell.py(536): run_cell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/ipkernel.py(306): do_execute\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(209): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelbase.py(543): execute_request\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(209): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelbase.py(268): dispatch_shell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(209): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelbase.py(365): process_one\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(748): run\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(787): inner\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/ioloop.py(743): _run_callback\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/ioloop.py(690): <lambda>\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/asyncio/events.py(81): _run\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/asyncio/base_events.py(1859): _run_once\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/asyncio/base_events.py(570): run_forever\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/platform/asyncio.py(149): start\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelapp.py(612): start\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/traitlets/config/application.py(845): launch_instance\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel_launcher.py(16): <module>\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/runpy.py(86): _run_code\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/runpy.py(193): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%input : Float(1:150528, 3:50176, 224:224, 224:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_reduce.weight : Float(24:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_reduce.bias : Float(24:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_expand.weight : Float(72:24, 24:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_expand.bias : Float(72:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_reduce.weight : Float(32:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_reduce.bias : Float(32:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_expand.weight : Float(120:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_expand.bias : Float(120:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_reduce.weight : Float(32:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_reduce.bias : Float(32:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_expand.weight : Float(120:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_expand.bias : Float(120:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_reduce.weight : Float(120:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_reduce.bias : Float(120:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_expand.weight : Float(480:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_reduce.weight : Float(168:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_reduce.bias : Float(168:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_expand.weight : Float(672:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_reduce.weight : Float(168:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_reduce.bias : Float(168:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_expand.weight : Float(672:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_reduce.weight : Float(240:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_reduce.bias : Float(240:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_expand.weight : Float(960:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_expand.bias : Float(960:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_reduce.weight : Float(240:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_reduce.bias : Float(240:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_expand.weight : Float(960:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_expand.bias : Float(960:1, requires_grad=1, device=cpu),\n      %backbone.0.conv_head.weight : Float(1280:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.conv_head.bias : Float(1280:1, requires_grad=1, device=cpu),\n      %backbone.0.classifier.weight : Float(512:1280, 1280:1, requires_grad=1, device=cpu),\n      %backbone.0.classifier.bias : Float(512:1, requires_grad=1, device=cpu),\n      %backbone.1.weight : Float(512:1, requires_grad=1, device=cpu),\n      %backbone.1.bias : Float(512:1, requires_grad=1, device=cpu),\n      %backbone.1.running_mean : Float(512:1, requires_grad=0, device=cpu),\n      %backbone.1.running_var : Float(512:1, requires_grad=0, device=cpu),\n      %gender_head.1.weight : Float(1:512, 512:1, requires_grad=1, device=cpu),\n      %gender_head.1.bias : Float(1:1, requires_grad=1, device=cpu),\n      %age_head.0.weight : Float(1:512, 512:1, requires_grad=1, device=cpu),\n      %age_head.0.bias : Float(1:1, requires_grad=1, device=cpu),\n      %753 : Float(16:27, 3:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %754 : Float(16:1, requires_grad=0, device=cpu),\n      %756 : Float(16:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %757 : Float(16:1, requires_grad=0, device=cpu),\n      %759 : Float(16:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %760 : Float(16:1, requires_grad=0, device=cpu),\n      %762 : Float(64:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %763 : Float(64:1, requires_grad=0, device=cpu),\n      %765 : Float(64:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %766 : Float(64:1, requires_grad=0, device=cpu),\n      %768 : Float(24:64, 64:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %769 : Float(24:1, requires_grad=0, device=cpu),\n      %771 : Float(72:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %772 : Float(72:1, requires_grad=0, device=cpu),\n      %774 : Float(72:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %775 : Float(72:1, requires_grad=0, device=cpu),\n      %777 : Float(24:72, 72:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %778 : Float(24:1, requires_grad=0, device=cpu),\n      %780 : Float(72:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %781 : Float(72:1, requires_grad=0, device=cpu),\n      %783 : Float(72:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %784 : Float(72:1, requires_grad=0, device=cpu),\n      %786 : Float(40:72, 72:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %787 : Float(40:1, requires_grad=0, device=cpu),\n      %789 : Float(120:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %790 : Float(120:1, requires_grad=0, device=cpu),\n      %792 : Float(120:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %793 : Float(120:1, requires_grad=0, device=cpu),\n      %795 : Float(40:120, 120:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %796 : Float(40:1, requires_grad=0, device=cpu),\n      %798 : Float(120:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %799 : Float(120:1, requires_grad=0, device=cpu),\n      %801 : Float(120:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %802 : Float(120:1, requires_grad=0, device=cpu),\n      %804 : Float(40:120, 120:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %805 : Float(40:1, requires_grad=0, device=cpu),\n      %807 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %808 : Float(240:1, requires_grad=0, device=cpu),\n      %810 : Float(240:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %811 : Float(240:1, requires_grad=0, device=cpu),\n      %813 : Float(80:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %814 : Float(80:1, requires_grad=0, device=cpu),\n      %816 : Float(200:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %817 : Float(200:1, requires_grad=0, device=cpu),\n      %819 : Float(200:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %820 : Float(200:1, requires_grad=0, device=cpu),\n      %822 : Float(80:200, 200:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %823 : Float(80:1, requires_grad=0, device=cpu),\n      %825 : Float(184:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %826 : Float(184:1, requires_grad=0, device=cpu),\n      %828 : Float(184:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %829 : Float(184:1, requires_grad=0, device=cpu),\n      %831 : Float(80:184, 184:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %832 : Float(80:1, requires_grad=0, device=cpu),\n      %834 : Float(184:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %835 : Float(184:1, requires_grad=0, device=cpu),\n      %837 : Float(184:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %838 : Float(184:1, requires_grad=0, device=cpu),\n      %840 : Float(80:184, 184:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %841 : Float(80:1, requires_grad=0, device=cpu),\n      %843 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %844 : Float(480:1, requires_grad=0, device=cpu),\n      %846 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %847 : Float(480:1, requires_grad=0, device=cpu),\n      %849 : Float(112:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %850 : Float(112:1, requires_grad=0, device=cpu),\n      %852 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %853 : Float(672:1, requires_grad=0, device=cpu),\n      %855 : Float(672:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %856 : Float(672:1, requires_grad=0, device=cpu),\n      %858 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %859 : Float(112:1, requires_grad=0, device=cpu),\n      %861 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %862 : Float(672:1, requires_grad=0, device=cpu),\n      %864 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %865 : Float(672:1, requires_grad=0, device=cpu),\n      %867 : Float(160:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %868 : Float(160:1, requires_grad=0, device=cpu),\n      %870 : Float(960:160, 160:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %871 : Float(960:1, requires_grad=0, device=cpu),\n      %873 : Float(960:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %874 : Float(960:1, requires_grad=0, device=cpu),\n      %876 : Float(160:960, 960:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %877 : Float(160:1, requires_grad=0, device=cpu),\n      %879 : Float(960:160, 160:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %880 : Float(960:1, requires_grad=0, device=cpu),\n      %882 : Float(960:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %883 : Float(960:1, requires_grad=0, device=cpu),\n      %885 : Float(160:960, 960:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %886 : Float(160:1, requires_grad=0, device=cpu),\n      %888 : Float(960:160, 160:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %889 : Float(960:1, requires_grad=0, device=cpu)):\n  %752 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input, %753, %754)\n  %324 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %325 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %326 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %327 : FloatTensor = onnx::Add(%752, %326) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %328 : Tensor = onnx::Clip(%327, %324, %325) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %329 : Tensor = onnx::Cast[to=11](%328)\n  %330 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %331 : DoubleTensor = onnx::Div(%329, %330) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %332 : FloatTensor = onnx::Cast[to=1](%331)\n  %333 : FloatTensor = onnx::Mul(%752, %332) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %755 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=16, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%333, %756, %757)\n  %336 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Relu(%755) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %758 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%336, %759, %760)\n  %339 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Add(%758, %333)\n  %761 : Float(1:802816, 64:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%339, %762, %763)\n  %342 : Float(1:802816, 64:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Relu(%761) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %764 : Float(1:200704, 64:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%342, %765, %766)\n  %345 : Float(1:200704, 64:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%764) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %767 : Float(1:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%345, %768, %769)\n  %770 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%767, %771, %772)\n  %350 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%770) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %773 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=72, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%350, %774, %775)\n  %353 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%773) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %776 : Float(1:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%353, %777, %778)\n  %356 : Float(1:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Add(%776, %767)\n  %779 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%356, %780, %781)\n  %359 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%779) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %782 : Float(1:56448, 72:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=72, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[2, 2]](%359, %783, %784)\n  %362 : Float(1:56448, 72:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%782) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %363 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%362) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %364 : Float(1:24, 24:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%363, %backbone.0.blocks.2.0.se.conv_reduce.weight, %backbone.0.blocks.2.0.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %365 : Float(1:24, 24:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%364) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %366 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %backbone.0.blocks.2.0.se.conv_expand.weight, %backbone.0.blocks.2.0.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %367 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %368 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%366, %367)\n  %369 : Tensor = onnx::Constant[value={0}]()\n  %370 : Tensor = onnx::Constant[value={6}]()\n  %371 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%368, %369, %370) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %372 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %373 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%371, %372)\n  %374 : Float(1:56448, 72:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%362, %373) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %785 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%374, %786, %787)\n  %788 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%785, %789, %790)\n  %379 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%788) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %791 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=120, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%379, %792, %793)\n  %382 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%791) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %383 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%382) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %384 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%383, %backbone.0.blocks.2.1.se.conv_reduce.weight, %backbone.0.blocks.2.1.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %385 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%384) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %386 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%385, %backbone.0.blocks.2.1.se.conv_expand.weight, %backbone.0.blocks.2.1.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %387 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %388 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%386, %387)\n  %389 : Tensor = onnx::Constant[value={0}]()\n  %390 : Tensor = onnx::Constant[value={6}]()\n  %391 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%388, %389, %390) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %392 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %393 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%391, %392)\n  %394 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%382, %393) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %794 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%394, %795, %796)\n  %397 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%794, %785)\n  %797 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%397, %798, %799)\n  %400 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%797) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %800 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=120, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%400, %801, %802)\n  %403 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%800) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %404 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%403) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %405 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%404, %backbone.0.blocks.2.2.se.conv_reduce.weight, %backbone.0.blocks.2.2.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %406 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%405) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %407 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%406, %backbone.0.blocks.2.2.se.conv_expand.weight, %backbone.0.blocks.2.2.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %408 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %409 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%407, %408)\n  %410 : Tensor = onnx::Constant[value={0}]()\n  %411 : Tensor = onnx::Constant[value={6}]()\n  %412 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%409, %410, %411) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %413 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %414 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%412, %413)\n  %415 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%403, %414) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %803 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%415, %804, %805)\n  %418 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%803, %397)\n  %806 : Float(1:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%418, %807, %808)\n  %421 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %422 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %423 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %424 : FloatTensor = onnx::Add(%806, %423) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %425 : Tensor = onnx::Clip(%424, %421, %422) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %426 : Tensor = onnx::Cast[to=11](%425)\n  %427 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %428 : DoubleTensor = onnx::Div(%426, %427) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %429 : FloatTensor = onnx::Cast[to=1](%428)\n  %430 : FloatTensor = onnx::Mul(%806, %429) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %809 : Float(1:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%430, %810, %811)\n  %433 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %434 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %435 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %436 : FloatTensor = onnx::Add(%809, %435) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %437 : Tensor = onnx::Clip(%436, %433, %434) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %438 : Tensor = onnx::Cast[to=11](%437)\n  %439 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %440 : DoubleTensor = onnx::Div(%438, %439) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %441 : FloatTensor = onnx::Cast[to=1](%440)\n  %442 : FloatTensor = onnx::Mul(%809, %441) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %812 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%442, %813, %814)\n  %815 : Float(1:39200, 200:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%812, %816, %817)\n  %447 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %448 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %449 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %450 : FloatTensor = onnx::Add(%815, %449) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %451 : Tensor = onnx::Clip(%450, %447, %448) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %452 : Tensor = onnx::Cast[to=11](%451)\n  %453 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %454 : DoubleTensor = onnx::Div(%452, %453) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %455 : FloatTensor = onnx::Cast[to=1](%454)\n  %456 : FloatTensor = onnx::Mul(%815, %455) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %818 : Float(1:39200, 200:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=200, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%456, %819, %820)\n  %459 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %460 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %461 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %462 : FloatTensor = onnx::Add(%818, %461) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %463 : Tensor = onnx::Clip(%462, %459, %460) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %464 : Tensor = onnx::Cast[to=11](%463)\n  %465 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %466 : DoubleTensor = onnx::Div(%464, %465) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %467 : FloatTensor = onnx::Cast[to=1](%466)\n  %468 : FloatTensor = onnx::Mul(%818, %467) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %821 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%468, %822, %823)\n  %471 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%821, %812)\n  %824 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%471, %825, %826)\n  %474 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %475 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %476 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %477 : FloatTensor = onnx::Add(%824, %476) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %478 : Tensor = onnx::Clip(%477, %474, %475) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %479 : Tensor = onnx::Cast[to=11](%478)\n  %480 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %481 : DoubleTensor = onnx::Div(%479, %480) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %482 : FloatTensor = onnx::Cast[to=1](%481)\n  %483 : FloatTensor = onnx::Mul(%824, %482) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %827 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=184, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%483, %828, %829)\n  %486 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %487 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %488 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %489 : FloatTensor = onnx::Add(%827, %488) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %490 : Tensor = onnx::Clip(%489, %486, %487) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %491 : Tensor = onnx::Cast[to=11](%490)\n  %492 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %493 : DoubleTensor = onnx::Div(%491, %492) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %494 : FloatTensor = onnx::Cast[to=1](%493)\n  %495 : FloatTensor = onnx::Mul(%827, %494) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %830 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %831, %832)\n  %498 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%830, %471)\n  %833 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%498, %834, %835)\n  %501 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %502 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %503 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %504 : FloatTensor = onnx::Add(%833, %503) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %505 : Tensor = onnx::Clip(%504, %501, %502) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %506 : Tensor = onnx::Cast[to=11](%505)\n  %507 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %508 : DoubleTensor = onnx::Div(%506, %507) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %509 : FloatTensor = onnx::Cast[to=1](%508)\n  %510 : FloatTensor = onnx::Mul(%833, %509) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %836 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=184, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%510, %837, %838)\n  %513 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %514 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %515 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %516 : FloatTensor = onnx::Add(%836, %515) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %517 : Tensor = onnx::Clip(%516, %513, %514) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %518 : Tensor = onnx::Cast[to=11](%517)\n  %519 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %520 : DoubleTensor = onnx::Div(%518, %519) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %521 : FloatTensor = onnx::Cast[to=1](%520)\n  %522 : FloatTensor = onnx::Mul(%836, %521) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %839 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%522, %840, %841)\n  %525 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%839, %498)\n  %842 : Float(1:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%525, %843, %844)\n  %528 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %529 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %530 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %531 : FloatTensor = onnx::Add(%842, %530) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %532 : Tensor = onnx::Clip(%531, %528, %529) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %533 : Tensor = onnx::Cast[to=11](%532)\n  %534 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %535 : DoubleTensor = onnx::Div(%533, %534) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %536 : FloatTensor = onnx::Cast[to=1](%535)\n  %537 : FloatTensor = onnx::Mul(%842, %536) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %845 : Float(1:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%537, %846, %847)\n  %540 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %541 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %542 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %543 : FloatTensor = onnx::Add(%845, %542) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %544 : Tensor = onnx::Clip(%543, %540, %541) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %545 : Tensor = onnx::Cast[to=11](%544)\n  %546 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %547 : DoubleTensor = onnx::Div(%545, %546) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %548 : FloatTensor = onnx::Cast[to=1](%547)\n  %549 : FloatTensor = onnx::Mul(%845, %548) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %550 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%549) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %551 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%550, %backbone.0.blocks.4.0.se.conv_reduce.weight, %backbone.0.blocks.4.0.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %552 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%551) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %553 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%552, %backbone.0.blocks.4.0.se.conv_expand.weight, %backbone.0.blocks.4.0.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %554 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %555 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%553, %554)\n  %556 : Tensor = onnx::Constant[value={0}]()\n  %557 : Tensor = onnx::Constant[value={6}]()\n  %558 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%555, %556, %557) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %559 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %560 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%558, %559)\n  %561 : Float(1:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%549, %560) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %848 : Float(1:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%561, %849, %850)\n  %851 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%848, %852, %853)\n  %566 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %567 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %568 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %569 : FloatTensor = onnx::Add(%851, %568) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %570 : Tensor = onnx::Clip(%569, %566, %567) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %571 : Tensor = onnx::Cast[to=11](%570)\n  %572 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %573 : DoubleTensor = onnx::Div(%571, %572) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %574 : FloatTensor = onnx::Cast[to=1](%573)\n  %575 : FloatTensor = onnx::Mul(%851, %574) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %854 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%575, %855, %856)\n  %578 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %579 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %580 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %581 : FloatTensor = onnx::Add(%854, %580) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %582 : Tensor = onnx::Clip(%581, %578, %579) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %583 : Tensor = onnx::Cast[to=11](%582)\n  %584 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %585 : DoubleTensor = onnx::Div(%583, %584) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %586 : FloatTensor = onnx::Cast[to=1](%585)\n  %587 : FloatTensor = onnx::Mul(%854, %586) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %588 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%587) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %589 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%588, %backbone.0.blocks.4.1.se.conv_reduce.weight, %backbone.0.blocks.4.1.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %590 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%589) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %591 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%590, %backbone.0.blocks.4.1.se.conv_expand.weight, %backbone.0.blocks.4.1.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %592 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %593 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%591, %592)\n  %594 : Tensor = onnx::Constant[value={0}]()\n  %595 : Tensor = onnx::Constant[value={6}]()\n  %596 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%593, %594, %595) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %597 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %598 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%596, %597)\n  %599 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%587, %598) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %857 : Float(1:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%599, %858, %859)\n  %602 : Float(1:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%857, %848)\n  %860 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%602, %861, %862)\n  %605 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %606 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %607 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %608 : FloatTensor = onnx::Add(%860, %607) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %609 : Tensor = onnx::Clip(%608, %605, %606) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %610 : Tensor = onnx::Cast[to=11](%609)\n  %611 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %612 : DoubleTensor = onnx::Div(%610, %611) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %613 : FloatTensor = onnx::Cast[to=1](%612)\n  %614 : FloatTensor = onnx::Mul(%860, %613) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %863 : Float(1:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[2, 2]](%614, %864, %865)\n  %617 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %618 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %619 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %620 : FloatTensor = onnx::Add(%863, %619) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %621 : Tensor = onnx::Clip(%620, %617, %618) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %622 : Tensor = onnx::Cast[to=11](%621)\n  %623 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %624 : DoubleTensor = onnx::Div(%622, %623) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %625 : FloatTensor = onnx::Cast[to=1](%624)\n  %626 : FloatTensor = onnx::Mul(%863, %625) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %627 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%626) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %628 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%627, %backbone.0.blocks.5.0.se.conv_reduce.weight, %backbone.0.blocks.5.0.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %629 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%628) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %630 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%629, %backbone.0.blocks.5.0.se.conv_expand.weight, %backbone.0.blocks.5.0.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %631 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %632 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%630, %631)\n  %633 : Tensor = onnx::Constant[value={0}]()\n  %634 : Tensor = onnx::Constant[value={6}]()\n  %635 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%632, %633, %634) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %636 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %637 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%635, %636)\n  %638 : Float(1:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%626, %637) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %866 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%638, %867, %868)\n  %869 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%866, %870, %871)\n  %643 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %644 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %645 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %646 : FloatTensor = onnx::Add(%869, %645) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %647 : Tensor = onnx::Clip(%646, %643, %644) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %648 : Tensor = onnx::Cast[to=11](%647)\n  %649 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %650 : DoubleTensor = onnx::Div(%648, %649) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %651 : FloatTensor = onnx::Cast[to=1](%650)\n  %652 : FloatTensor = onnx::Mul(%869, %651) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %872 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%652, %873, %874)\n  %655 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %656 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %657 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %658 : FloatTensor = onnx::Add(%872, %657) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %659 : Tensor = onnx::Clip(%658, %655, %656) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %660 : Tensor = onnx::Cast[to=11](%659)\n  %661 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %662 : DoubleTensor = onnx::Div(%660, %661) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %663 : FloatTensor = onnx::Cast[to=1](%662)\n  %664 : FloatTensor = onnx::Mul(%872, %663) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %665 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%664) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %666 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%665, %backbone.0.blocks.5.1.se.conv_reduce.weight, %backbone.0.blocks.5.1.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %667 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%666) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %668 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%667, %backbone.0.blocks.5.1.se.conv_expand.weight, %backbone.0.blocks.5.1.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %669 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %670 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%668, %669)\n  %671 : Tensor = onnx::Constant[value={0}]()\n  %672 : Tensor = onnx::Constant[value={6}]()\n  %673 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%670, %671, %672) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %674 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %675 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%673, %674)\n  %676 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%664, %675) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %875 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%676, %876, %877)\n  %679 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%875, %866)\n  %878 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%679, %879, %880)\n  %682 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %683 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %684 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %685 : FloatTensor = onnx::Add(%878, %684) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %686 : Tensor = onnx::Clip(%685, %682, %683) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %687 : Tensor = onnx::Cast[to=11](%686)\n  %688 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %689 : DoubleTensor = onnx::Div(%687, %688) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %690 : FloatTensor = onnx::Cast[to=1](%689)\n  %691 : FloatTensor = onnx::Mul(%878, %690) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %881 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%691, %882, %883)\n  %694 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %695 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %696 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %697 : FloatTensor = onnx::Add(%881, %696) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %698 : Tensor = onnx::Clip(%697, %694, %695) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %699 : Tensor = onnx::Cast[to=11](%698)\n  %700 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %701 : DoubleTensor = onnx::Div(%699, %700) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %702 : FloatTensor = onnx::Cast[to=1](%701)\n  %703 : FloatTensor = onnx::Mul(%881, %702) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %704 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%703) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %705 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%704, %backbone.0.blocks.5.2.se.conv_reduce.weight, %backbone.0.blocks.5.2.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %706 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%705) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %707 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%706, %backbone.0.blocks.5.2.se.conv_expand.weight, %backbone.0.blocks.5.2.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %708 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %709 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%707, %708)\n  %710 : Tensor = onnx::Constant[value={0}]()\n  %711 : Tensor = onnx::Constant[value={6}]()\n  %712 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%709, %710, %711) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %713 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %714 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%712, %713)\n  %715 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%703, %714) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %884 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%715, %885, %886)\n  %718 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%884, %679)\n  %887 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%718, %888, %889)\n  %721 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %722 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %723 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %724 : FloatTensor = onnx::Add(%887, %723) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %725 : Tensor = onnx::Clip(%724, %721, %722) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %726 : Tensor = onnx::Cast[to=11](%725)\n  %727 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %728 : DoubleTensor = onnx::Div(%726, %727) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %729 : FloatTensor = onnx::Cast[to=1](%728)\n  %730 : FloatTensor = onnx::Mul(%887, %729) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %731 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%730) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:936:0\n  %732 : Float(1:1280, 1280:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%731, %backbone.0.conv_head.weight, %backbone.0.conv_head.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %733 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %734 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %735 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %736 : FloatTensor = onnx::Add(%732, %735) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %737 : Tensor = onnx::Clip(%736, %733, %734) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %738 : Tensor = onnx::Cast[to=11](%737)\n  %739 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %740 : DoubleTensor = onnx::Div(%738, %739) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %741 : FloatTensor = onnx::Cast[to=1](%740)\n  %742 : FloatTensor = onnx::Mul(%732, %741) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %743 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %744 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}]()\n  %input.138 : Float(1:1280, 1280:1, requires_grad=1, device=cpu) = aten::flatten(%742, %743, %744) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/mobilenetv3.py:141:0\n  %746 : Float(1:512, 512:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%input.138, %backbone.0.classifier.weight, %backbone.0.classifier.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n  %747 : Float(1:512, 512:1, requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%746, %backbone.1.weight, %backbone.1.bias, %backbone.1.running_mean, %backbone.1.running_var) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:2056:0\n  %748 : Float(1:512, 512:1, requires_grad=1, device=cpu) = onnx::Relu(%747) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n  %749 : Float(1:1, 1:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%748, %gender_head.1.weight, %gender_head.1.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n  %output_gender : Float(1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%749) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/activation.py:299:0\n  %output_age : Float(1:1, 1:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%748, %age_head.0.weight, %age_head.0.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n  return (%output_age, %output_gender)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f07c5c3db454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Export the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m torch.onnx.export(\n\u001b[0m\u001b[1;32m     11\u001b[0m                   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m# model being run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# model input (or a tuple for multiple inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     return utils.export(model, args, f, export_params, verbose, training,\n\u001b[0m\u001b[1;32m    226\u001b[0m                         \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_raw_ir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0moperator_export_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperatorExportTypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     _export(model, args, f, export_params, verbose, training, input_names, output_names,\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0moperator_export_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0m_retain_param_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_retain_param_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference, use_new_jit_passes)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexport_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 proto, export_map = graph._export_onnx(\n\u001b[0m\u001b[1;32m    648\u001b[0m                     \u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefer_weight_export\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                     \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_keep_init_as_ip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: ONNX export failed: Couldn't export operator aten::flatten\n\nDefined at:\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/mobilenetv3.py(141): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/container.py(117): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(725): _call_impl\n<ipython-input-14-b8c514028c28>(2): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(725): _call_impl\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/jit/_trace.py(116): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/jit/_trace.py(125): forward\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/module.py(727): _call_impl\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/jit/_trace.py(1148): _get_trace_graph\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(342): _trace_and_get_graph_from_model\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(379): _create_jit_graph\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(409): _model_to_graph\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(632): _export\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/utils.py(85): export\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/onnx/__init__.py(225): export\n<ipython-input-21-f07c5c3db454>(10): <module>\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3418): run_code\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3338): run_ast_nodes\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(3146): run_cell_async\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/async_helpers.py(68): _pseudo_sync_runner\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2923): _run_cell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/IPython/core/interactiveshell.py(2877): run_cell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/zmqshell.py(536): run_cell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/ipkernel.py(306): do_execute\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(209): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelbase.py(543): execute_request\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(209): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelbase.py(268): dispatch_shell\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(209): wrapper\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelbase.py(365): process_one\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(748): run\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/gen.py(787): inner\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/ioloop.py(743): _run_callback\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/ioloop.py(690): <lambda>\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/asyncio/events.py(81): _run\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/asyncio/base_events.py(1859): _run_once\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/asyncio/base_events.py(570): run_forever\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/tornado/platform/asyncio.py(149): start\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel/kernelapp.py(612): start\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/traitlets/config/application.py(845): launch_instance\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/ipykernel_launcher.py(16): <module>\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/runpy.py(86): _run_code\n/home/anhduy/anaconda3/envs/age_gender/lib/python3.8/runpy.py(193): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%input : Float(1:150528, 3:50176, 224:224, 224:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_reduce.weight : Float(24:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_reduce.bias : Float(24:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_expand.weight : Float(72:24, 24:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.0.se.conv_expand.bias : Float(72:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_reduce.weight : Float(32:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_reduce.bias : Float(32:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_expand.weight : Float(120:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.1.se.conv_expand.bias : Float(120:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_reduce.weight : Float(32:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_reduce.bias : Float(32:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_expand.weight : Float(120:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.2.2.se.conv_expand.bias : Float(120:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_reduce.weight : Float(120:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_reduce.bias : Float(120:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_expand.weight : Float(480:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.0.se.conv_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_reduce.weight : Float(168:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_reduce.bias : Float(168:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_expand.weight : Float(672:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.4.1.se.conv_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_reduce.weight : Float(168:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_reduce.bias : Float(168:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_expand.weight : Float(672:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.0.se.conv_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_reduce.weight : Float(240:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_reduce.bias : Float(240:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_expand.weight : Float(960:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.1.se.conv_expand.bias : Float(960:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_reduce.weight : Float(240:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_reduce.bias : Float(240:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_expand.weight : Float(960:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.blocks.5.2.se.conv_expand.bias : Float(960:1, requires_grad=1, device=cpu),\n      %backbone.0.conv_head.weight : Float(1280:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %backbone.0.conv_head.bias : Float(1280:1, requires_grad=1, device=cpu),\n      %backbone.0.classifier.weight : Float(512:1280, 1280:1, requires_grad=1, device=cpu),\n      %backbone.0.classifier.bias : Float(512:1, requires_grad=1, device=cpu),\n      %backbone.1.weight : Float(512:1, requires_grad=1, device=cpu),\n      %backbone.1.bias : Float(512:1, requires_grad=1, device=cpu),\n      %backbone.1.running_mean : Float(512:1, requires_grad=0, device=cpu),\n      %backbone.1.running_var : Float(512:1, requires_grad=0, device=cpu),\n      %gender_head.1.weight : Float(1:512, 512:1, requires_grad=1, device=cpu),\n      %gender_head.1.bias : Float(1:1, requires_grad=1, device=cpu),\n      %age_head.0.weight : Float(1:512, 512:1, requires_grad=1, device=cpu),\n      %age_head.0.bias : Float(1:1, requires_grad=1, device=cpu),\n      %753 : Float(16:27, 3:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %754 : Float(16:1, requires_grad=0, device=cpu),\n      %756 : Float(16:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %757 : Float(16:1, requires_grad=0, device=cpu),\n      %759 : Float(16:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %760 : Float(16:1, requires_grad=0, device=cpu),\n      %762 : Float(64:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %763 : Float(64:1, requires_grad=0, device=cpu),\n      %765 : Float(64:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %766 : Float(64:1, requires_grad=0, device=cpu),\n      %768 : Float(24:64, 64:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %769 : Float(24:1, requires_grad=0, device=cpu),\n      %771 : Float(72:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %772 : Float(72:1, requires_grad=0, device=cpu),\n      %774 : Float(72:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %775 : Float(72:1, requires_grad=0, device=cpu),\n      %777 : Float(24:72, 72:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %778 : Float(24:1, requires_grad=0, device=cpu),\n      %780 : Float(72:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %781 : Float(72:1, requires_grad=0, device=cpu),\n      %783 : Float(72:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %784 : Float(72:1, requires_grad=0, device=cpu),\n      %786 : Float(40:72, 72:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %787 : Float(40:1, requires_grad=0, device=cpu),\n      %789 : Float(120:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %790 : Float(120:1, requires_grad=0, device=cpu),\n      %792 : Float(120:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %793 : Float(120:1, requires_grad=0, device=cpu),\n      %795 : Float(40:120, 120:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %796 : Float(40:1, requires_grad=0, device=cpu),\n      %798 : Float(120:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %799 : Float(120:1, requires_grad=0, device=cpu),\n      %801 : Float(120:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %802 : Float(120:1, requires_grad=0, device=cpu),\n      %804 : Float(40:120, 120:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %805 : Float(40:1, requires_grad=0, device=cpu),\n      %807 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %808 : Float(240:1, requires_grad=0, device=cpu),\n      %810 : Float(240:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %811 : Float(240:1, requires_grad=0, device=cpu),\n      %813 : Float(80:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %814 : Float(80:1, requires_grad=0, device=cpu),\n      %816 : Float(200:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %817 : Float(200:1, requires_grad=0, device=cpu),\n      %819 : Float(200:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %820 : Float(200:1, requires_grad=0, device=cpu),\n      %822 : Float(80:200, 200:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %823 : Float(80:1, requires_grad=0, device=cpu),\n      %825 : Float(184:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %826 : Float(184:1, requires_grad=0, device=cpu),\n      %828 : Float(184:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %829 : Float(184:1, requires_grad=0, device=cpu),\n      %831 : Float(80:184, 184:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %832 : Float(80:1, requires_grad=0, device=cpu),\n      %834 : Float(184:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %835 : Float(184:1, requires_grad=0, device=cpu),\n      %837 : Float(184:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %838 : Float(184:1, requires_grad=0, device=cpu),\n      %840 : Float(80:184, 184:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %841 : Float(80:1, requires_grad=0, device=cpu),\n      %843 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %844 : Float(480:1, requires_grad=0, device=cpu),\n      %846 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %847 : Float(480:1, requires_grad=0, device=cpu),\n      %849 : Float(112:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %850 : Float(112:1, requires_grad=0, device=cpu),\n      %852 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %853 : Float(672:1, requires_grad=0, device=cpu),\n      %855 : Float(672:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %856 : Float(672:1, requires_grad=0, device=cpu),\n      %858 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %859 : Float(112:1, requires_grad=0, device=cpu),\n      %861 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %862 : Float(672:1, requires_grad=0, device=cpu),\n      %864 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %865 : Float(672:1, requires_grad=0, device=cpu),\n      %867 : Float(160:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %868 : Float(160:1, requires_grad=0, device=cpu),\n      %870 : Float(960:160, 160:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %871 : Float(960:1, requires_grad=0, device=cpu),\n      %873 : Float(960:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %874 : Float(960:1, requires_grad=0, device=cpu),\n      %876 : Float(160:960, 960:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %877 : Float(160:1, requires_grad=0, device=cpu),\n      %879 : Float(960:160, 160:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %880 : Float(960:1, requires_grad=0, device=cpu),\n      %882 : Float(960:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %883 : Float(960:1, requires_grad=0, device=cpu),\n      %885 : Float(160:960, 960:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %886 : Float(160:1, requires_grad=0, device=cpu),\n      %888 : Float(960:160, 160:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %889 : Float(960:1, requires_grad=0, device=cpu)):\n  %752 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%input, %753, %754)\n  %324 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %325 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %326 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %327 : FloatTensor = onnx::Add(%752, %326) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %328 : Tensor = onnx::Clip(%327, %324, %325) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %329 : Tensor = onnx::Cast[to=11](%328)\n  %330 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %331 : DoubleTensor = onnx::Div(%329, %330) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %332 : FloatTensor = onnx::Cast[to=1](%331)\n  %333 : FloatTensor = onnx::Mul(%752, %332) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %755 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=16, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%333, %756, %757)\n  %336 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Relu(%755) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %758 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%336, %759, %760)\n  %339 : Float(1:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Add(%758, %333)\n  %761 : Float(1:802816, 64:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%339, %762, %763)\n  %342 : Float(1:802816, 64:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Relu(%761) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %764 : Float(1:200704, 64:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=64, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%342, %765, %766)\n  %345 : Float(1:200704, 64:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%764) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %767 : Float(1:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%345, %768, %769)\n  %770 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%767, %771, %772)\n  %350 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%770) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %773 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=72, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%350, %774, %775)\n  %353 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%773) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %776 : Float(1:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%353, %777, %778)\n  %356 : Float(1:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Add(%776, %767)\n  %779 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%356, %780, %781)\n  %359 : Float(1:225792, 72:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Relu(%779) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %782 : Float(1:56448, 72:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=72, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[2, 2]](%359, %783, %784)\n  %362 : Float(1:56448, 72:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%782) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %363 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%362) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %364 : Float(1:24, 24:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%363, %backbone.0.blocks.2.0.se.conv_reduce.weight, %backbone.0.blocks.2.0.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %365 : Float(1:24, 24:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%364) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %366 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%365, %backbone.0.blocks.2.0.se.conv_expand.weight, %backbone.0.blocks.2.0.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %367 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %368 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%366, %367)\n  %369 : Tensor = onnx::Constant[value={0}]()\n  %370 : Tensor = onnx::Constant[value={6}]()\n  %371 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%368, %369, %370) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %372 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %373 : Float(1:72, 72:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%371, %372)\n  %374 : Float(1:56448, 72:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%362, %373) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %785 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%374, %786, %787)\n  %788 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%785, %789, %790)\n  %379 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%788) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %791 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=120, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%379, %792, %793)\n  %382 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%791) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %383 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%382) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %384 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%383, %backbone.0.blocks.2.1.se.conv_reduce.weight, %backbone.0.blocks.2.1.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %385 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%384) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %386 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%385, %backbone.0.blocks.2.1.se.conv_expand.weight, %backbone.0.blocks.2.1.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %387 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %388 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%386, %387)\n  %389 : Tensor = onnx::Constant[value={0}]()\n  %390 : Tensor = onnx::Constant[value={6}]()\n  %391 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%388, %389, %390) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %392 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %393 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%391, %392)\n  %394 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%382, %393) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %794 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%394, %795, %796)\n  %397 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%794, %785)\n  %797 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%397, %798, %799)\n  %400 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%797) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %800 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=120, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%400, %801, %802)\n  %403 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Relu(%800) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %404 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%403) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %405 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%404, %backbone.0.blocks.2.2.se.conv_reduce.weight, %backbone.0.blocks.2.2.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %406 : Float(1:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%405) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %407 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%406, %backbone.0.blocks.2.2.se.conv_expand.weight, %backbone.0.blocks.2.2.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %408 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %409 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%407, %408)\n  %410 : Tensor = onnx::Constant[value={0}]()\n  %411 : Tensor = onnx::Constant[value={6}]()\n  %412 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%409, %410, %411) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %413 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %414 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%412, %413)\n  %415 : Float(1:94080, 120:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%403, %414) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %803 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%415, %804, %805)\n  %418 : Float(1:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%803, %397)\n  %806 : Float(1:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%418, %807, %808)\n  %421 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %422 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %423 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %424 : FloatTensor = onnx::Add(%806, %423) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %425 : Tensor = onnx::Clip(%424, %421, %422) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %426 : Tensor = onnx::Cast[to=11](%425)\n  %427 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %428 : DoubleTensor = onnx::Div(%426, %427) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %429 : FloatTensor = onnx::Cast[to=1](%428)\n  %430 : FloatTensor = onnx::Mul(%806, %429) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %809 : Float(1:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%430, %810, %811)\n  %433 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %434 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %435 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %436 : FloatTensor = onnx::Add(%809, %435) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %437 : Tensor = onnx::Clip(%436, %433, %434) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %438 : Tensor = onnx::Cast[to=11](%437)\n  %439 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %440 : DoubleTensor = onnx::Div(%438, %439) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %441 : FloatTensor = onnx::Cast[to=1](%440)\n  %442 : FloatTensor = onnx::Mul(%809, %441) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %812 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%442, %813, %814)\n  %815 : Float(1:39200, 200:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%812, %816, %817)\n  %447 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %448 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %449 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %450 : FloatTensor = onnx::Add(%815, %449) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %451 : Tensor = onnx::Clip(%450, %447, %448) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %452 : Tensor = onnx::Cast[to=11](%451)\n  %453 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %454 : DoubleTensor = onnx::Div(%452, %453) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %455 : FloatTensor = onnx::Cast[to=1](%454)\n  %456 : FloatTensor = onnx::Mul(%815, %455) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %818 : Float(1:39200, 200:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=200, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%456, %819, %820)\n  %459 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %460 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %461 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %462 : FloatTensor = onnx::Add(%818, %461) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %463 : Tensor = onnx::Clip(%462, %459, %460) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %464 : Tensor = onnx::Cast[to=11](%463)\n  %465 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %466 : DoubleTensor = onnx::Div(%464, %465) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %467 : FloatTensor = onnx::Cast[to=1](%466)\n  %468 : FloatTensor = onnx::Mul(%818, %467) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %821 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%468, %822, %823)\n  %471 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%821, %812)\n  %824 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%471, %825, %826)\n  %474 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %475 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %476 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %477 : FloatTensor = onnx::Add(%824, %476) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %478 : Tensor = onnx::Clip(%477, %474, %475) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %479 : Tensor = onnx::Cast[to=11](%478)\n  %480 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %481 : DoubleTensor = onnx::Div(%479, %480) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %482 : FloatTensor = onnx::Cast[to=1](%481)\n  %483 : FloatTensor = onnx::Mul(%824, %482) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %827 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=184, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%483, %828, %829)\n  %486 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %487 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %488 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %489 : FloatTensor = onnx::Add(%827, %488) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %490 : Tensor = onnx::Clip(%489, %486, %487) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %491 : Tensor = onnx::Cast[to=11](%490)\n  %492 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %493 : DoubleTensor = onnx::Div(%491, %492) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %494 : FloatTensor = onnx::Cast[to=1](%493)\n  %495 : FloatTensor = onnx::Mul(%827, %494) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %830 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%495, %831, %832)\n  %498 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%830, %471)\n  %833 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%498, %834, %835)\n  %501 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %502 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %503 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %504 : FloatTensor = onnx::Add(%833, %503) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %505 : Tensor = onnx::Clip(%504, %501, %502) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %506 : Tensor = onnx::Cast[to=11](%505)\n  %507 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %508 : DoubleTensor = onnx::Div(%506, %507) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %509 : FloatTensor = onnx::Cast[to=1](%508)\n  %510 : FloatTensor = onnx::Mul(%833, %509) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %836 : Float(1:36064, 184:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=184, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%510, %837, %838)\n  %513 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %514 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %515 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %516 : FloatTensor = onnx::Add(%836, %515) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %517 : Tensor = onnx::Clip(%516, %513, %514) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %518 : Tensor = onnx::Cast[to=11](%517)\n  %519 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %520 : DoubleTensor = onnx::Div(%518, %519) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %521 : FloatTensor = onnx::Cast[to=1](%520)\n  %522 : FloatTensor = onnx::Mul(%836, %521) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %839 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%522, %840, %841)\n  %525 : Float(1:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%839, %498)\n  %842 : Float(1:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%525, %843, %844)\n  %528 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %529 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %530 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %531 : FloatTensor = onnx::Add(%842, %530) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %532 : Tensor = onnx::Clip(%531, %528, %529) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %533 : Tensor = onnx::Cast[to=11](%532)\n  %534 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %535 : DoubleTensor = onnx::Div(%533, %534) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %536 : FloatTensor = onnx::Cast[to=1](%535)\n  %537 : FloatTensor = onnx::Mul(%842, %536) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %845 : Float(1:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%537, %846, %847)\n  %540 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %541 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %542 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %543 : FloatTensor = onnx::Add(%845, %542) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %544 : Tensor = onnx::Clip(%543, %540, %541) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %545 : Tensor = onnx::Cast[to=11](%544)\n  %546 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %547 : DoubleTensor = onnx::Div(%545, %546) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %548 : FloatTensor = onnx::Cast[to=1](%547)\n  %549 : FloatTensor = onnx::Mul(%845, %548) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %550 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%549) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %551 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%550, %backbone.0.blocks.4.0.se.conv_reduce.weight, %backbone.0.blocks.4.0.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %552 : Float(1:120, 120:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%551) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %553 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%552, %backbone.0.blocks.4.0.se.conv_expand.weight, %backbone.0.blocks.4.0.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %554 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %555 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%553, %554)\n  %556 : Tensor = onnx::Constant[value={0}]()\n  %557 : Tensor = onnx::Constant[value={6}]()\n  %558 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%555, %556, %557) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %559 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %560 : Float(1:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%558, %559)\n  %561 : Float(1:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%549, %560) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %848 : Float(1:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%561, %849, %850)\n  %851 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%848, %852, %853)\n  %566 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %567 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %568 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %569 : FloatTensor = onnx::Add(%851, %568) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %570 : Tensor = onnx::Clip(%569, %566, %567) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %571 : Tensor = onnx::Cast[to=11](%570)\n  %572 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %573 : DoubleTensor = onnx::Div(%571, %572) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %574 : FloatTensor = onnx::Cast[to=1](%573)\n  %575 : FloatTensor = onnx::Mul(%851, %574) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %854 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%575, %855, %856)\n  %578 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %579 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %580 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %581 : FloatTensor = onnx::Add(%854, %580) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %582 : Tensor = onnx::Clip(%581, %578, %579) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %583 : Tensor = onnx::Cast[to=11](%582)\n  %584 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %585 : DoubleTensor = onnx::Div(%583, %584) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %586 : FloatTensor = onnx::Cast[to=1](%585)\n  %587 : FloatTensor = onnx::Mul(%854, %586) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %588 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%587) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %589 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%588, %backbone.0.blocks.4.1.se.conv_reduce.weight, %backbone.0.blocks.4.1.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %590 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%589) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %591 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%590, %backbone.0.blocks.4.1.se.conv_expand.weight, %backbone.0.blocks.4.1.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %592 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %593 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%591, %592)\n  %594 : Tensor = onnx::Constant[value={0}]()\n  %595 : Tensor = onnx::Constant[value={6}]()\n  %596 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%593, %594, %595) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %597 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %598 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%596, %597)\n  %599 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%587, %598) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %857 : Float(1:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%599, %858, %859)\n  %602 : Float(1:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%857, %848)\n  %860 : Float(1:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%602, %861, %862)\n  %605 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %606 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %607 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %608 : FloatTensor = onnx::Add(%860, %607) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %609 : Tensor = onnx::Clip(%608, %605, %606) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %610 : Tensor = onnx::Cast[to=11](%609)\n  %611 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %612 : DoubleTensor = onnx::Div(%610, %611) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %613 : FloatTensor = onnx::Cast[to=1](%612)\n  %614 : FloatTensor = onnx::Mul(%860, %613) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %863 : Float(1:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[2, 2]](%614, %864, %865)\n  %617 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %618 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %619 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %620 : FloatTensor = onnx::Add(%863, %619) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %621 : Tensor = onnx::Clip(%620, %617, %618) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %622 : Tensor = onnx::Cast[to=11](%621)\n  %623 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %624 : DoubleTensor = onnx::Div(%622, %623) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %625 : FloatTensor = onnx::Cast[to=1](%624)\n  %626 : FloatTensor = onnx::Mul(%863, %625) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %627 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%626) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %628 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%627, %backbone.0.blocks.5.0.se.conv_reduce.weight, %backbone.0.blocks.5.0.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %629 : Float(1:168, 168:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%628) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %630 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%629, %backbone.0.blocks.5.0.se.conv_expand.weight, %backbone.0.blocks.5.0.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %631 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %632 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%630, %631)\n  %633 : Tensor = onnx::Constant[value={0}]()\n  %634 : Tensor = onnx::Constant[value={6}]()\n  %635 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%632, %633, %634) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %636 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %637 : Float(1:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%635, %636)\n  %638 : Float(1:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%626, %637) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %866 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%638, %867, %868)\n  %869 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%866, %870, %871)\n  %643 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %644 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %645 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %646 : FloatTensor = onnx::Add(%869, %645) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %647 : Tensor = onnx::Clip(%646, %643, %644) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %648 : Tensor = onnx::Cast[to=11](%647)\n  %649 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %650 : DoubleTensor = onnx::Div(%648, %649) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %651 : FloatTensor = onnx::Cast[to=1](%650)\n  %652 : FloatTensor = onnx::Mul(%869, %651) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %872 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%652, %873, %874)\n  %655 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %656 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %657 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %658 : FloatTensor = onnx::Add(%872, %657) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %659 : Tensor = onnx::Clip(%658, %655, %656) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %660 : Tensor = onnx::Cast[to=11](%659)\n  %661 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %662 : DoubleTensor = onnx::Div(%660, %661) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %663 : FloatTensor = onnx::Cast[to=1](%662)\n  %664 : FloatTensor = onnx::Mul(%872, %663) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %665 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%664) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %666 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%665, %backbone.0.blocks.5.1.se.conv_reduce.weight, %backbone.0.blocks.5.1.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %667 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%666) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %668 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%667, %backbone.0.blocks.5.1.se.conv_expand.weight, %backbone.0.blocks.5.1.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %669 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %670 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%668, %669)\n  %671 : Tensor = onnx::Constant[value={0}]()\n  %672 : Tensor = onnx::Constant[value={6}]()\n  %673 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%670, %671, %672) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %674 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %675 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%673, %674)\n  %676 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%664, %675) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %875 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%676, %876, %877)\n  %679 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%875, %866)\n  %878 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%679, %879, %880)\n  %682 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %683 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %684 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %685 : FloatTensor = onnx::Add(%878, %684) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %686 : Tensor = onnx::Clip(%685, %682, %683) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %687 : Tensor = onnx::Cast[to=11](%686)\n  %688 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %689 : DoubleTensor = onnx::Div(%687, %688) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %690 : FloatTensor = onnx::Cast[to=1](%689)\n  %691 : FloatTensor = onnx::Mul(%878, %690) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %881 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=960, kernel_shape=[5, 5], pads=[2, 2, 2, 2], strides=[1, 1]](%691, %882, %883)\n  %694 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %695 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %696 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %697 : FloatTensor = onnx::Add(%881, %696) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %698 : Tensor = onnx::Clip(%697, %694, %695) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %699 : Tensor = onnx::Cast[to=11](%698)\n  %700 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %701 : DoubleTensor = onnx::Div(%699, %700) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %702 : FloatTensor = onnx::Cast[to=1](%701)\n  %703 : FloatTensor = onnx::Mul(%881, %702) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %704 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::ReduceMean[axes=[2, 3], keepdims=1](%703) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:116:0\n  %705 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%704, %backbone.0.blocks.5.2.se.conv_reduce.weight, %backbone.0.blocks.5.2.se.conv_reduce.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %706 : Float(1:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Relu(%705) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1134:0\n  %707 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%706, %backbone.0.blocks.5.2.se.conv_expand.weight, %backbone.0.blocks.5.2.se.conv_expand.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %708 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %709 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Add(%707, %708)\n  %710 : Tensor = onnx::Constant[value={0}]()\n  %711 : Tensor = onnx::Constant[value={6}]()\n  %712 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Clip(%709, %710, %711) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1188:0\n  %713 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %714 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Div(%712, %713)\n  %715 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%703, %714) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py:120:0\n  %884 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%715, %885, %886)\n  %718 : Float(1:7840, 160:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%884, %679)\n  %887 : Float(1:47040, 960:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%718, %888, %889)\n  %721 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %722 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %723 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %724 : FloatTensor = onnx::Add(%887, %723) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %725 : Tensor = onnx::Clip(%724, %721, %722) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %726 : Tensor = onnx::Cast[to=11](%725)\n  %727 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %728 : DoubleTensor = onnx::Div(%726, %727) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %729 : FloatTensor = onnx::Cast[to=1](%728)\n  %730 : FloatTensor = onnx::Mul(%887, %729) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %731 : Float(1:960, 960:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%730) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:936:0\n  %732 : Float(1:1280, 1280:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%731, %backbone.0.conv_head.weight, %backbone.0.conv_head.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/conv.py:419:0\n  %733 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n  %734 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %735 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={3}]()\n  %736 : FloatTensor = onnx::Add(%732, %735) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %737 : Tensor = onnx::Clip(%736, %733, %734) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %738 : Tensor = onnx::Cast[to=11](%737)\n  %739 : Double(requires_grad=0, device=cpu) = onnx::Constant[value={6}]()\n  %740 : DoubleTensor = onnx::Div(%738, %739) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:16\n  %741 : FloatTensor = onnx::Cast[to=1](%740)\n  %742 : FloatTensor = onnx::Mul(%732, %741) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/layers/activations_jit.py:65:11\n  %743 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n  %744 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={-1}]()\n  %input.138 : Float(1:1280, 1280:1, requires_grad=1, device=cpu) = aten::flatten(%742, %743, %744) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/timm/models/mobilenetv3.py:141:0\n  %746 : Float(1:512, 512:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%input.138, %backbone.0.classifier.weight, %backbone.0.classifier.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n  %747 : Float(1:512, 512:1, requires_grad=1, device=cpu) = onnx::BatchNormalization[epsilon=1.0000000000000001e-05, momentum=0.90000000000000002](%746, %backbone.1.weight, %backbone.1.bias, %backbone.1.running_mean, %backbone.1.running_var) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:2056:0\n  %748 : Float(1:512, 512:1, requires_grad=1, device=cpu) = onnx::Relu(%747) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:983:0\n  %749 : Float(1:1, 1:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%748, %gender_head.1.weight, %gender_head.1.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n  %output_gender : Float(1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%749) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/modules/activation.py:299:0\n  %output_age : Float(1:1, 1:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%748, %age_head.0.weight, %age_head.0.bias) # /home/anhduy/anaconda3/envs/age_gender/lib/python3.8/site-packages/torch/nn/functional.py:1690:0\n  return (%output_age, %output_gender)\n"
     ]
    }
   ],
   "source": [
    "# Input to the model\n",
    "batch_size = 1    # just a random number\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
    "torch_out = model(x)\n",
    "\n",
    "ONNX_PATH = '../models/age_gender_mobilenet.onnx'\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(\n",
    "                  model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  f = ONNX_PATH,   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=12,          # the ONNX version to export the model to\n",
    "                  verbose = False,\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output_age', 'output_gender'] # the model's output names\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"../models/age_gender_misnet.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n",
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"../models/age_gender_misnet.onnx\")\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "for i, result in enumerate(torch_out):\n",
    "    np.testing.assert_allclose(to_numpy(result), ort_outs[i], rtol=1e-03, atol=1e-05)\n",
    "    print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanR,meanG,meanB = 0.54568475 ,0.42776844 ,0.3761094\n",
    "stdR,stdG,stdB = 0.21924357, 0.18996198, 0.17315607\n",
    "\n",
    "def get_val_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=(meanR,meanG,meanB), std=(stdR,stdG,stdB)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "df = pd.read_csv(\"/home/Data/all/testing.csv\")\n",
    "img = cv2.imread(df['file_name'][500], cv2.IMREAD_COLOR)\n",
    "\n",
    "out = img.copy()\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "\n",
    "img /= 255.0\n",
    "\n",
    "img = get_val_transforms()(image = img)['image']\n",
    "\n",
    "age, gender = df['age'][0], df['gender'][0]\n",
    "\n",
    "img = torch.unsqueeze(img, 0)\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "img_out_age = ort_outs[0]\n",
    "img_out_gd = ort_outs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Age :21-[[25.087376]], Gender: 0-[[0.3627907]]')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATAAAAEICAYAAADY0qgzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAADy7klEQVR4nOz9e9StW3oXBP6eOef7ru/b+1RSREKumAhiBGxRGpAWAhkGRZRBuoGkY7hUNFrQjYJgQ0LoQQiCXSDY0o52MEqRDhJMIiixQSB2oBRoZYQ7zSUokhtUCIRUVVLn7G+975xP//Fc57vWt/c+SerUWWHPc9b+1vW9zMtv/p7f88xnEjPjVXlVXpVX5RZL+WhfwKvyqrwqr8r3trwCsFflVXlVbra8ArBX5VV5VW62vAKwV+VVeVVutrwCsFflVXlVbra8ArBX5VV5VW62/AMDYET0WUQ0iOh7iOhf1Pd+HRFt+t7Tj/Y1vl0KEf1RInpGRH9CX5+0jjYi+g0f7eu7VrR9v+2jfR1WiIiJ6MNE9Bv19UX/e1UAIvpyrScmoqbvTf3veeWlAYyI3kdE30VEp+/LBb+ZQkS/hYj+ZyL6biL6a0T0Cw+fv5eIvlE7xhe+xCH/NjO/xsx/OL331freh/WYv5KI/n96zr9JRL/ycM5vIqI3tCN+DxF93XOun4joNxHRd+rjNxMRpc//KSL640T0QSL6NiL6temzL03n+B495yCiH6yf/2Yi+lYi+hARfTMR/Zr02888/PZ7tIP8nPSdH0ZEf0Dv8+8R0W+2z5j5nwPwi9PrB2Z+DcBXvkQd5/v/54noj+k5vpOI/jwRfTER3b2Z47wdChF9HBH91zrYvpmIvuAlfvZjmPnXpNcX/Y+IvkCP92Ei+v1E9HHPuYY/RkR/V9v8LxDR5xw+/3gi+j1E9AEdq1+ZPnt0LL2ov+gE9n8nor+tx/2PiWh5mbohop93OO7reuz/LQAw85cB+NH5Po7977mFmV/4APDpADqAvw/gc1/mN98fDwBfDuAfhwDtPwPguwD8s+nzXwLgswH8aQBf+IJjfRaAbzu89+sA/O7De78KwI8F0AB8BoBvBvD56fNvAvDTXvL6fxGAbwTwqQA+BcBfAfCL0+d/BcBvBFAB/HAA7wfwsx451q8D8EfT688A8FSffwqAvwzgZz/n3r87fX8F8DcA/AoATwHcAfgnD7/5QgB/4vDe/wvAb3jJe/9cAB8E8G8A+Lh0zf8RgB/xEegrF+37Jn7bXuI7/wWArwbwGoCfrPf2o5/zfQbwj76g//1obZefosf9PQC+6jnH/CftWnU8fDeAT0qf/3EA/wGAjwWwAPinX3YsvaC/fJke++MAfDyA/wnAl39v6kb71d8AQOm9T9f6aofv/YnH6sK/95IN/GsB/EmtnD9w+OwfAvD/BvAhAN8A4DfkE2ul/XcQ8PtGAJ/3feik/w2Af+fK+38C308AduV3/w8A/1F6/U14eQD7/wJ4d3r9RQD+p/T6dQA/Kr3+LwH86ivHIW30dz1ynk8B8JcA/KpHPv+dAH5nev1uAH/8Bdd+0YHwkgCm1/ut19rq8L0C4Ev03r4TwNcgwM469bsAfAuAvwfg16Tf3uv1fBdkIviVuX0BfDKA3wfg7wL4mwB+6aHdfy+A36399l9/wXU+BXAG8I+l9/5zAO95zm9eBsD+PQC/J73+4Xqed7xEHf8EAM8A/AR9/S9o36zfl7H0SH/500jEBcAXAPjW703dAPhjAL7s8J619ZsGsJc1IX8hxHz4SgA/nYg+IX32/wTwYQCfCOls77IPSHSl/w4ys/wQAP8KgP+YiCbKmL7/JUT0Bx757B7Aj4cwjbekqLn3mVfO+ZVK5b+OiH7Mcw7xowH8hfT6L2Cmy/8hgF9IRAsRfQaA/x2A/8+V43wmgE+ADMh8fV9CRN8D4NsgHen3XLmHJwB+LoCvSG//RADfRER/SM3H9xHR/+Y59/Fmy2dAWOfve8H3fimA/z2AnwoBnO+C9KdcfrIe77MB/Foi+pH6/pdBBvwPB/DTMfe7AplU/wIE3D8bwL9NRD89HfdzICD2Tkh7Ptr3APxjADoz//X03rEtvzdl6h/M/DegYPDYD9TsfwbgTwF4HwRcAGnTbwTwFWqufwMR/dRHjvHoWHqkv5A+8utPJaKPxZuoGyL6NAjb/F2P3d+bLS8EMCL6yQA+DcDXMPOfgcyWX6CfVQA/B4KorzPzX8F84z8TwDcx8+9k5p2Z/yykU//ca+di5vcw88985FJ+O6Ri/sjL3dr3S/l1kDr6nem9nweZMT4NMpv8ESJ65yO/fw1Cp618EMBrCowA8AcgdfEGgL8G4Hcw8zdcOc67APxeZv6e/CYzvwfAOyAm739+OJeVnwNhL/99eu9TAXw+hF1+MoA/COBriWh95D7ebPnB+vfb7Q0i+irVZl4nol+gb/8iCKv6NmZ+gNT3zyUVc7V8OTO/wcx/AdL+NmF8HoDfyMx/n5m/Ve/Fyo8H8PHM/OuZ+czM/yuA/0Tv2cr/yMy/n5mHHv95fe/YjtDX73iJunheedPH1Wt8B4B/CcAfYeahH30qhIX9MQiZ+K2QNv3BVw7zvLF0rb/8IQC/TDW2T4RMPADw5E3ewy+EMP+/+dj9vdnyMgzsXQC+jpn/nr7+PYjZ7uMhWtG3pu/n558G4J/RjvsBIvoABAA+8c1cJBH9+wD+CYj5+VKrzw/C4T/8Zs6nv/83IRX+L+vgAgAw85/UDv86M//fAHwAwpCule8B8DHp9ccA+B5mZhVr/zCAXw/RoH4ohN3+nw/XcQ/Rk/LE4IWl/DkICH75la+8C8DvOtTbGxB6/oeY+Qzgt0CkgB955fffm/Kd+veT0nV+PjO/E8CfhWh+gPSP/zr1jb8K0Vozw//29Px1yIABBHhzX/vm9PzTAHzyod996eG4+bcvKsd2hL7+bgBQJmt97ed9fx33scLMGzP/IUh/+Vn69hsQsvA79POvgtzjT8q/fYmxdK2//EYAfw7An4fIIr8fwAbgO97kPfxCPNKPv7fluQCmg+fzAPxUIvp2Ivp2AL8cwI9R0+nvAtgh6G/lh6bn3wrgv2fmd6bHa8z8f3rZCySiLwfwMwD8C8z8oZf9nZ7HHt/ysr/Tc/5rEG3ms5n5Ra55xkyvc/nLCMYAfW60/YdBqPfvUnb6bQC+CjKz5vKzIfrh+15wHQ1iTuX7+KEQ7eVI2f+iXvdHqvw1AH8Lcu3PK98K4Gcc+scdM/+tlzjH+zH3tTxJfSuAv3k47juYOdftm7n/vw6gEdGPSO95WzLzz0h97c14aqf+QUQ/DMBJz/cyJbf5C9v0RWPpsf6iE/a/ycyfwsw/DDJB/Rlm7nhB3aRj/yTIpPN7X/LeXq48TyCDaFZ/H9I5PjE9/gcAv1W/89UQVvYEIth/C1R8g9DIbwbwCyBekQVC73/ki8Q5/f2vBvA/I3laDp+vEPbyJyHerjsA5ZHvfhZezgv58yCz/sU1aj38pHTeXwkB8X/okXP+Ygir+BRI4/1lqBcSMkt9AGKOF63X/xFiFuVjfB2AX394r0DMrx8EAc+fABnQv/TwvS8F8D9cua7PgLCZnwZhQ78cIg2s6TtfiOeI+Ajh9dMfuff/I0Qg/zfSdf4Ibc8v1O/8cggwf5q+/ngAn3M4fhZ23wcV3AH8JoiZ84MgE+hftPbVe/ozAL4YIvZXCOv48Y+1+0v0xa+CeNueah/4/vJCfgjC4J9CnApXvZCQsfUz9H4WAD8fopf9WP384yAa4rv0fn8uZOz+4JcZSy/oL9Z/CaK1fSsEBF+6bgC8F8Lsrp33Wltf9L+rv31Bo/1hKFAd3v88yCBv2un+IMIL+ZsAfP1hsPxByED/TgB/FMA/9ZwK/EOHTvAAoan2+NJDh+bD47MeOfa1DnTRkSEeq+1wzt+eOtxfhDgtvhPA1wP4cem3nwkxEe01AfjN2pH+vj7P7uN/Tuvsg1qf/wmAJ4eOsyMNBH2/aNv8fb2+v651R4fv/TUAX/RIffxsAP+Lttv7rnS4iw6EGcA+E+L1Wp7Tf/5FCMh8j9bXn4OA/tN0H78CIj5/NwRE/73ndOr3IQDsCYQpfACPeyH/C63X74K4/n/ac9p96ntX7uXjIKbThyGT9Be8YOy8EMD0/S/Q430YwNdCvbD62W9Pfe9HQoT779Z7/gYA/4fDsT4T4o3+Hoi4/5kvO5ae118gwvs3QSa9bwTw895M3UAm+w9ALJq3DsC+Nw8IgH3F9/dxvx+u66dAdIIPAPjp+t7/VSv9AzaoXj0YEM/xd0MnIohZ8wGtqy9LdfeLPtrX+nZ9QEIcPgjg39XXF/3v1YMB8SZ/UOur6ntT/3veg/QH3+tCRP84xKT6SxDz8L+FzJK///t04FflVXlVXpUXlPbir7ywvANC1T8Z4pX4rRAq/Kq8Kq/Kq/IRLd9nBvZ9vgBZ2PrbIMLjf8oS2/SqvCqvyqvywvJRBTANhP3rAP55SDT5NwD4V1gCYl+VV+VVeVWeW74/TMjvS/kJAP4XlkhpENFXQZZ4XAWw+6dP+WPe+YMAiPMBzBL4os/HGBiD0XsH88AYAzwYzCMJf3Is0n8lJl7+EhGICkAAUQGBgEIgUPpMntvf3uU8vXcMPUcpFaUU1NpAheJYFGe2q4i39BxF/hYqfs5S7LOiX30s7Eyqwk6Rjy3/xzXYcea/01XZEQ9noOvvM8Dyzyy0elvF5/nXRPA2cWGXGUPbs/cOtr8m3Kb7t3Yr2h6lULoGbfcxwDDR2K5xxDHsH73eMeScg8eh77D3tYubT885HZe87h/7/vX3rtbyC7nGZbvxxfsvLvu+/T1m/vg39aOPUvloA9inYI6I/jbISnkvRPRuyOJjvONj34nP/0W/BMyMsWvH3ncBrm3D66+/jmdvvIHv/uCHcH54wOsf/jD28xnbwxnn8xl979jOmwIDCdBQQa0VrTUsy4K2rKh1QVtXlNbQ1jvUsqCWiuV0j9Ia6npCaQtKbfjQd7+OZw8P+MAHP4iHhw3PHja842M+Fnd393jnOz8Oy3rCenqCUiqICmT5KQGQ8xIRqBYUvYZ1XbEsC+7u7tDagrvTPdZ1lcfpDqVW/50DGZGDZx/Dw5NrrQJ6taAUfdQKKqSvDWgrqABF3ycilAQqQEQ8O3jo+/a5TRZ969j2HXvveHg4Y/SBre8YnTGGTjKcBjkRxpBr37YNvXfs+4433ngDDw8P+NAHvgvPnj3Dhz70IZzPZ+zbhtaaABcYS61YWsP9aUVrFfd3K7h3jL7j2cMb2LcztmfP0PuOvZ/BvYNHx/n8AAKjlgIiBojBvKPvG15/48M4P7yB88MbePbwOvZ9w3Z+hr5v6PsGHh1gFlhgBmE4OPLoCmGMSgWVCLUBhQiNSIFTfmP1R9DJmANurJ4H5PtW5Ya7Nin4VFwKQBGXbsdiPdvzDC0bD1a+/e/8rW9+/Ntvr/LRTmh4bWqYJx3m9zLzj2PmH3f/9Gk0jM7QNiPG7Ah/79rB6Qp7ifeMaWmjWueg+I6dlwdj9DgPkQBU/I7AdG3SjHeMVRkQWUeK65l/nVzPUXkJxEDShwWci99LLsXYSgaswghcMraE6VwvKlfr1djHYYBMPIEP9fHIo9aKqmA71QkgUKDfG32gj4F9DPAAmCkAk2UgDz8lTX+QGDpnJnW4B+XksP4CFP1LU/unwwbzT/UQfXmuB6t7A8JjCfB6vFxrjx+I5aPNwL4N83KQTwXwtx/9NjMwhs5Ew02LobO2P3r3mTybHlacgVFJAz3AJw+c+JzcXB1jgHsHpcFRSxEGxcCyrljXBcsij9aaMjBCH3auirY0lFrlUYqyrxWtNrS2oikrc6ZWxESqtdioQlUAtE4/eOhghpujbsaWoqBFKLUomGmduHknAycPQWcEZnodrccprmd4vdVWQV3rDQOGFD75xBGC6ZVgxMyMu/snKLUBVHBeH7BvZ6nzMTB6Ry0CyMwiHWzMYO4YQ0x6kDJRZWwdAtZjVMxmFqPrfEgoKCRMt9YGgDFKB5eBWqsAJo+w19UOVis01ZXWJmcb8pK5EgDiDKbZzk5AdAQ5fRbAyj6hjcEvwjj5zWFyubXy0QawbwDwI4joH4Gsnft8aKaLa4UB1bTYAWwMmXF77+g9AM3eG/odKxmYXFvyx3U2FI3MDp4EATAeMqDJBh4TltbQDg87NgOqa8ngKC1Aal1XtLagloZWmw6g6ubfdD2lAMnkYyIwGGWkeyg8f181olKKmk4y+J1v6IBk5mB0qo8VIgFHA7k0OgK81AQiBaJSQSB0+51+bwwBuQtdUt8zEGNmrKcTqBSZJGrFvjXs244+OvZtUwA2Nk6A61YDQ6G4lCLXxgWsz6lU+a6ZbwgCb/UlbSTf69oGXIq2eQHI7DlIvSmYXGeuVreZQWWzMSaLiTZfTBbDP8j6lmCmgigg4I1gYtmCOI6F/PmtlY8qgDHzTpL14Y9Awij+M2Z+PN8XszOqfd+FeZ3PMvM+nHF+eMD5nP6ezxi9g1Vot8Fhj6q6lDGkUoqwMqooqChUZTbWz0AykPrOYNrBEPBkMEprWGpDWwh3d3dY7+6wLicsi2harDM1E4OooLYFTdmZ6F2if9WSrkUHsoGUFTGpCuqyhB6mA5MRYG2DEVA2Vggt6W5QxtlV0+E+wmw6AicCxMVyn6KpFUDkt7UWFAJqqeiDgULY9w7SAU9E6Om3Ajjz4JI2kfvvvePp06fYHs7YtjNef/3DOJ/PeP3113Xi6hj7BmCgMIPJNCgD0ybXz0CpAiS1dIxBGLzDsLdrnaEWVF6gyj72UpXZKmwM5XKOXwxQlfbtZtQqOwPEdB1yfK/gcWkeusmfzPlrgMgJcI2B2aQAf//55QherwDse1mY+b+FRO+/7PdD91J2NbQT54e9x2MAxtoAZRzBuCT33RW2lc1IZWzW5YRFMAZECwNDZ3kRzFtraM6eqv6WlA1kL2VFbU3MRv1bqIDU3Cx05boAv3YX4JUxhPF3KMmkFM2LQJXUbCEMknvwWXv+qZqQSW6ztohGcUbhzgVSYRnKqAqDC7we3Sy3wejHM4YcWl5rDW0RR0prFXvfAQDn8yanHyJ1YwB9DJACGKmZLPfPyoIrUCB1DADdgF8HP+z7NnFV1DpEAihi+lPpwsBKiFsMFkZuv/dasuNG3+VUeQxGUb1UWBRB+SIIgE0pF3B3YMBZ4kgc6/IH6bKEMYYccYsQ9lEHsDdbyGb+pIHte8e2bdi2TTxVSQ/DgAKYH0EZhQIO5UdoYaWYGVFcC+kAMIRt7H2gMwsrI1LG0FBLw3o6YVlXLK2htszeCKUCRBW1LVjWFW1ZsN7do1XxggoQ6pUSgWoVdmUmn/2toRUJU1LTYermSfsrwdwMIIDiWk0HwykFkcIth+gvVeeyjA1CA/OidSv2pjJGEg9f4wYxn/qkdQWzIDNiffCXKkc0/QoE7HcbtvMZA0BtC/bBeHj2TDyYQ0zGvXcQINpYNdNMB2gx0C7CxKiryC/gPQyAIEBVC8BVhkhtDX10UOlifuqRrR8KeAGlVPCgaSoxpjRgIDbU1DVQYVARGSCbkgZdYTRaG19Ylv46g+Plp/rKweu6Y+WWys0BmBcFsp4FezUrTfcyreFa47jbO/Ft08UEJNScLBVURGgfXX4zhmhsex+oragZuqDWRYBpES3LgafKgJBBJCZiWxTklgXrsqBW08qOpmKwNdPKxCNnpqVNoMqyfMbP4AAZJARhIgUOlATCYAKYMCp5XydiFEKAF8wcFRYpsz5jKLuZvGiUmGoyFc0kNVM+mNfw9jRANFbhk4m2h4W7WNubZ7KUKiEOLCEKBMbsGRZTFoVQuIJbxeg79lKwbwR0MaVZgQwOfBqeQFUmqNpBYwAkXs5RtO4AyPRhVKrAuKWI++QM12cBfxH1fqS3/hMOKyKXx02/N2kWUgDnLZWbBDCjyyYI99FdsLcZMQbUQXBOoGVatR1z8j5SAjEdCPCBBnceFIYMrCpmTluEVdXWxORITM60r1Ib2rJ6zJeAn5iTNN+o/NZAKz1mYT/rJ6z+PhmAzgQc6Hwcm82iwZ8CFA425EM4rknBjEsI8DZDjFTnNnDN4QK9Hr9OAzLokGeATNjXQGRrEwE81eBKQUXFsqxufrqDYjA2KsK6leH4pOHgXlwrJB7ovboUIceq0sYoznvcH0tVWFmtoNHA1F0PG0LFvLZYgc9ivgK3gpWZ19EgyT+5ol/NDhM7wOX3ot9cec5XD53PAmfQN1RuDsDMZW6ic9939G1H3zbsakZaGMUYQwbHI61iYMZM4upOWph56oT9yCy/DRYxdgz0PrD3jraKIL6eTljXE5b1hLosHjZhPaJUMRvX0738XU5oyyLvL2IGKqro/3odtaK0qgG0Ta6lVQmD0FguGcS5juQvOXIhmcgCODV9n5VooBbv5K0UZV/DjZqiKDgIWmcaP8ZhEvbegx2PYFNyPeSa3VDQwiggDI3fAlgB2HROGgNMYvDX2kBtwRMUjDFw/+Q1nB+e4eHhGT7YGh6ePcPgHmEuCthD+w0V6fDGLnvvYMj9jEIoYwMTNAJfHCJqSKNQQ6sMqxEeHZ0I3HdgKPixalmcGbBBQwZ3IJAimOhg8gnVfjdPvmZqZwNzkrXmQunMj+EjhYnCNwZewI0BmMGBB1uyzPK25GRiYcYQkvoZnrRo8ln0Ds5Bajpksd9/a5rJYJjgW2tTQX4B1ZYAQ4e/MiljZ+aBtDiwR8V6j5oPILV4sMy+5Ad2s+SD149jx00mJ5hFd8lmqNZAMUaXjpdrKF4d2khZ3dFLmUFZYtJENyvMGJT1GJp+N5hF95SDu6jPzGiteeOu6zPwYCzLIn0BhEIW5a7nT/daioa0mESgDwFlC00pbhKSOlcKGBhNWC51NS+v9yG5F1LY0bq0SSVBzwwqwcwu378u0NuR8kRxyagMpQ5HtX5ibf1mzM63QbkpAMvF1jr23tFzEGsKZB0qrjrZSAMakPWJMai041ABUwUbaKG6KQn1DyUdFgVF4raWBet6h9P9E9isOpigqj1aW7AsJ6z3skRoXVaJAyMLloxrm0zZYrpPU1NzDq1w9uVgY1rX8ThhEyascOFe2JHB+Rw24frIsW8fTHAxrbUeWYJqg284cggD0gNWQEItOIA4A1gGQruXZfFNoR0Qnz7Z0GrFtm/o245928Jp0IeiMgT8idA0xqLUDaUuKAModQMzoVT93QCIGhikgKsT2xA+OkoHDYa4QIo4UNjcHwFiXc1kqy+RQMwuyAG9UQL2kg34ksXq6h+EcnsA5qyKXb+wUArWpUU0IJ0sCfgWy2QzpoUhlDT72qJrISe6oJgZex9gdF24zQAVlNpQmFCbCPci4IvIbogyWLyIwrLkM4muF0HewiUYMXjpELBaWkFpRY6tHk0JSgVAQOcBGsYseAKZzDYkGh8zhWI1RkhNEzJdMA3Bi/V1Glmupjkn++SSNeTfGTSG3uMAXRuALlHuY6DUIvWsxzMv4a71T3pzbooWqZ9lrHjy5Cl2lRN638F9YN82SNDqkBg2YnX2dEiMszLctuoxe/Qv96CqJjaEobGva9XQCq37AdNI9W55DqmgyfwLY1BEfg2hODIoI7HGijOztdNkrMP00fUP7PMD67+1clMAZpqx9JBYTmIPAzCwCsOma1loAoV5KGaYxfkImBhbknMNdxBg7+gDHjohZgdQWwCYRc6LWVNUtxEGVlpV3UqEevOkKfdxgbVotLyBmCwbkt/WRbUwD2oVcJX6GCLMlwAGO7TFfBXFbnlTO79WlyRwIAyK0SJmpLEFKKBYGyiIyVUHbAkBCdZleiUMitQcQzCEWqubabsuzK+tAb4ETECyd5ZF1FpPztagAKas7CngITX7dsbY5Tejd/R9c0/g0HPJxwQqFa2tGEXEd1KazbwoUA9nWO6ZhHgnSePB5M52DJCvtySjfsqMQbNsb7VtoD5gFRiTgs88zChUJKh2Ghhz+IzVe3kElK4xtFcA9pYUdtNwT+sfzYR0Dew4Q+FgUoFgEfZVgcfWI9qq/t6h0dcdfeygEsHThIJaxXSzSPu2aiaLtroYP1hBUoNUV437assiQr9qRcYqjJF4BH4tWJbq6yktzIKI9D473FvGjMFApaLU00xLNSl1zB2q5WL2de9eqnMbUB7m0PMB5B8PpITxjAqgO5s9nHRqDzO3aq0YzKgWzqDpkaxN7ZqyB9ZM+rvTHXhd0U8n7PuO7XzGs2fPsG9nWXYEqLQgrGw7b8KACmDhFm0hjBHhJZbSqJddWSCAPsDYYZ5GqesKoiEieKkJgI6rIwjhq7TvWFaLhPypboMBXwLUXFjZM10BKJ7a7wdSuTEAQwj06W+O+7LIeCvHiUUmM5rAIpttNhsyS6B1HwOEAeIuZgNLpzKPXCkak1XDFFXqIqZdjTAI/5sEY3l0GAjYkp9Swwsa12mDKixpwRcdJEy6jCbuO4u09l644pN9gpfr/HntohQJO5Dj2mc6MK8ONDhY2znFnMd0r+LVhACDaWC5EceAr3GEBbym+DjAl53V2sQ0LUXZkXhIQYzC5tyQUIkCCUYttYFHRx0yRCT1UkWnqvdrjh2Jh5PZQhgp7LVpgL6YO0CK/B+rZTUufb6QejdJAx7WAW/Ma3XLnNs5phQBuNAVw9LgR491C+W2AMzNR/U0ZvNx10f3FW0+CKLISLalQVSrPEosKcqPwcDoDB47mLqsqStFzEJ9GKNamjxa1QBWArioBrY01FVYmgW5ttY8N5Yv4SlAbbJGsy1V/4p3sygj84HPJKsCFMXEK6ZzfiF5kOkpl0Bu1cEAWMcegSTDRlr8bqAk1T78+VGD8eaxJA0IRuleYz+mXY+AR1XTV8xvwhgAoaKWgQ1bYmN6XcMybnCcR9ezntaCfd+wqKeylALeJeAVdhxWtsRQwd2up/gyo6oeTsYAlSJSApSVD4kxk8HfwdjlXnhHJ12TSAygAzo5yVLKMO290pJJ2RlT+xKxT4ayaoKB3uP31FPd5gae+LNMcGblW1+zCYEZJaPejZXbAjAAMxOwUIkjNZ5DEfQdZI/jRdiCifym8Wh0OXfNakCE1jQti0XoWwiECffKkqLTpcSBHhWevYcF5EwiMZBqy5ciaJVK7mDswr+bhYQ4JhnzSiESkAHBaZBc1BlMWDZxXg4s4DGDWUSFi1Zk37bvSd3Lce34U7R+ZgpIQKReVyCCYw24puDVQ/t6fyhRD6ZJrus6faeUEutkbYin/jQYovFRQSlyLa2F57MDGH0HMFBKByFs86LrNEfRbMCpfgbb1Bhi/BTO4tVDRppggS2OL95X4cyO8+xwLBzhsxMDm75yu6blTQEY4xK88nKVKebIZ399y8MiNMJ+Mhu9v0xgyGBsvWsXJ1BZJMuC62eyBKjUFokJXUdTwKwVtTRUNTWP4GXXCQSAHZcNSdBq7oR6RZrRxYCieMxYjhOjIJWQ73pXTUxpDrzUWd3Y3cgifoBYZ0A8AfZLchPIANbuS7BiHkAOhAfGtizSNgZcbv5NKyySeazfAQhcq7BOImFRzMDplHQzeKqevneMXVLzjNGFbfsaRWE9pS5pUbq0YS+E0XfRx/YdhVRPI0JXp0EZTSa+Eb7GwRK8Y3H+RISiotjUAg78ZjFEuxij9ZUDLMuaUnN63cIdAJcM7PjXvL63Vm4KwGQyG5rU8JKBBT027qCDiBFLgw5hCiLqi/mocyiIC7oC4z40fQwAWroCUQpKtWVDVZMWajI8WUY0ex4tq2hkqJD+VYvOs6WgLRpmodH5tVW/toBVjk5edKAR9JzFQcwYGCPSCc4WNcE8bF7BWoZOFs68nFlZnFOaudm8YBq4mYU3rVNzJjirylYLW1BpQSnqQS5NFkkXRu8DtAM8KECM5+uVQSqODdOLWmuoVNCKOEJsFUBrC4gK9m3Hw8MDcH5wLdU0VHHUyLWBCyoBXBt4XbCfG0bfUahi3zdQeQYGY6eC0bt4/5jRu0DXYBa5g8cU9UVILJUhDBk5hix/UyZhYW2MUkSfZSIwCyQmTpycBKmG8hhRNhsroNzYxC2V2wIw4ELEB+aG8QZKv8mmlL02o9KPi9AobIAOliBLYwmDNaO9MqjsubTIfTu+pcWxpHiTw6DM4Gl5soJBKYuqxUNADKjl+qKj2k2FyagmRtgc3qF93Psnh+cu5hIs2d8w8Io8zC5F5rofDihlOp68KGpG8RU9JgI2qRigaqxVIQ8ZYC66sEeZWBqmUeaJrBCBa0FBmKQm7C+LaFP7vqMrY7b6MRMvPIAEqgwUAQtiRidCX4Sd195R6w4ejFoaWFMm8ehAKeBRwDTSVWqbMHT5ToIcls/NisiFYEkTzcQNOm0TilT77BGOb6SaYnOyHOrvFYB95AqzbPzAzFPkfe+hlUzMDICZAvPSHmlUo82kQi0dzmXWmoWXwddLFg1MlRQ6har9SJq/EFBFWK5NBPyyNBGaWwCamaxIHc/MKPNaBqhAZlkHiuBkFjMWyQev1V16cfhCdHiHOrAm4YtlWfm34dqPK4mJw77qElO+DkAXPxsAyz9Zn4z6D/2r9y5mJI/pO8diS8n0xhxECyra0KVeFHFSkr23a6qcBozuJp3cVYRAQJk6lYZSCa1pwKvGJIIEEJmCKQqzHOpUlFxlkqYoMVA7VwKjSQ9zfJth6bF6uPQkP15MLsigd0vltgAMkZE1B686/T8wMWnIy5lMBgajjBDrkb4vheJPElAdZCiFX2TtR0elxJmluK6SBfmiLn3IYmUO8HLzK12zg7N39BTbBVz+jiYocvbl95N66vzS9DI7f5iIjxUhAeT0LhSfxLCSVmm/ifqNIM5rEeHHeLEjyPl35I2JoXv9Wf8AT+I+D0ZrssvR6A17bboagYWB+mJ0zW7BaoynBJmRLUTXZ1bR3UZtas7KDEgAeFSQZvMyaRIuByAmQK2/wRLmYR7mqSku9ICXLTT9axPHrTEvK7cFYMrAAEhe9G3H2LvnALtkX5BWp+js+Tt9dG+2AoAKp4EvPayoGWOdNTSmyIYKiInFtnxEfPFqPopXrWo0fqvNwyEKkQvOXp4LYHMENiFYFzRtj3FKQpgnnMaI3EgGj8SdNGOQaWN+b1faYgIaZ4h+AgGz1BZDVRqQetsyKCFeP3aea+ZxdsDkazHA8rqzrfeGAhgKlgVqjoqADzBG39E7YXRg7JvvMTrGEGaGgQJdBCuzDmxZWVtWEBX0dYd5FgWoCMSMMYoYgNxBLPnKwGawmnktRRawy9Z2nWUNprQLe3vxoW5eBn+8fjngyz/TNri1clMAhsHo2y7go5H3OQuFb3emm0jwgKeZoWQCsJqFookqM1BnjoWDBQYG1bHdhCJXmARP2vpKJgqIUZ2MNBWxrIOM9Yy1VI3TYgy2pSaHQewAIv+N5PaXYM2qaWLIN+mQ++UwT+UOUqyZCcFHOxISW2aM9WpfJnjuYyio2DhWxia4YZNF1OM0qRAAKs4e7SpdWD6YRDFQEcCX2BcAZUdzsVCJXSc9ZkalCqpxvDHW2GRk79j3Mx56B7OI5Nsu+0jy6LA0PMSSIUPAC66VERW0bRPS1Qda60pkGdQJGDu8c9l6SwyAi7PXqDtboSFLvDxO9hHqJfrYi2mZYB17e5EGRsOb9bZA7KYAjHWWZMacPifpXw5ezrR08OiYZJ39hZ7n9YRAca0LSuez/QhYBguoDpbziU+MyZhC0WysyfN4dYch7TgXbEPvWv7E5qkwHQUBHpb+hrJJ4rWWXjGnc1q9kDPVyUZJDIdJwetCQDv8xuoY8Pp/tEz3aUzMTCXOh0wmcryegE1/aKZdmK0pADaZ/3J9w5dojTE0Tc+QdbEKrBq3KvWmtVk5sVYFgFIGuLI4doYEHksUPzvD415hCcMiRizb9noGO1eyHMDmUY7vzZVPh9cXVe1tkwOAoz71eLeFXzcGYMzYtgel9meMsYE55f9SF3hOohe/BUx7sMHah7EOAaLOYj7QYHSWnXP2sSurKuiDJQ++RrkPhGeyVAml4FJBbUFZZCfttq5YTyvaKjFjYZYlXcUWm+v6Ssk51dUDSSiyfYiYGsyirRHgezCSdd/ZpMozun1QLCtFLpbrJQGHmSpksXQp7iHXbQ47kFqy+UC9l0BaDZCDWdV8ItvaLQGetpF53MQjK/m5RhnCXtXMMsbNfWitimYl+UMiCNmAYh9qvlnIBKAZMWRbO8lKO/Scupdn79j72dFsuBmpS8AwJMkhFZS2oIKwQNqp77J5SKcqZGuIOSvGuyYLmLiTSBaFoJt9EK4yYruni8/MGXVgvF7l2l4F8T3//e3pYDcFYOAk4ns8kM1kTpycUclPlIWltWXOmfW5rH1UIV5ShQqYKRgOBkDDl/4ElbdZrPhjSgG9LGhL8xikvBRoFqDz8yPDsc/gDCVTfv+2MSp74Z+RM9BUkZO54Ldk9akDQ26t2IduEgYryH+CQTk7yAF5s10OA69ronweSKQmcilF2nBAAQSgoWs/9Rx2Dwag6VReD4OHmFvmADJGWopvYtuWBRaRxWNIdP15iF7GaQs57XA2J7Iy86LHYt2urpcKrqyTgYTNDGRz2R5ZTFcWn/sZjGMVB0CwdQ6KzXHBmNi93T9ye8x9xmvrxjDspgBMvJCqgZn5mPfgu2avGLnwwabmn+/1KKYRG1iNrmllCB3AblHoDOz7jrp3Dw/I+cRAErfVWsOi6XJOdyesq2ShMA9kLhdi9GFAOzQ4zde/MDe8QxTmnpmfB/Oh1GHtSECYWzYZGPMBkZpXUssyaaSwCh206cJ0MMswy8xhOqsNuKPmJxczOVqm1Dkc2XevReUDmKzc7DG2eDLXSffdOwWRprpeF4whS7m2c8PeJEh53zcUIuzbGdt4wGYTnQPYCF2/hC5JBIxS1IQklLqrxUCe0dd0zVgpZovBD/XkM5It9bL8YwPEZYK+F5W5zue+cmvltgCMJf7LACxSSF+ajNPvMDeRmylqvjDS4ABsTxsJZLXOClsTqNHUYtscNtyIXF/LumJdTliXBet6ShpEDE7roEUZnQGqbhJ02bV85szaxZXP/f1Z7JbHPOizXuX6odoYtv6SAM2PJebYSCAmy1mMf1k2DIZpNjbpBxiTn1sYHukEk8zSKyK+JTvMZqgtMQJk0b3MQvCFygSgkuY5I9KcYpp/X3P32x2WIuYfs6TsrkTYq2yttu9Vct/zkJ2MdvUeG4aNmCGLmv1UdckQAfsm2lrZgqlbIw9NFjvUQZACwA5NTtMbpodJMksLvDW2fa1TzPWfC0cj3Vy5MQBDMiEDvAaPxwHs0GAikIZQOplRzK5riYIiIGbLLbJAjGTaxN6R1WOMlpQGemmLqDMcO+646QLAvFiWf8pGtzgFLxmT2QU0/X3s9unieQbTDHJuHuopM4DBQ0S0ftzaY78OII+DIyM8Xph9PH92wdQOz42VhTMim93k5hyYfXIqKhNAJyPbT5QQdW5pjMCqQ5J4dgsYtRDO50XAa4tg4Ws9jvQ3VAqIOwZY1l4O6RvcK4g0Q4WzWL13Y7C5LjNLTayb9W6nGECvp+tj4Vo85K2XjziAEdEPBfC7AHwiZBi8l5l/GxF9HICvBvDpAL4JwOcx83c992CJgUkUfpc0w/0aA6P0iMFZ0h6G03o8fW1ahqcVI7gncVlWLG31BIO1CNta1xX39/c43d3j7skT3N3dYTmdPG2ORJADwGWyRdKBAmDKOuGggvi+zbS+WLvErkTThrizpeq/fX7VXqu//NeGTOTq4mHgcf3YDvhXju9p/BFtcwyNuMbErg06KoSim8nuuX4ZCDqb2KTVF0edihlXzECTvGINAEv4xOl0wuCOfey4gywE3/smaxzHLmEWGLKWU0/OVCHpeCqoNLS6gOsAj45S5TMMn00QIqJoZASSpWyaX8yuHdoS0PsQp0AKL3kOiP1AK1e6+vd72QH8O8z8IwH8RAC/hIh+FIAvAfD1zPwjAHy9vn5uYSDMOMvUmbbumgdBmE2ADZLZhNOJ2o9t7wVykQeu1mImYiQltL0cm4LY6bTitAbAZf3mWsmaVywD0mu22Tnfk/6bZ95rD1x8Dv89WWXoXzN/Au7tPyChz6FObdPfuGakr0b4SrCh9GFq0CxiyxHyIE0tk54ncM8TwQFE49zzKec6n82v+Tu2M1MELzeVCGSDFk0j3ppm9U1rYtOlW8vJ+dLaWQuKdtadID1dt01gxoztXkRFSKwr9XdCnphz/Vnd8FSHx8ctlY84A2Pm9wN4vz7/biL6qwA+BcDnAPgs/dpXAHgfgC9+wcHchOzdUkjPgq4LxyRsAVATUPMxjN5l5nIdKsAMIDUfAdTIXtF03eN6d8J6OqGtq+yqva443d3h7u4J3vHaO3D35AnunzzR/R7bHKl/AbCJOaW4ML1iH4G+1MmvMAENRfqcF+uwqSPbsRlgJpVdIrGg7bhkoSkxaGTQL0vRiUMOJYK0JjtMcVPj2mCYzFf4lmmiu9m9H65Xr/848KJbHL9L0/ddcwSAUlApsRVPI2NZ5TkdI9jasogMAGL02jB6x8O5SD/cCkYvGvC6S8bXvkcq7IPUwNzQWwNBlxy5M8pWhkSmWZM1ZHF9QTNP46G9iSwxYxYd0ux8pX/cGlhdK2+pBkZEnw7gnwbwpwB8goIbmPn9RPRDHvnNuwG8GwDW9ZQAjDH6zL5CK1AhlSIx3zATQtl190h8qL6VTcig6BKMqkyrLmoWahbWRYX6dcVyEnA73d1FlorJFIprnPQbZ1+mhUXAZNQBpuc2YZvJmb19c2+9YhZmAOB4TYCHTHCBh4uIvmhfSzFkRcC2DwNDdjDzK7liAh4vizUlzPw9u6IjW7g8bjxPjHMSltJ3KHiaZ4oF6y5EqY7ShTjjLFVMwEXS/YxCGLxIdl8AO2netK3HtevDwjRKLaBRQKNOLMxuztaUGtsaY6AQp+uSiaLUEqsmrmig/orMN8kXXeEW2da18pYBGBG9BuD3Afi3mflDL9JkrDDzewG8FwCePn2Nw4QcKWD1OPsmYZczw5JBQGzBruTAxlAAS7I5wUzI6gt2a11UpF/UlFgFyBKA6f3CzFAPO4i6mAEsmTTxvUutDIjF6ZPZ+RwhYDaz7MjztVixtZk02Jc5+bUbgMHMFcswOpuEk9mr9T6bt5fXlQsRTQuxD3dzUSdZF8uCNhGlBfDp/k1DIg2CRb8iyFu7QbUn0cS4KpK3Ibt5j669TZw+6ADvh/sECcPmkZaWdf07PFmiLCcaIeAzI2OXSx52sda/nqN1TUB/rR4fYWa3VN4SACOiBQJeX8nM/5W+/XeI6JOUfX0SgO940XEYIt4DKdo+ddjMeGTtq6YHBltWX1DRNWZFZzOQ5nmy8AnIIDbmVZuyLjEZ19MqQv3Tp3jy2mu4e3KP092dJiyUpIZXB+eBJebM0xY3VIxeQS0bUrSF5KAy8CpVN/0g6O7TwYzCDApTw1gmARr8SdGB037yZPd/bD8/Yoyoq/hjpzyAlV3ckeR4fVg74jrgYgIpOGNldQ+b7sRDPH4GtGym7cE41BMFs7P74zQhps+0kUCsYTO9gsC6S5TWCzFq2bHzQO2SmLGTpuvpjNGlvWQJGKPvG8C64QgIPDq6rrQYSVVkZclMEmqBGm1dlanKJFiAku8pWNfVtnIknCexWytvhReSAPwOAH+Vmf+D9NF/A+BdAN6jf7/2hQfL5sBhbp1nvTRjuVkYrZmFUWZ7Lw/ebN7ZxrSRXXVpi8Z3rVjWLNhfXO7V3iMS3Sy4hvAqHavANA39TQlzoaTdifwBxOY37P/Mx1BxOs/b82YQ8OD5453k8CQ7/FSvXm/xKntG87f88ugAYoc6guk5F6aonSu8cHZjxmjZ6zcBtLFhPab9F5qg9IDoO1qHuZEAMdvZvLEFXCsKdwCaoQQM5gqYZKAsj0bFSDskSZ+xfSV1hcCRvVk7JpBPFeSTt//VX9iE/XxTkaa+cYvlrWBgPwnALwDwl4joz+t7XwoBrq8hoi8C8C0APvdFB8qdyhiYDEqNStbUxCMJzzJKIl8XGL436T5kQ/gwiwjQHbprkQSEbREvY11XLOsJ6+kOT548xdOnr+G1117D/b2K9qofGUOMThWXcfSU2fdIGZjn9zLUcSs0ri8ALIGXjy1+VAEzMLGsBubpDF1J3h/6hGAphqBrGm1wk3/P4zcZbv6yMsbZspS26kd2pW2RmenRKCqkyWbSb02/stz2hciZth2dGd4mXBjHyP1dPRDz+dLENi5jC03jY929qNSWTLmBMUhYfy0oldDrLmbmA8C1YZinkYBtW8EAyn4G7wPMBcUi6s2s1DTRExmf+g4UXQsKMQZZtlbdeUmz6k4OruPvMTPmW9PF3gov5J/A45b2Z7/Z47lITDGjhhuanaVZZ3O/Hie2wTGoJPeSDlr/2MAnlgpV30JtRVtWrOspmY6XOweZyWXmmyGFT+ap0xBFXFSx69AvehiE/yYWZBf7McJvNfT3xBJilDmIP0+twekvs5iktnuOX6/dgn7zso/HvYHcuZ9MRTGJi2uBbP/HESZwkxuIpo5F3cwGePKJLEtU8wnsn1lgKhmo+k0Yz1ZAvlYP+aHXzKnxZEMWiLYFhiTPbygdADeUQQImAMYg8FjAXVcvav6xyE5SgSpu3z4o8rr5npJa52Bhe4PBJVkPXl+2ic3w92V975wWdwIxhi6kj15ya0TspiLxrSGlWIWrqVWL7JqcEhta8Ch5uNsMMA5ezKolIQBHAawW0bZabSl8QtY53t/d6YYeKec9RTLiPFht/GQmFSI+/Pz22tgbJRCz7D0q27mZCQpDsKTzVL9bgxTLxkH+dgxc1VmYYKl6fL8HyPmmPYMPT/ze7IwUxyfYjykWw2tlk57nKM5P5qner8kBxrBE+xqyI3mpGBi+3CjYKSkJ53jwgGXgkPXh7ADFWj12Td6WeiFmnnIpqFRkkiAC0ZB9QMGy1lHbanSSzBeWwaRL0KssjZINXHaW9bfSdzRbr4apSN+xkBTNPuIAlZkVeb3Ee/A+aH3riE+U/st95VbKjQEYudlgmtU+um88ATW9bH1dYRaNASV2FipFOqDOWkyynARUZcPa2gDd2LS2hmrrGnVhtmWWAEm6HbDmFYMkwPMdk3wOnzuObY9m7n55WAei+Tc+CAPggMSKSuqQNBG9g1lE/l7xDh7ftYESez9ajFiYkhYcYMAG0mMRMGzPDsSx7Ylck7BkA1cmkh3PjUHDjLB0fr9iCmDJwrsPXgH2a9uCZW0oPziFLmBInJaFlQztSIQigDN2AXUmzc6rNcAM7uq5LQXMVXXxJp7tDjQCuMju3aNXMBX0ISl+lnUFiLH3DR541xk0OvZutS39wdh8UFr9iafEme7av2fAlp08F/Vz4+WmAEzMiOLsiilgImZL67QKYBAmxSa4aKeeV0JqQ5Olgragw+LCffV9GtNOQflh5mjeLUkziU3iOeeNNzyhsnzPzeG4rgAy8xEGqE39z7AvgQIQM7ChwMGiTdd6AK/DYaehYuxJP6Tp49kUYVwfKJP3MQnr6aLDVNSms9CMuK54fkhEk/QduG7ITBrSkM/DsM1SmIsktSzD+4pPNNpnSCc91ym1GLiVKnm/bHIaAAo3kO6iXXdbwSEMrNYKrhV1VIzRwWQMUj2pep1z+7BXSuxqlCs36jzX/Q8EwDqWmwKwmYFpI+59mrWFmWiGAmZhVig+ELR1ASjYsZkE2mGrhE+0tWkywhNOdyfc3d1hXVeNsq8x8Cw9inrCqPQpH7sVS38jrMNcRDRRoWAppLlfyVmPmQBmLhYKIT+qJ8JBxqFPXyt5HL+MeJuIFcRc0QXTzO6VH3adNpH4b/Ki8RDPe15FoReT23O+Lk7gLiVi1uWDqqdmyHMigCtJmhwDHmbRphiqGZUQ7lXfYhq6aoNF72IG8cA+hLEL+4kKFH/BkMwXLN5Jic4fqFWj9EsFIGsr9+2EQoTed8l8QaJfoQM8JNXPsP7pQAZfQWImtk0eQ8H9Wis+xsB+IJSbAjDSqdVmWJL2BZDmoIJ4ExSDiVk79ux2trTQJKHtLtjXugTz0kfRNZAORDArRNMM8+ztmgIJbTBTJvmPFOb0BZro0PXOmGfZeMf1D9OTTD/y88AHBjiZaImCydPEd4j8RxmUoNU+g1x+crhiBW7XlPJxEtOaf2N1k9lz3AwFlE2fiQlu9yEldrgO4Zth4jn5ngnHTMsxEeYX0RmL7h0p0kIVIK1mjo7IWFIrhu+O1CNMhwd62gKQScxXN6pz3yLLaGEsOl9cTJh0kCZ+IJWbAjAA3qgYwwGNzT5zhJDOJP1L2ZY1NigCVdW0sBxNRQNRq+4y05YVdZHniyUlrGXK2yUr1GwLrAELIp9FadjFgWIXjKD5yAPOFA/tcBoVDzc2528ixnOcz48S38saVaCTsa9gYbE4PoGX6kNmcPlReD7ipdl5BJIZQMYYOG7G8fyVAwlViTSsQExAT4udfmsDuGpbibwgmmDW00T7G/IdzbCx6zKzQr5RE2RPRxUMjc0DyooJTAVAlXAeuzYCgKF7YRaUvfqCcOaBfV80R1lHbZIRtmwVgwZAO8j7LXzCPE6Q1g/y3WcPt77jVfcomL0Ea3+7lZsDsBfW8TwJhYbAKmYriwHZsg4R6mtbxMuowHW6u8NyusP93T3uTnc4rXdoy4pSF4AKBjP2vaOqzWIDQkjCDF4WcFl1MW8heW7R93Mcjgwmm/l98KTvFtPGAgsnoPA1nfoYecxN9WKfp41RNIW2ZRjtxs7A0zE5v4fkXWSANR5J5g/V+dJF5vAXM5sNJ0HhMDAQ9nFmdmoCfuNcDAsb0fWDBBQM1UlHaHL68wjF0C0JrJ6K/PUt77QR+4GVSvXJDR85jV87kToI1F9cClqrwJD4QiAATJIlSt79WisGd9RR0FVmGJZU08zFMSwUH6blGU8+ygHhhfeWvywc135L5aYAzKp+mnlSMb/e/KM88KKhzXykKVVO00Xaulg7rXWUYFWJuGdo+uk+JIo67V59DbwsOlzit2LDW7FeZ9f2vBREh2ch0GDYDjhxbMBS1md2dgQaOZQFmh7Ow5ZNYs4oYQBmWlqsWkjACGVm6X1DIQ339IvLLBQ4glg21cj5J8Emm+gBxHlB09zqBnasf4VwcQKU6BPyXOs3HVAAjmJzEpMs9Px2s/afgcaxmEwhloBMmHlXKk9y2Cparxi9amhFT/2jgPq02tTbhKw9tG38+tL35lqyGrwOYNZel4EWb+9yUwAGwAVyW8wNaGfJkwwFwxiQgdaHQJdt7Q5A1hPWhrqcsKwr7u7ucLq/R1tOuL9/gvV0wv39Pe6fSLLCdRVdzNiKbSNfa5VMq8qUcuyWddi22Ca3FbUUtHpYZsMz9I5uizdFw6IUfmGDrNYqWTVAvrGRHiqASPtrXhPK0Bg5ZWa7peUerGCWWEYGQYQZY4JxTrdtbEjuhTzoOCcPzMXMSE8LPWQ3nqFpksHse+zozTtoSL35GT1SnTiE/eqTlLCSPhRUQRClPvqLDd1BsRO2mY/yneFAGdNnAK28Hul4GiRaCnSfKcDWUi4NbV0AAlbepT8yYx8rmBh9bwANMDdwP4M5M3TLoy+uHlBcSz73PyjltgAsMRwGYDqSuLA5fUefKool9g+b16E6mIBYTcnqhN7Xao/qf0spyqQuExVGMGFiXROY2XvxWw8lsOvjFBnOs4lCGawBWBCnHNfSBaU6AGKn8Mya9BzDTpLyd7GCl4VU2SRwFcDMbLbr9lMncZ+RniuwJW3mwtRJ4Cx1Mdfx9P3UyJPzIf2KdMKy2LW0UML1s+n4zs69l+jx8s0ffmeXQXE9BioBbnF/HpuV+t6wdOTFnheMkZIfDrua4/UqE0sARpQGwCN1Z20k7XM47I0B4G0BGC47vegFjEaxBtKF9EeOQbrnX9WMmotufWbpoZsGrC4tgZotGarzBrWmS0GXruSI/JJeHzeznW8qg4OFEIw0KAEqQFUAEIbCkr5YTZUYV9cUGcBMvZ7OJfsUcoQyKHiNNBaPY3UyI137m+ualYGxHwTOyrL2JL897C4EZVCJ5flxU5qdEOB141qO3X1sYxHLpZWc0kqU0qQRd6ZAx87g5FrYzasItA32J79OaG/XZiBm92ZyguqXtRTAvJC9Y7SORXWx7iy/o+8V0M15rV/k+shxYKQb5j7e8+O67Rojr9htlpsCMBtADATD0YHSdfszAbHhgwOQeCkuOvPVhlIMjJo+qiwHqg1EFYUi4Zx5GxnQDAdFRf/mf80zGcAVADbNuiip81vUd9yUdc4AMbsHKIXABIBZCBcwsZqamWDwCZLYIsD1cMFIjR9L3sdc55fPtcezDfMrM/5hWicdzMayOA9KZ39jeg6EZGBgfgy38EXX9tue9h1gYyhhZsJfy0D3oNDp+fC/+foYHPfNtsck0u8RHhNi96bIShG51i5TE0ztI0jmX9IQnVK0L+pyo1IqRhmg0qPza1vGBKFg7pMDIRm/LywZvF4mHvDtVG4KwHzAkek+loOcQAXoXXSKnnQuQMwtk2LzXo5Uq3giU3iEHZM01bPRBT+asSqLzG91+t3MtNJzBxEdPsaEzOOoM7izMMwDGYiOZmZZLKe5BDBSE9kGCfQ7LpY5eMXj2OXtskIG5hg/CbuOAr0PZq83Nc7S4LBsIsagHgMyVm1tAis7SwIu8godMagP1DBFU8XVWl3b5BGt8Mhgtuu03wI4TDwEjtxi+b4cCLW6jv3k0P+KJ0Ac3g9N7GX/l9O9caKbjOPlZ/klGwHM8+tbKrcFYJCNZhlArYRKRbd/L1hrw3besaGjimKv3q0CWSArniAqDVQbqEiSwrae5G9bEiurzr5Akqm8M4NaRdHF222RHPlV0++UVqPDVfE6Sm4ouDcSUIaIaQjpqzC1BE2UCSQA29U8sLTaAJJpoSzPvF8gr5tWmgbgltDFUo91huDWTnwmQEq6cNyHCZxHJhC7Ot7VvLWF4XZ/ZrYOtVmNkeUBz8zoffOc+1YvwbBNgwsfTuhWQCHlOiVVq+6uLtHuqe7tHGMAo2PolmhdlojLXx6auV5S1VAy8x3sNIGibHYrpi3r89677r0p32B3JcsKkFIbCjPacgdAQifqsmAQUMamYS6RIqckAOuWDCD3Izj3jW8aiQQBTOBizXSdSb/dy00BmDWcFFNajfloil4aPoABeBCodHH9rqXJUfG0GBM76FSlkG64UVz0P27QQGSR+bGrjMU9CXjRxWxn9+KdJvWdlMvCSYTPtzx0wwhdPpX0K9kiLqK3AULtQzLEVqheJovWM8jZQLHjxdkwPXcvJi7NjAiJiHt0QGPWTWBn5pQfcX8BBh6bxgZ0PQEY+2f5Im11hHNdP2ecQ5AsAyAHC9PEAMEA7RoCLO16AxKiTwoTtNZFgHG8AzDN9UvWV4KNuXhPaacjT2A5s0q/P9MEylzX1ws5g57aCrfHxG4KwIBoO+UYsb0XSphKpulMv7TOUSNo1R6nBa0uKGVxBuaCfa0oS0NdJMFhLOoWVgPbFitfI6s4ms493cChkwjG5o7H0/dtsNrA2nVHpr7brB6Kkyxcl+MVkri1VhfPM1XKIjreIfXyHL9GOBquWXifrt0ZGKX3KAa+VchUP7OZiIMJaWxojIE+enw/p0s6ggfMTFd9FJjuy1nb6MKGxn7BYmPDmNj1fXoYm/IHYv4xipcnHjbmZ+y2gDHEYzzlwFf2XBpKZQ2PaarXVpTSQVR1IxU+nIsxhYREh0r1Tle9vj8Qyo0BWOg60IHq7CPNlElkcBkku6/FhV3d87gsq++gXW3fx6bBrXWJ3Yg0tEI2ta0e1Aq9DgYwRkcjAnhIdgLkwc0H8Epmnx/JdlTKmhB75P8YA9u+ofeObe/oewfz0N2BlBPoLF9rk6j/tvhM3lqX4F1dEVCoxqUAzgRQLNEPdCxcRpzD6zQOQKa9EMdgs+L0kw8PHB5pWDMJMxzxO/NGuoMDxj7tinkKk+D0u6HLdnrfY1f3YeaqmHh7Aq8Asn0C0FyO4H9sX0lnRNFPDiwwrj+0S2P4ptfaXguMErooZ9H+EpymfvVC7LIJ4UXfe3uVmwIwIgjrkVdwi99n9KNLnnyjjnwQAzAJobBNSuVvqU20Lltca8kMLS7MO5QAwjTLawegYimCWc2DefAThc4UJuZlhLkVZon677swg33fse0d2777AJMdyyH7Miou+ABou/ylgrZI4KjtLF5KBJJOgwjhwLhmX7hjws1Rh2MYkmpatsnEsuM5pF8AGE8AZu+zZVNMZqYPfgCoR/AKPcyYGmc2pfXm5rgB2BjY+u6sz50M1r+SBjW3ElTViHZlCzi1ahzwawl5IFkVqpUWe+jeB2Zl8CgYmm5n+mH0FOl/03tSJ57XH49MRjdabgrAgq1IkU4mQ3+gY98Ze2fsY6CPgc4qbhM8BKKQpoduDaumhl5Pd7JdWl2EtlNFLYtmpNCEhqeT7rgtoRNT6Ltdiz7MhKqFQDwwLJ0PwePGQvNQrc06tnrdxDPF6T6Hg9f5YcPed5z3HdtZQGzbNvTB6H14JL0sTi9obXHAXdY7v3/bSck24J3CQPJ7INjmE+GQ0BbR5w6CBzaSzcV8L9N7SICkPz96Jg2AbLKyY8nvNc3MobfY5w7yfce+b+j7jm3bZoBi2fHKWZfqjUfz0s03A1LMgGFyALNpnAewUWnBjFBn2fYZa3JNqq6HWQaLyPaxe/qma5TJXDrXyjxJRj1dxCbeSLkxAMs73SRajgAQ5rSYGXCGQP5XPIWlFN09O5iVifj2fd8Tskj6k2pJDktkpJiYxJVyfHc6fgIy/651pCSyh/mRxoGtbVTW1Yds9Nv7kA1/VRoR0yPEYWDDqGP6rXRg2QqsaHZSWb5U4jMSlVH8IaGLCckM7uXWjJuk+jkfHACZlcVNibZlm+Ua8xkBHMFeeDIhZduxGNA6JQBsoC5mowBYx647Zw83EUVbDG/hbF4ON/nsHszcyow/3Tcp757p/9QvArijXoyNZ3PSHEcDLBvjpsnNwHI68DX51d6/MRPxReWmAIyIsC4nAHPEuC1Ezhve2ia1EihYxZtYiwjyq6xFW04LltOKdVnEC4nQ1SxbRa3K1JYTlvU0R+V7dlgbNJKRlY6ASaITFeuQRCp1mRlMOjNrEioViJkIKFVyNxcGVdnHsjRGxY46GKMC4ALaFbEANXXUizYAHnZsieKXRcMysHtl1F2uWe6JNEZOZuVSlbVW20xE1l3K+kZ5Lim8yUdweOgOkWXH2SWblIaoClqja1tqwkrk9ZoWrIoIJ+EOF9Pt/O7J3MPM3rYN+75j38/oI8R6YWCqJya2lePwwtSrB8Yym9jxWZjXeRKVX/AFmBy9kCCVMthYsjLctPtQ3sRjqmpcYtlj5VbZF3BjAFZKwWm9A4Ox7R0YA/u2o4+B/dxlQFpGBRfIi0bWHzUtNRGTNxGIzAqQX2t8TnXvo8WIxbbwkhPfUh6H+CqDn4rNokhmWugdBPJMDDndjzFN8RZGJ+u9a12oCF929CYLYPYuu91s1EUzY0mN7M4NQAapxoJJyBM7GPc+FMAGSpVdo+uoKLotmbFP4JJxTuYScEWLkXq9XEbF6WmwND6+f/EYiRWx3msOvwhg6tuetMNNmNi2YfB1Dexovub6hzkLDJDAnm0XzJLRFRnURcMaOoGQ9knPr+a3GfWXgcwf2g9k81rJfzbIFsnbtRKyR/J5uMSHv7cKYTcFYEQF6yoMjPEA3ndhEvvA+bwpKzNqH3E1tciGHbUEeLlXscjSIkaQg3RCFcElg0A5AlgtPuvn3iKsJa2FrNV3FDKT0SxhN0JZN/xQ1/px0TclQCMitNqx1waqO3ofACr2vYPKBmDDTh2872AIG7VBSdShCUMxKqOXjliobqsKOurS3IT03Za4oeiuIHnTWns9g9oVWpBMmFgOBSA7XxJo8JXnzozUNPRYrX139iTAJKDFg9H30AmNeW3b5gwrsy0DMjtndIVs1g1n1oZT3l7ZfDaAKZqmWpLlR6szxSNVkZvsBmK6D0NhBpXqayAJu4KihGd4tWdb9tAOlD7LLO2YzfVWyk0BWCkFT548hQV08mBwZ/RNOuRQd7UxGSiI1FpBGsM1mYBXYrggrs5kQi5Y2gnrIhH7kkFgUUBRLcJnY/t50fNGYKzlrxcRf54drU8xQztk9RldNpctuuGHdN5SGnobqL2jNBmchSq2fRfxHcLMANn9Z+zDM612HhhFBn8ttj9h14SJKV9VF8DuvbvXkplRq7LOUpwdmghMQjh8EqiHqgWRb8TibFe1pDneSgJ0XQ4Yw8FKxPjuoRC2z+J+fnAzMQR7fa5hJ/u+o28bepf3BjNG734dtiPRkQEaczZHSG2mGcoOV4V0nUIxoA1t9NI8S/LCZfV4XyL3djdUALVusOViwzasLUXDS4bDj5mmk6PFHFlXWHM2a0G3B2JvGYARUQXwpwH8LWb+mUT0cQC+GsCnA/gmAJ/HzN/1gmNgWRYwM1rdsJeunScl1Zt/EDPllBWi6FZql8d33SL9LgJXTdS31G8BWgZC8lOKjuidxCUv/eLh9E7isgdLwAssZiqqCNVFNTqQLA4vVNAX8T31wdibeCJLVTZH8M1Q7Vw8LBRTZm5iBRcbvEWj2vU6Yr/F8Czmexss+pw1gOmIuS0mc9H+uiCfzL9jAKmbdXNsVt87+tgxese2CZjte5iLAWYJwPYdo1vYycFDigAwu4fchvKaD+JS1jvtcQkg+TtuLroehum9cO4oSHJxhsuleLbYoX9t9YVn0nnEHpR7yLWfP0ztdEPlrWRgvwzAXwXwMfr6SwB8PTO/h4i+RF9/8fMOIAzsiTTkgLCOhx2EivPWocto3YRkqmoiprCIIsGdMurIB5A1XFHNynKELSlDK7XE2gy1zCPn/2R9JIpOrr4RUX7/ZYt5CsWMZNgGI713dJZZm2EJAXXvSmVaNujNXLl2jcABTBCmkSQeVN1spAFlTJg7BggV9WKmv1oOwJUDRw2ERu/oCjRy/XIP5/NZQOt8Vu/iwHZ+Ju/t5/kYxsrYPJFdHQE91au1YdVJ5tJ0bM36UQuTroSp15R1N3d0aKQ9C2M2M5nIlrsVyaFfqueyNnYqTgLdxLZUAIxaZL/JUQlFdV4qHQUdzB3u9bRJ5Ni0no4bt0aynlveEgAjok8F8C8D+I0AfoW+/TkAPkuffwWA9+FFAEaEdT2BmXFedux7x7Ks6IOxLIt7HoftAu06xcy+APLBUvaOUruL/EU3t20q3DffjSh5h6wkESFYlmlaEdUcrnJyIuJfw7GvqYfJD87B3JJ+VDR3u22yW7bhbLE1OaqYfAO1tPC2Dbu2y7ipx8p1EZ2nazTQkvgku1y9DrlqZcN6fnTVHWWh/BiSMbXr+k6Lhu/5r4LYtimj2nY3FftmqxPOvhTIzM6+dxX8xSS1eze9yRwnkk1Xya1pqApSc1R8YvWkC/f1vZpSHlm2WiIC8czIYwKx0B5tUA7mG5aCOk9YQkssdrCUIs4LotQS2ke89cw8VNMyUX/CNRP3tspbxcD+QwC/CsA70nufwMzvBwBmfj8R/ZAXHYRKwf2dMLDzw4596zidTmAG1vUsOcFY4qG064Smoxkvsxdt23cwFVBtkt2iFhf6hXnNIRO1Nm98yQdvNutsPgBQJsQKNEP3TzRPo3qMcJ2550BEt8KYPFdyYdl6nig8iFsVb1prTYJ7ye4T4AUaHzZU89H1fleWxRyLxTxlbSjAS8M+aAYwWQRve1MGWpNpQ8QAYo1jV31u76J17V12pxaQ2gLAlFWdz8ayNpzPZ9E/e8foOx7Oz/z7dv3hacxhHca4Q6estYIKxYYrpSAH+XpfStppBi+iFB0DiOdxHEEL0+uiXgAexuoP3y9FNU2LyTN2Z2E8BXuHOzautWaYqNqBDvJFeFhvzoL8yAMYEf1MAN/BzH+GiD7re/H7dwN4NwC842Pe6Sbkvu0giPlEVPBw3rHr7L1tXVkYUOuC0hZnUSKC6o5CewehYyzdo9YlO6vkyJcNPZaUjTVyt2eG8bziA1vohph2rL9V1laee4RcF5DzEcM2CLFztNrifFxQdYkQM2R5kcZQ9RRtbiBxDBkAtNP7+8frUC1Iz2d6mF3jAbeu1sn0SOZjV0a1bRIxvyuzMjDr+46Hhwf1Kp49rssY2Hk7+/GkliOWa2Y+SCEvxR0V5vULR0xMXsU2ZEmamDEwv38O4CQ0MAZGkWvpw2QA8fgyMXpiWJ2rgoiakKUoc9PJkwlcWBIcMnu6aU+KmD0o0VqYuBnNT2+9vBUM7CcB+FlE9C8BuAPwMUT0uwH8HSL6JGVfnwTgO679mJnfC+C9APCJn/xD2bxhbVnQ2oam6xlbW4Au+kLfodt5sUfTW0oSPaYHRI4SgYA2C7cmkffNslIkvUMdhdfperYN06ecmBpzmFWe/IXmnxMi9Z4VMT3lEwMycvMMrsXEDE0YVcGnSLzXUJXXBl/ejPdYHjMvzJw9amiiI+IqajHzITtHvH8MjRgTmIXHMYOamZARlKqCfbfF7XpPCWSnkBRnP7FMRyav5SKOz1ZoNJ3gyAHLQCrVlVBsf38kRpWZVTB2S5Njf43RRphGsDKJxysUYGXHyMfkYz2bOe8dV9vJ7chkT+IK/r3Ny0ccwJj5VwP41QCgDOz/wsw/n4j+fQDvAvAe/fu1Lz5YBHLaQMyzJ6gDnVAbS/L3zhLEShLyIOlMCiSLG9n1AUwokFlO0uvc4XQ6YT2tWE8Ny1rRliKeOZb1l+kGJ1DLznFO16nreuFJnUtxQ3HqM/qi6Hb37sECwNV0DPii7WEZFRKDsueW6VWxE6BLD58NOm2fMKeamkvGStJgF48sOUsBkce52XGO87vF5/Vx8CT2WJQuwLRj34c+RKi3zwyw3IRM7Ay+O5NcSDncjzhngOWwp4GH1ZSCdWn+XpiVkWnXzU0k4Nf683iyrqY1pySGuorAimtrLBIAw9pQV4KUAqCCuQEsoCWhEgyuA8xVUlPvFUziAMj56LwrOTYlwFWWZnNN/sUtLvH+aMaBvQfA1xDRFwH4FgCf+2Z+bOAlQaUqwAOy1KawdwpzysC9k9CRpqBmI08DTiVdTr1YMmQxP4B4Kg0gPFOKz2Cpo17wKAEeNoE3sjv7bwtdxhCZdmJErpSiOwdpds8h98tiHzsLGOqtskDWaxHml+fRgVvEZG4auFsSU6nJzDLhOwvbXsXpvtjA9pEQiSy8x/N4ZKYVcWAROW+KYi1V25MO+hXp/ej162NZFgfmkwKY/e7I1oJBSR2LR3P4BCA57/XjtNOTYBzbviN6pdqHqWDoahEu6hbR1DmmcYmWKtkoPIh6zEwMZGE9ZiOmBshAldho/vxWy1sKYMz8Poi3Ecz8nQA++80ew7UH64S1obaBti7gvYCpo3YAXTJUgGzJhsYaG6BRgJ+ZmWaOruuKVXcmqrqIW8RUBR/tjKxrD693gFDI3IQkaPwOQvsqjJicjc3FAUP4TUADcb3LYBGE4rQdN3dJ/mf61hhw0R5jBlk/BxChAUX2sZx0oezMyPsAFFwM9vDExiDOm+fmVDYTMB1iuGTpz+5m4/l8dlPSQiwMlCsMaHSTFZ2Ess7VWlVHDTloWZvXWoWBJRCe2yCbzFLPZraO3iUfXTSHs2MPM0lk12QEC5MgZgyddNk6BktoCiAhM4OrJKnoA4UauLDv2VBKSeAVs+nxFVl/5eBag6b59+bKTUXiMzSFMiCud2CabceQjuDLWrqlURb2NABPedM1vzkTgTw32Ir1dCeb2J5WrOuC1oSJkOs7oucUIs3PFOYXcGBgZr5BvZZ5DR3pHohDlocASZoguCs/veUm6ZRWeUh8ky+CHmPK3mo7NQ0zdXS/AFvVksEyhwUUioSHxrqaMi/RhSgxMPkNKLywxzqQNDsDzBK3NPqOMXZ5na9dvYny2MTzuO/YzmfspnmpZ3LfNj+PBRrbRiutVQemZV1TiInGaykgr+uqwFaxLssMYMwzl7E2VuAskFCdDsKmTqOpH6SHOStsMnM5iqSxxfQtMsGOAkZFRbNWR9VdiUYpGIVAoyDvUVqIwKVgsKwOsRRNefQAx/bhK89uq9wUgAGmpXDSdUKM7YNRlG7LllekTCvE7mEzJaLRSqlz4Oq6yua2tlEHAdbrRGcRt3cplw1vAObCqb0PmZWLzq6DCLIMScxeMogijapWA3QyxWxfixHZGvY9mVNmUvVsntmaQTUpx5hvXouzsCtsykIFBODmfQEuvl9SHaiJ7UofqzZkjwxcIycN7PKwezK2pc85sTgcrjnasbkHeV3X0O8SgMln4WVeWwj1bhYeGxGMzsCg4UkWmQYKpfWIyOYjO5hllmzXLX+hfUtj0opF3VdlYSwTxaDrdZ4mxSwHeCNEK+cbwazY5vdvp9wUgDEz9t4l9xMAlIK2LqBagdJQzzva1oFyRt07QLtuFR9kmlm1r1J8i/e2SI789e6E0/097u5ExG/qlbJz0xVv3fH67C+zsA47s6Xc6T0GtJG6PjYIvxyugc0akvy1gW4ZWc/njvP5AfsuoNXHELayD/RuppvJYjpoBkW3LYIumTUcHQFHrcxCAI7akLOW5PkzJpzDLHwwT495QfUxzCObl7EoPQCr1orTuqLVpu0mwCTsquHu7k5MR2Vmpn2ZuG/HWCzjw5XrdE/t5Yzl3zcvatbuxpi1uth8eQY3AJNGFk5Cy8zaxLNOuyQRYDP3Zb2kpFoCbC/LmV3NGXCP5mKw8CNre/uX2wIw6EJWnQnFW9YAGliYMFjcyLXJsqIik5dOnC4kTUGKtjt3awuqzthtad6pyTU08fYA0EGv3h02AR2TyQDMgJYzldpr7zb+m2GHd/MlDyLz4Ik3ruN8jpio3mPwb/tIntKc0JpQdB+tUorGoUZ3jrisQ0xY0F09inhtLf4pssxCNUdTc8IEtt8d5/wwsa4zsplNJm8eRxBpU9bV0i7r67IIqCnLMvPRIuqXpekC9oj1anUOs3Gzz+7rkLLcLIGLuDYHPfb4uxmkNYwH6gkWwczNy7kkheqKYyE/mCt4aBgGkuQAU1dnWUN0z8zYJoP5JspNARhYsmZaACdRwboUjAa0xqCygcqOjRkoHY1lcTP3gAYTTk2sP51WYVx3J6wneSynE5qaHVQA2dodAJRJ1BIg5k4odn0qhPs0y2YPoO7SjARsAmI6SBEDKA9eYyEiZstSmmfPnolXjmMg7boDWdV9IWtp3uGHZrpg1jWVKXMCd02JDTHFBw2NrJ9tZYs8NxCbdyC3wWLgLYvNNSOWDkcBfgdulr0YeezgvmPsu2tffdvQtw1j26PO7DilYtGEk3cnmXzu707aridhY63h7m5NE5YysDXqxOK+rB76GGAzVUnqtZvpiONmHwfAnTytrNlxc1YNdWAMSZ44uPtelFZhFxBysQyp+HpdgDHGohOsiP1S/bb/pLZHMut9cQcCvCL05RUD+4gVNgArGvNSSBbXglCbmIoDBcs+AOzYB4G77o6j69KmwEFLV9Ia2rJgWTVDqwq75pWT/jD8uZl4RKprgTT/PgCE23wGLU24Zzms9h4zpC0w1k7NHMn18l/La/VwPmPfdmwGYPuOrpjBLCFwzBARW1MCRQiEid3NQczNP41NsvCLLEoP085Spy+H2C+rE1YmrHZ0gP1lqJK3q/2dQTuvYeyuf/q5AFjaIlt0fzqd3ItsAHY6rXNAsnoks5nmhr0xwcymVHOLFNfpeQKybtc8xkGTDC9svkfTAuF1O5t+ZFqoenfInSsdTAVcmgBS0U1bStdwi5AxEnapjQp0qO5KkGdk+zIArwDsI1gYltYFIKt4XcwMqqiDUQehLR2MgjZ0xuSBgQFmCSBEseDECtIt1Cxp4aKPknJ5+UyWBo8NykLk+paMeIRYq78RM8GE9MiG4ANky7nYpVNHLqvIzGCBm88UwM7nzQHMFrLzCLa5qi60tOHZZJdmyQjDseEBjhg6YCJ+6aIFWBmUW+Q0gVcAmpIAe8/ez0dT89ufX9PF/L+s7cR+BKUQqoryxqrzX2nP1bUuN79qSUByCaBuznbtPw5G3deUjt4Petf1GDfzAodZOTsyMhO3fi51qBqlMbC0ZCg/BLxMn5Tc+b5Pt02mbEkCjkU5sZgauIjkf5uXmwIwAyyGxYGJVwko6ExoywoqDShN1jquO87bwHnreP2NB4wBtOWEdb3DerrHcjphOcUGt+KNklABicYGQCGBeofy2CegQplFB7hEsKw43Ib+fjhw7WfdDSfFMPG2+3IZGxj7vqOPjvNZ1wSmpTSvv/EGtn3Hw3mLdYG7Mh2QA9jpdBJhexGHRG0N63oSPWhZNb129Wj7WhZwJVQm1CqpkqHSn5ttuhBbDJEqChtJ8LB5tWygUJUX1Fl2NNL8+aXIlvZcjHOMeLgp1CXMQow3uO84mZAgQiVCKxVLW3BaVjx98kQy7bYKYmDsA2ecBTx6h5lJBoi9d39OBjI9AKyPiKofo7smZ78Vx4luEjItNN9nFsZDw0YUMFPAsTsKlWlJ5ZHqfAVcGGUU2RuhSoQ+EcTs5IFuHkxlvhm8hlabrVK4Fq/HDnSvTMiPfDHx/Og+5oJaAVDFwgVUhugBJAOgbgPoI3bebhXhWg/RXiLMlVJLL5Eur+dFyecW8Cra+CacE7HnyJ9F3Yh1slmbmSWuqEfCvZ68bueHB2wGYBrI+cYbb+C8bQJg5zP63rHtcnKP0idJGbS0BdwZy5AMrgCh1oYBoNm1QUJJqEXkum2g4dVuNltuCotrU8PavpKl4Il96YSQXfucBszEuIydKesK9mW/D7AkFfNrFSAzJgKo6bvDQSUcIikFtZqqBioTAxsWgZoTK2YzcM6lb+02gZcCmCyDy9po1k+NvXKyJH3amIGniDPG9bCLtiEFM2XXB61r8hp73duTVwzsI1pMgI7NMoqYj9Bdh5hQqoBJbR31YQOVMzbdM7LoPo9tXZWBnVw3WVbRw4oyMZkM087VOmI8VgjwwQYCSNPBSIaZy7CA0YdHked1fH3fwb2jbxKkueu6v76LxmXrAB8eHrArAzPwevbwoDt0R0wUq317f3+PdVlxf7rHqmEhWx/imesrWtuxtMUDPqXT29rNGKzHSVlAg7U+OPJoHcxEG5wGMnGAx2d5AbSU2jmDF9s9qvluoFnYTcglhb7YPZiZbfXoKXlGR++bm+uj7w5ghi4efnII9bC6tmLvXRP27Td9KIABR7lL7nfYB8qg7GZRYXsRYDAqMaChFYNYqa5sTEOQ9ONKHMPkL7HYP2teZl4OuFx5U+W2AIwkC6uMz4ppLZhui1ZJskiMAZSqeeRBeDh3WYZRJe5rVbPRtBIJXLWFy+QR5s4QiFTPCfalqb3sY+kkkHAOYrLEAmnAqkaG0ETk7ZmpyVuzi973vEyu+dHT82G7MRlwEFrbQShoZddof0JtOwD4ngBD193JuGF/ULGtAfQ+dCsvWRdvZqNlsNXvwSwfCq8XUdSP6YfGniAa4rEEcPJUN7BLpEgyGEuCbP3q4vXXhzDXZwr823bW/GEafjJ29D0yWow9Nvo4KuBmeEbhAATE3owZyGLbM3amJseal/H7UQ/Abju/274LRRtlcJG0BNpPnf1eqUsg+i2IPEFjxOXZRiO3WW4KwMSOF0YUkeBZzGwCZLQAXNA6IPoYsD7soH2AUproVf9aOh7fcciXduSTR5wTEXJImH0srMDMTw4TcwIxislXCEoMixxjFJHUidocDiBjLOkdbN4u+bzvAzt17G2g7JI3rY0BOrAEwAYn++kckAslkFItK+uABmIZoIxEePqfZEciG0ZGapPGmOrikqTQ1A9azckmYwH+cLNO8r49ewgGZoz22bNnkvliO8fay/MZlnc/Mz67Nru+ktZ/SohIma47rj8ADFBNFAFgZlI7OJo5CYN4S30UUkkpRWP5+ABGz0GhZD3YNcdHQZtfcJS3ZbkpAANiO6/LHYVE2yl1wbLcAVQxUECtgWvF1hnL1oHScHd/j3t93N3fqwmpoQaVQIWn2dbnSTWdmGb9IK5NgKRW3WikJa8XCKNKHFlrDUtb3MTwcAvTTfbQT3LOq/NZAlhff+N1PJw3PHt2xus6OM8PmwjKvWPXXYjMC7muJ19Ws9yJiH86nSRhY1s83EDYaNNwBFlOtawLWitYVgn+rK14NLs4PBTEIog9NpcwKzMl/bCxZozOf5NALCL+5+FkDNfu5e50wt3dPe7u7rGoU4KIwKp3vfHGG3h4eMAHP/gh2DZrtiD8bPn09y2lqD67ue9LmtSkzmEYlszR2BcxTcBln8WiNeuhMRfVxExd98pfdOYPZXgJxIh0N6QK8lURaj6aKFuq5M6HGaM20VFMwjJqYBvlXpE53/blxgCMLjq6vvCZRWZi2Wl7UMPaGds+sC4rCB3QINZlYl8xk7sgqofmLEk73dL3Etswc9McDGJeJaAbrAvAlT0iNsVwNXdIFgkBNQklsFxZfZedd/resd6dHMBWZRQPD+b90h16+lA9SIRtYyfNFjgvC5akGxmDqU08eP6oOQ9YCSdHJVANBua3ehyLxsxwIJJTqwYTOzponHMReTiBXetqWXONPVMwURPtz+czHp49uFh/3s6TyD4tV9JYr10XlBuAgQgLq+e7FhTvHxQ519QM995CpEAXgGZpxUV+MCadTVSrQ05gojoFma5P3t8vH5YYUVmvJUkEYB5Kt+7NQqC41lcM7CNcyDQwmBZWZAaCPGpbsK4nPHnyBKU2oKyoZQHRgn1jbNsOpor7J0/w9OlTPH3tKe7u73B6chdamObOlxPamY19IUahgpiyeaDKAAMPVIjrm9A0vkuyCPBgtCb7EHKPQEOLASrJA2Qz9Egi8OiyMPuNN97A+bzhjYcHfPj113HeNrz+4WfY9h3Pzg8CZnt3FlMQG6SupxNqkywMAmyLB+6uGvDZWsN6WrEsFafTgqoMrC1Vg0aFqZYawaxucSc87pLuw01LguxzWVzXEoYiFrmwKwmLKFhqFeZcKopuJGuZMixI9cnTd3jE/aoMjAfQd8Z23vH662/g9dc/jA996EPu8fUI+CGb38paUtuSLTK99l2cKDwGqJA4eMxMrRLGMDQcIge+EsVO6ro1QDDKEis4zBFVOcDDzXgoyHhGEnN1y3fINbCc/lqTTkq2AAzNFCJmvPwH9g2QJOEjTBcjn49vLIritgBMzETZOowOdsmUy6o11LoAdcGydLS2o7YmEeqwHGLxmBlY8XO5DXRROD4HgpIzh06joMYaIEgAeGgw4YDkc9LkZCKFeDcDoMt1ECK+RIRDvZwFdTmLeUyEtm0ACs7bLkG5pWLfdl8CJEkei5qv1ZMTXnu0VtGW5usGa5XsrMHAlH0p6xLvljaFVQ1DN7PQikgmCiwMwqf72cSS6rzUxIydZb1LWKTqlyW2f4tF01JnwuokULeWAuYEKJAQkjE6SrE9Fgi7seoxvN6OTJ17l6Bf7woHU1LvsZSCvKA9s3dfowh460s2WRz6njFQYVXsTCz2ObUQi1gelo6az23aKUVKJNHWrvX1t3e5KQAjgkTe68zB2sqkU110bt3Hsa7Yto7WNhnYgxXAqi8fakuYV7VWmzLD41gmI9IL60C0WUyuj9yMYAYqJF8YkZmPjA5C9BSJX4tuJizE8zwBbmIAAnxdc/LURTYrQSlYtg2ggnbeJAUQVezLbheqnjs1v1rT3cUzaLXpue3G1FpBXQpas2SAyrqE/MKSn9IRwAZ7ptowU5IoZhd2pV7tlieJAPE6p4F2GaAtKNRAKIcQBq1jqho2YNlIxOwdLCsUuuYm23fZibzWiq0WlG0DFHiWNepHrlG9w34PliwydLDBcb8GYtfir+Jexb4UwGXtJ6x4r7oXJIOrvKkJDdNeCGF6lwTcea7QxkJMCJ6F9pH2eDuXGwMwEXDFGJHHYICKBaFWD1I1lhX5naAxRXC9pWrEvehg1U0iqEnkugOi7V3SNzvJ3dg6e7q+ISBYigYskgwovxZbZ8MAj4jgH5CwiJ50MSui9zPO+4bNYpd8MXn2xtp+gqbRpPQzurdhtYDeyYu3RDjCEqEJtampouDl7nsFp+MSx4Dn3FIxUG3rskGEXgoqyS7TlhLcHICs+3K4ealrV1tbJPnkurqmBzXpeJd6rqXi7u5OAMNSgBN7fyitiN6177K79wgRX5wm4jzh0eWGStyDBKay5+vfu2z5xkN2xOpd0hoxJKKNKPY/SJ1ZTe80YWVvrpmTPPQoQzqlgyBLm4zYr9RYqvXP64TK+qn0kdZWyeahwJwnzFsoNwVgQIRR2LAIEyFm6GxOXmxE63oWxWD375J3LJuOrCuFuWhrBEOgBZRpZCOQxCyECrdcoTsvF4BZsm6aAGx5fQHNQgEXkjgDWIdv+tp1mUtetGsz7pwrHZL5wVhiCYDLM7esFUwzsusrKae8hg+Ywmy6VjhTEPduojCFteg/JVV2VCcKkd8Gnx0satR2XYocWNXTfUsqGfmhhS0QFSwWE0YE24rOZYalapjF5gDWtw2Wv2vvuzCzLkg6UkyXRd7XWrHvO+q+Yy9FlxltYN7RTaRH3ItVUHZO2OQXDMrqIWXryPYkpe+n5w6PCcSsLu0VaxtY/UqmkpjQbrHc1FULaxKhlg1YVIApFDmh7FGWiO26MF1ozoseS4tk4DqgIC+CZRyBKxcTmYHU75i8w3BhEOWsFAgQM9ahL/qQUAhW8RkQUbyPlA9s230Ha/NQGZsSvY3cDLWO3oypHoCbdNMLAS2gNEr6l2T7MPZlpiOg1kj6K4NETMiwdNgzUZAQIo8tCzPz0nhxhpLMnMl0XBaPBdt1L1AL3SqlSIgF7vGOSm42L0vz7BUMduFenCSbLB/ilGGi25rUs8eN2XpHiyk7n+W9fdvcabHvYsKTXktAVmyDFiBdnB0bC/MOoWltmQEaHYN0lyKCZqZoAFW15yWY2zirM/xUn+yTmQKYZq9d1/XRcfd2LjcFYABUc5JnAKat14NChy6Vf5eDAi1A83qKXj+8/Vj/yRkako7lLCTUMjGtIgKHC4ChC5lHxIxZnkSzKAcBg5JwzwZ0ltCQ9Try+kC4eOu773BN0T/BcLLJWEvkuq/KtmrJZo150iixpTT0UmyX336ussPzCyampmFmIc7GvM1S5pBD7B8PVo0xNK9sAZnXryxzsKtJB+71U31qFKiHsiiQMagTSi/yXGYDSdEN0WPLGHJtJXbHIl1zaxHFct1Sj8HyVXBXRDdma30TGpahOyKD3bOd86Elq8H6d+qd8nO2/bhRLJYMiLpOS/Iup5C3f7lJADNWJAJl1fQ4JliG+J1TpWTaXTS9sDEROa6ZRxyjFDJjgtKyH/1eeI809gtDh2PyKpHBns5+RY7H6oUbg01eUZUjzFPWpCiy7Cjlvx88DVJhKQaICl6tYhCley/ewZeDYN9a9fea5tZqtnMPAVVNx0IcIQEJtCyMwq9H78XeMvXHdDJ7iLMi6im3UbUIc0DNRZbsGCUAjDXXP2D5t1jN6JAMLMwg723Z0iL+wYxSGSiyOoGIwdxBQ3QuUvurl4JqKxYYKLVLrGhpKIUB2kFUhWnWitpk42XumiqnJnPvOHGaWW4VScmM9L4g/X4MqU+rq10aPulnpjiaI0mXMHm969aA+jso6HpuPD3PLZXbAjCOtWa+M3FtouuoKWEiK3oH8+7fd9MKZvcXnxEV95JYILRdkRCAzb7k4EUGTfodmSc1u6YOvpqHrB2usEaqSx4rLjrgC/SaTbcQEJtNVgW0ISlgfA0ii7kHkrTCBNINUOH3bt7SNTOwlIGjqpmVxlGYlhqxksktHYArFwEi9u9P95CeuhfsqEVmncfaLZnBkj5IlgN5QkYuerxjl2HwDj9W77uYr5oPzJIQShod1cL6rkyM1UMZaXJ633HWODtbV7lvEmg8+vB7WpYFqNKYpm0VkAf+VkoifjKTfSma2n5Mlm5H0gpI7jlJb219WKrTGgnO2GUNra56U1B0U5ZmNmYtcmvlpgAsMhMgTJtM3bVRhnrvBncNloTHujAOg8S1CTmDQYe9DHZg32Kf6fKVgQXWJCWzHs1c2LN16dfjAmtVE5Jlz0gxYRkeemD37tfIftxsChRISiGJXyKnflVdqu6FJJr0vmJmhB8v/5XrdnPRQTt3fISVTQbAnmTnWivG7xIzdoac2kcEep7eG5qDy5YAlVJRy6Jr/OaNVwjFPbmWDBOAxOhxZAthtqyrHXvflOmaBqZZREzg37umPIosuV0X1gMxWZou6Bu1aLs74dLvuonuqZr0PkivnYpkw6BwYslmt2GSuhlpfU+dO5wRTP8cJZO5dW4LxG4KwADrhKTgoqK0AlgtBKIB7jv60BR5vYOIsbQmNJ+qr3sUVpEXM6vI6emQpTELBXA5GpFtOCrpgHtODazfk6diMBWPqtZrL6bXyBZrrLn0JQm7sDMCsJNwO+nQIuqKhiszdFEdhamCWfYGiM0v5r0DCTbzU2JcBU0FevFCmlljWRAGLFungZixh9n8izYivWLftdzeZ57TgemPhIlKOEVPwOVAYECmJrcJ9rZjteyqfS8JGluTuifZAIYGg7aeGImZ5d0BTLRuxti7h1YIWA30sSmQqSdydPRNgOy8CYid964eYTErAchyNoMTrQPlRwABNS3dyZpj1KPVjewNSejgUsC9J/PdPLJzhgnrx7GKA85M6VC/GcRi6r2d8pYAGBG9E8B/CuCfgAzxfw3ANwL4agCfDuCbAHweM3/Xi47lkc7OxmJEiHml6VDItlOTDtJaU29YBK0Ghb48h8nL6R7CDHQTyVQvhgXWG0GRwW1rI9N7cjQ9JmtUuKRNDHOCdHG0BI6a3TZYz6aZThmq2wxW5sNqwarH071buoQFskyHyJYBKUi06uJ/LBEiRFxSmJDBDs3ZYQ4COMtk+yxpdZlJZpP4WjmaNj7IzLLiSCFkjCyyadgFx3nNpLJtz+RvdzDzyctSFXHeRfyYz975L4TVCojYdXmOMkpgYO3P9g+HlmV6rrMv/0nuKTphqTxhAJTCYKITp1526NjHcJVHKv/xz96G5a1iYL8NwB9m5p9LRCuAJwC+FMDXM/N7iOhLAHwJgC9+mYPZ7EIkZqJkA9AMmiSzoxIdADJTrXerCrALTncnT3x3QaFVa8kJR47uNRNfwy4sqEVnPA941K9TLA2Rb8pPLEocYE29DABF7gOExgVDZY3Riy4hEl2mdJZNfHssmbGwizGifoAws+ValLGmGdiyStRKup2chk40gmQnIpBF3h+9jkessPdwBZp0gMO9qwYsodnkcg3ETJbMiQWnyHaCD2bPiKvm/RhDdnMfFsulTCxvZZfZiIOYMmWnk+I7LQRNOy7626iyFVRBiTrCdGkuU8i92oGHgFhJBiApsOkeBcLl5biDdHMZhui+ZYfHOxZbFpTN8LkZDPCumY+3WD7iAEZEHwPgpwD4QgBg5jOAMxF9DoDP0q99BYD34UUAxiwZAggoo8i2X5omWea0IhYhF6AMF+ZbayhNGri1Fet6knQx6pECzLyLTicsTN44OCbdW0Ma4S36CamgymFiGWgp+7Hn2h9lkGusTilABwNcVN+zLJ6xv6CxgX1nHYziict/5XuWW17OXz1YC1iqbrXm+lekx1mWqvFehLaINrYsVZcNkecCq2TsIwE14OzkWgy4cy4jYLHj7rSBBtv1DzVoVMexjVKYMYeCXCQ0rGAzIVXb6oN1dcQc0CvC+MGM0pg66U8djDOYbad3TTCo4FOqBMO2RRwBMPkBAJKJaHdfAJ9omSWfvRoLCmLFmbvUaUmbfZCEW6B4uqJ6qAPzutKQzWostMNTf8fq8olpeUgOycY3t1TeCgb2wwD8XQC/k4h+DIA/A+CXAfgEZn4/ADDz+4noh1z7MRG9G8C7AeAdH/uDfGBatkvRMDqExnSl6br8A8WXGbUqKXYWzY21LFdS6LDZQBaqQBrFDh9MBGMihFIYY2iOBRLmJHmajK6TMzA9WrAzitMpSZBvVFlWQ1QxRkEduru2gZO6/gcLC+tdBve2dw8tGLVrpL+AQE2dtdng901eE4CtspyqNg1c9ej88NRO5jABbrZB7wP29OC1gLEsY0Va37bxa3q4lZkepn9pn7jQyVwLqsHAAGuf+F3qWNo+wUgAYJSBqjtYMZOHsBQ277MJmEBh8Y7WtNMQDfg9BoANN91sSzbBO2XgCA+rmd4OfD5xasBOKa6alHzfdh/Owux5ESaX48+yeXClfW6pvBUA1gD8WAD/FjP/KSL6bRBz8aUKM78XwHsB4BM/+R9mn0V15h5dYnIG7xgFQGEJ2KsyYy6qea2nO7S24HR3j+K5sVaZsdQcNG3FGpb1WCZueubR8Fj7gGYV6ck2W4BGpJnuAGNyoY3obbigb0GFZUC3/YKCm/xi7wFcti5y3zv6YLStax4sYzLCyJz1aVlqU03QBnzFsoj2taymhZF4Mwtga+dh8UoC189vM9N3mJ0tRJaGMHF9i7GRNabnD6Br4GUeVYttM4vPcnlZ+h7TRgEz62lyEgDqJAIA3uHwM2K3JA9rSGxOvtN15URKrWNH8GVVaQOQ7aweTwJgMWdx74l3+RMSG1JmUE5xbjU2phGPbE6+aEuwDKhDTviBUN4KAPs2AN/GzH9KX/9eCID9HSL6JGVfnwTgO17mYDn0gaGdCwTJ86AsqVRgFDf9iq75OtLtCKXIpuPFxBRMQK5gvh7IgCUKgIrrjG8bYyG1S5UghJZkpyg2d9sbseVVZfbgSnEQmKCvpghLCGkBYRRCcQCL0JBWs3AfGpiL+tXMRQTrcibg1ADOayaTIyhYMLD0HvuHXtmPLTo+uvod+A/ajQvqyWa3zyU+TDbvdcGflR+ZLGBieLG+I/2nFGG6hSWS3pg/iGM3db8O1uh9lkX7LN5pZ96aTkiWUBXdjq0LFvHQfhuOCPBcf1f7pPGy3IcPmtdjr6+tPol2vS1g+4gDGDN/OxF9KxF9BjN/I4DPBvBX9PEuAO/Rv1/7MscrVXQAhuhP+74B6LKBx5Cc91QbKrHuXChLZVqtvure1ke6KH1p7eiANfI+PybzRuGmKgp5stZkOgLwbKJ2vByaY2/KTyW4VfQS+YBNLgJpiIddhw4S0ujrIR5NrhwDDsmEJQMw0qVDBmCij1RbUqfg5Z1fwevapH1ZbaoRwdhWMDG9ZDXPrgOXHycNLosFm0wgkt3QC+uaBVLWZb/Rv+xalW1vJtdUqMP0gOwIoKFrPlnqrSk9tsnQ5xZ26Awgdi+n7AFpsb/VYr8Ivu3aDgnxkaVlEn+WN++VNlfwQvy1ir2ovSui/KVjBBNrLbXIKgHNNmIE4JbKW+WF/LcAfKV6IP9XAP8qpH98DRF9EYBvAfC5LzqIUX6AdOHuQGcAEA9TxRrBD6RJAT3ti3V89pmOOc2mUH1T3dlV0c0j0AGZGc0m0rWL5g43wDOTwa/ZfqgamXmp7LPc55g1bhFiKfhJIV5RJsT2V7ZzEEtNFk3fQwXq4Syh2TkrDACT1Mg60BXAbFkL0aEXq6MiIzJrZ49lyqmdQNNgmz+7xmH1WQKs/NwyP8DvQ2s1gca0LtRAz45r3+NwiJCBfuobOdrf+pqxOImiP7C/tEntfC2yntL6VNF+VE0i2HcJtC47AAaXogxOs8WyOKYKFd3BXUEMrPnJYgY1tjd4eGxbDjFy6yCZ3ZFxZE6n9ArAHinM/OcB/LgrH332mzqQziCALiliidMR9qERyzZjYl6AbB1JrwiCQB5NGgBG5ClnbLRFQh3VdPJM6JYLXQDSwfb083jsDy7d3MWYmM/0dpykpdmxLFkiJCrbLTSld5Y3sZh3CxBxnkgEYKsTjfMomXnpI8waMwVzHWqdHEAICJ3oqnk4Vc8Bzq6YjkcGFtXLDiJ59x8/jgKhLcEZzJryhtWcFPaWj5nPma+3HpIGSsxhd/0Oen7zAI9SPCbMmH6rkbySe8cgAliOQ6XoRtv6HrNn1rBQEG8HivdtOpZ9zW1382gb5e3TfQULm3VEmfBemZAfsUIgtCrmo4nAez9LGMEQCKlF1uUQBlrVTAskHsNSZFNQGfQEW/rqC5WhsyXFICabQe0iwi6S1yWAyxiNJzucBpQdXwci4Od0sEJgKumpdCtBZK+WDChhj6z5yRZnT2U+JwGWDUEsJqmnUgzUEdlTzaQOXJ8IFIcGnhtlehpfV8AbIz2MvbJjuw1o0XEqCjFqaegkoQ/UJNzBglVNv8oMhG3gomNA4qQoaZ1LW9AHe+qhMQa2vntga2ZPE0iNADTz3maP5abpd7izbwDCXa+HR8rXxcroZOKo+w4iyCqRQhiWe4zkOLvuEj7QpcKZsPfuE1/hdO8G4ENWC0yEl5WlFlvELdJJOC6q7uak94W4t1spNwVgwKXm4kGRSuNtphJgEKCJHVc4rdlmB7BLhkOHc4VZpcRCf8I+TkmjLCd+cmQEetAALTNZ/BKc5QTLsmuNrxSCMgchT8NJlGpgehKaHnLEqtchWVySuTgtvtZxw+mSXqKwtkegX/qxg3qqv6lQ+psZkV5HMhud/U0MLMwpO32wjgrG0IEvBy1ujtEEYsa8mJHqglIONVvBwSgadMy6TtECVFkDks0C4NTOhQpYj8OssV8smVVZE6iZqC7edqmTfE9Rubne4P0L2memycXZ18FDaVksWMYJv2xjv03KjQFY2PZu+ukgNDYDQDNZii4ADGVYbNK/jQgxNZllvZwHD2rYgYVT1CtXwQBbMCNS5Ldu1jEN4vyj1LHcfKQQ9Ef6ifAOWJyrCs+52yrIlQS8BmAUZqg9ih83NBEbC4/PuVEnl9TrkXpJWlNgl5hDwlTSZq8J0DitIPBQg9FhhhIAMZESWDGL48a2SROxvocWpCL40Osvpehu6oyK6vpaXjAuUfqaFWSkPoaqCQTVwwJWYLRESlIBVAJcbXJkv25p484MVq8DZ9TRxiqlat/V+jMmaG1SCEQ5AactA5MgZRA5m84m4rIsWFfZhcpYWF610TXy/5bKTQGYmFRd9YuhqUWUPTnLSDOU6TY6WHxAwTSLyFwBguYbF9MBVRcw86VOFeCUgFS3x8rnm0CMMsNDYoJhXoLhQr3P/sbw0q0V8iQFEvOW9KHidqlhzgxU2SPp96V/j0HYpvVkAGOtUzKWd/wRHx98AB2rFnelyPGs3mybMluQnrfKMWLnJp/k7qJBOAKbewlL0tD0Zs1xYwwogyFrrJiEREQwg+/6440FBUQ1OVlZfr5ctpUAKtpb5lyr8lT/1nYDFrtlZvXM0IM1wT2othh81g6LAu5ALeKBX3RD5WVZ4l4QW/dddIAbKDcFYKaFAPBObj0i4q/key58TmZLdGwJdJSdZIZ1yjEwhjYsq/h5pNQ22IyiZ7HoaC6lEszezFtyFmWBprVAtTx457ZLO4Z6iLXBHkYhs650ZgetBFKc1vw5+CB9Pg2m+brh5hGSeU0+qDKei+4lrCHSmV2CS24Pz/1uy4V60svYoM6qNk1DfFgLm4+tdWwR6u5hBPseAcEWx8TAqHdxDpXob9dipkT4DpYaE5N8QXLtD4xdgWtYhUAnNDueth+KaKTqlfTPEtI5WBUIgFVL3WMJEjVwlSxKf3gIkW1kvK4rzFuJxHphbXpD5cYALBIaemZSSkPKRrqahaxb20jyP4bnfDt2Hv39/DcVjj9H8sX1eSZYPobO5i8wxQrJLXSb5e0DgoaM6Ms0SMNDl5gVAUnPF3PFB/HhNqdLOvTgK5f7qJnB8wszKXlcfj8DzUjAlvUoy7LhDNEno8SyZJ2Vi/OcQhq0ghx4AoQDwIhD97JzExEGpcXiDJjA7aa3I382M8Pho5mIdD3rHteUKFieNHQ1ms9YblF4/qHEpDXVNyeHQkgFtgSsegBv9jzamlG7123fMLqkEHrFwN7yks0bG7k8fZ5n75zu2UBMnlqnMVp9CFngiQP40ez8Yb7OJmPu3I9df/IhTFd6ZGDToe0nCbwujnsoR3PSDsrp+0by/G4pMy6YnqyvndrFfeBIRHkC/Gu1YCrANRNwGvB2Lr68t9DRLk3ICzz2eoj7mmO7Qmc1gV8+iElvJkU0HYOIYvE/F1hKJT92Oo/VZFzmZQ2l0/jx83mn30SzzebklJ2izICdEwBcqa+3e7k5ALNATfcw1eKCqHtXoBZJns1dKyMPbi2ytiM2NqD5GN41OJ5k8CDE72Dv2aBnWYJiPy6wWdlYhachgI0RECKT6gsrAofBzXqcNIPjwMjyrJ/uyfLv62GSZhabktg9xIHyuaH7OLL/9cXnHmSqRIPlQYM0TyPruk1LvjeQhXxni0XMRMAi/Xmqd9ObuA+wivqul3oFw2/OdbEEiDR9ZvdhjD9XfqxKyIkEmdnhiNX0zYGlFu4QOcnYv4vcT72/aiscJ6gJ8K3e1BzmYJGRKum4A3sD8yb31SWr7HbeEqjeTrk5AANsQFoYAfkO3RYwKGMszA8iuEs7B6uGblBcO8jiJgAnFz6QoLFWSutjxx4KYX5KpUIuVMcxWXe5URBOrMgYib2Rx1jGHhd39Xj2E0ubzGTaGk33RBPzmnDMnwiImcmpx0Ca/Y0xpnu6NAFtAOd7Og5EjoF7NB9hEgA5C87XbSXHZcVxQtexHYe8RtMaKTP75hANu3dAslEHgLl+ygGgprM5eHE4J2YmOTUsgnpHe+RwIDO/5+9LHTIgMYD6G4v+Z92WKd+H7P04R+ALA5NJyOrJtoG7NRC7QQDLwCQ757CiVE2bJZiJ5EtuFMSqZRp1AIu0I8K8AhhsZsxmk+kfrjloPJWBVwlxymdjCjvJO3Sm+cbo7Cxxqzw9DUD0U+Ra8YESefXhnTUvxLbfXowpP+Wl6evmiF4iKyDlQWpaluWT9/pzEy9ZnNdAa2IVol06ZdTt5/K1ZW0HiME8Dgys9x0W4Bt555VtJTCbKyKqn9myf4yZzenvLRyDmX0fz4v7ctYWJmqufHb2NP8uGiBRYwO8bDJroLBvjKz3ZDvW10MgrvW73gXAtm3z67ilcoMAJoWEfkFhBXnR4hgDpKbE6ANjtzgh/QsDGxnpJQ941Sye14ya7RlE8nwy1aAfgh1w8tCwjR/AUDd9eMdmDU8XcWeHnPb5kew6ToFi9mvJnaUUQlmUm4ZpZ3FnFGqSOeNikuSFDE1pbcWol11maJCRcYE9tsl2pgw2imC0ucI4eSLNfWmJ/w6a09EjaaBCXX5nrKumqHvJnX+5ZCiXC7BW0d4AovdgXBOu0AyIM9GevaLwuprP7UCpABTpe55fjOVN5rcBP7Fk1y0FS22yAbBu6CIKijTiGMK+XjGwt7j4wEHsECQDlT21tM3G9uj6V4e2JocjMJunSQcxsx/TmA9jZjwypg6zNgIoQmJN+ctYgSoImR5MgSHveMECD3lcZ7PM7VoHtmRgskASDZYgyyFgGeajGjk+VVv2CL1GYkT+5jA73RzWO1Osc1bmrFQRXZxqfKWWUj1m4LB29Zu2K9VLMf3Q6zo0JWLZ2/HoifQ6dgSf2y3raFaCiRdMLCqZZvmar5VrLCqCUS++ncxmYbJXbPv0Opnyic0i9S3XMYl8A+O8c7zVSWbCdt23VG4OwGKRbVpeActQPADewV3yLOFBUgHvfccAY1lPksKkNZS2oC0dpTYszKDSUUbK0loi+4GDGKtHKRElTcGVzFq43EN+dQaZMfjlx/KP99cCj50aNnhHZF+WsDeb6eeZ3c8LgmQhlQwdpLO6sRhnW9P2Y2lg0lAALhNbcE3sMATtuBUFg/wi5buVPI3Q/Du7UvixL3Usy7Zrv2BnrFbGGB4SABSgSexV5Qrm7nFytswKQATM6oxkA5rTteRr8ngxXXZ0LAlOLj5hr2ee382Ao59N9zyGR+Lz4ZgGvyUdyx0hGjZEHJkvAHjEfs6sEnrebO7eWrktAKPcESn1nNCbZJYe6LyD97PTgoeHB/Qh28DXvsqeo1RQGb74F2nrelI0crc5YhCmyVxfzw1PyOwKYRoS4Rhrw/4PwB0+uAZs4AhQCYhFRHlmJgAsew7C8rAQC45lKSQDKnsdgcyc9F6LmjNTHBnHfRhb9JqXtrAgTNZBJAvFLd7O2o6V7QZby5rlxLCMK5ouhgAwhmxkYqWq3uX59T0ANYGRTX7KsKP70CWYTqCa2paMFQcIxzVmDe/IvPwMU5Vm4Jh0QbMG9CeeZMCvP2ooszc535wWyPY+iOj+OI49Zxcob6vcFoClIn1sSlClepExDElZ0sdAZ2EjrS0Yg7CsO1YW86o2WU5SmBE7G+sgo3BF2yA8Trc2rs1cKZZNwkwopPADQDIGMut6oTAVRO8ayVS09zixr0s9RYlEtrpgpyOQD3oZFAMDXXKaJQZWquUFE2+HxALLqoTJbJpATE7iVaKLyKkVv06ukle+lK6bzibyYw83Pdno0aF+2RME5sE+eGDX/PLMjFJkP0iXChzAYuPew6zjwFOugNYl07wOaDJp5DCQdG6rN2uPCwY3m3CcQkoCvgX8aqkqR/D0O7sYT2bgISYh2EsohZiRNQGYe+XB7tV83Ch+e5abBbC5GP8aOlilYw0U8CYDrw9GbTtAFTsPyZ+vg7JrLqRBIWTW5G63DiYLt2HauHZ+BCkBPOUMp2HKyKZLzOCmXXGH5rNXs5jNo2fue90WIgEbEMzFZunMBKQ2kkdsDNlCjIbfgw8wgsTGgdLaxoTW2rk9cBY2oJVF6S9sP+BCJMtwyNwhNhiHg5SFIsA+TahgjELOq0n+rngr++iuVe1dUs/s+64pY5rsqt1lR23RPEUWYObDrp9SJn2IMrM+sCmaQSxd+AWbmry02VTz9rUlcWFSunlJSLsSpbZm7V+2sY3OeqTHJER4iS89goVMSCqf3nfPDhtrIa233k65LQBL1H0yZxKChEtftynrNuMQ+hio9QEohNIalrarC7xfCrZ2ZDZh286dvFBmEV27FMbcFQys8kvtcCx90cHK3PYOYJhjyQarpzQvMaEEXGrGDNYdrLtmaADDciUXjhgmj1bwagxmy0ExtdqPzAQxsHSBeZjNcwWEJH+dRcxfT+bRYfBPOhlZ0GtH7wJgpVT01pUN9fBmmk0cgmG6r9Sm/v4lyLk5mADuwgPpAPW8By7qIBNQ1uNMV2BdQJfLWQYPnwx4uoqLJJ6y/R/QgZTxI29Cwleb4u1cbgrAGMCu9FwSplLEORFQBmleJk12qNHgvQ/wJulWmDVtSK04rSfUUaUDqHhdS/HcTzQ1flzFQETqdyA22tDZz4Ls/aLhSU/9LYbstB1R64Rdt0gbw1KuAKPr+RHBHawmnnmsslgs7xlrscXEXZgMBmiROlsWWeCLChHafT9DM6EkSNgoX5buiOx6CEyWoRThp6iKhlpJedBGrJgNHs0Fb2L3IS+ZbWs2JjNN/u666LrWgcGy+zqzLIshEM6nE0op2PddQ1/ccAXYBrd4ThhFzbODmelMGd7qVIqb5aYludYHUwfSrDQkpMcfY4CHhvgcQCQhK6wh3Gg3cPM6nI+RVz2YDcAQ1gUu4BFJFh/Oz7DtG7bzM+zbA0Y/e9XfUrkpAIN1mmTKAHC9BylUwGccyzMFcbPXdkbdFsmAaVthmfVvZpGlK8lnTrTLWQkuO1d+L09m9lpYXZiRgymW3xjj8j0gkdIKuzGaWGaKATKGxQOdZZD0scvffQejAzRQqQINYA5lblo3pybZBMBuSdqgipgz+QoHqSFFcv9kZl1QxmCgO4nY3g6pIg9tkO/dwAwAqIj5b7nezTzKAa2+7ItsotONYxWkwQrOmt0CIz6KtEaBrpT6wFQfF2xrzM+H7ageedHCbJ7vea6CALmog8NyOZ1M5TyaspM09dTo6FrB23bGvu8YfRd90cIocFvltgCM4QAGpGUk2qmyYAodIDLACftggDShXalY1xP2JzuapQK2sVkiS6UdLKyoYEAXebCQQYs8IsIuR5jadHkYg9AHA4N0ohYzV7bd0jElPU6vR2y9AfgAkOR7kk3AwGLfz+hjT+lcFMDAuMMJzLJbOZA2eiBJKVzIXmcrMLq1DAwZ6LVqVtELczFXigGVJCgcrA/b4PXAqqxchl5Iid2F5smMsenu5OI6qbVhO5/RasX5fI7rnPoM3DlBlldNew6bQwWAZYgUxgkBeGfDswl8YeKOMHEtSaMwYwFWpPufJ8FjOHU6xxAwEofF0GwcXR0dcpzed7lXLuhj0+wmAXbn8xm979jOD+jbprsi3V65KQBjaCplhIzhlW5u/QnEtEOpGQkaHnXcLUrfl2/MA8iKp6DjlIpGp+XHwhIFasJezISCSCPp1Uz0MImhO22bCdl1EHRbvhMaR1cgF/Ohi0du28UM44Ft3xS8uoLcLq71InVBag6RAmPsUpPAjOAeLw1Z1esoYdD6XmZR39EoCrjJastAN+tbM2OxzUqIDuyQYrMNUgYSB2Qwd627Ddu+4Xw+o9SK9eHB27bo0ppiubzyY0RqaNbgO5+QdGfukvrdJMgPXUhuDpPedWG5mIvsmhMHcI9sSpsUkHs7pvvz82gyT5EKQoSfwVN/Q0O94Wmp0mDsfRNtVHYSMZX18rxv83JTAAbMCoGNHzMbKYFYuL2TCAzMs+FE52dTMs43z64ZwDLLyr8bwLRdGw9ywDXNZJg8whzBqprNYQyg2/POfh92Kfs0s8s9bNsOy322bQpgbF7ILvs9+rWrxkWxW7OwzsgGigMDczE4m1AZmufZRAHfsCvYVDZNg3k9sv4PPAFX/oyZUZFAROvHwij2XUCsnivO29nNv6aXuXhbJmDQGBhm9nWFfj+J4U9D3O5B7X+++kh9UNmjmICH++Z0dHue5uMAy+v1lgG19+67xtha3JGuySc4neTowut0G+X2AEw7cWQJGGmBqoj7tRJABQtkkO7UdQYld5+bWWNuep5SiOq5Due8WMOmf488zPo9jfmLrpWIBZDYlzCuvTNGVya2D/2eMhnEsqLwuukqgz7w8PC6DN6xY9vO6L2jLpETnUg297WUKq01VE1uZzvuVEtPnDVAktlboiy07u0joos68fskBcQiW76VSn4dpFkbmczMN/OHp7qutYKLMCJLzpcHah8Dfcwidu8d2/kMooLXX38dve9gENbTCWMM3ANYEpMzYJnuI0kKMHAky75qTiI135Vh7ZuY675x7b5j7OEJda9f2kUIg8FqRnoqbW3kmAAVfJJZ2vV4zu5YWZSa6baO1CdXDbMY3ANULd21Lj6fJ6zbKTcGYHmGPpodMcNb/EvVWZ4sTXSCGhNUx8Vspr85CNdHredobl44FuQSYWzE2Bc58zD2Fc/94QK+/JXfk7vNfXuwLg/JJrBrbM8mADZ2rKiorU5r+4qmIY7UKqTZOUK8d1M5ka4Z0BAvMAO4E4mpci5NwcnET21pv3bmhQImnjM+aH2WMVBGZFMgognI9n0DEdDaA6gQ9n1F7wtqVzAyj7Wa1OOwuoq17pEMLGHUAUJDTcfMiEOni8Xkft0KVOD5PQdKP7GBl1wMH75/jXXZbwcP0OhxKwZqKWeaGvGwRePP1THfxuWmAEz6k7CRMWSvx2szqGyyQB642DUbgWw0RG6+2EzZ9x27rqnzhHD5vAnE5O98TUTkG2v452yERX9j5M5By4ATEshqGljXh3b80eN4owvAnbfupo/M+DveePYM+77hvJ2xPTyTmf/pCYuu76RS0FrDsqxYlibPm3zWdKswAypCSmWN64AV5nC8YaK3TQIzcw3R3L2dqRLl+30aUFQIVamBmZB5khiAa5oZ3HrvwLbhjTfeUNOa3BNpLHD0jgJhgaTaF6EGwE5gxmqey9t9dAyt+9E7xt7D67lvzsTGbmL9HuajsqfuwruZdboDU7IgzWzMDgFWBmbap/XXYZMxy6TG3RiteKHBsjuXb5bijadtXnKL3055SwCMiH45gH8dUmN/CcC/CuAJgK8G8OkAvgnA5zHzdz33OMcHzyalrVOzQMOIqdRkh2xsIra8gs2tbBqYvTcuBqKV+aXqbKHyhJ4RE6ibf6zYY+Yjs+S6Z5FdRNhnRrfA1j40nY6Yl2OkgZO2H9t3eQQI2yataia2hmVpvompa1+REEivlXygEiOlLPJb8bu+rI9gsJzuzz6/YGFxhLlh/RzkQEbJ5NODSWyXtk8tRSP/9YgawNsBbOczSinYzmf3TIppqR5JdblK/9Hn2cvM2iOsWS0EIjHgOeOJ5iTLntYk3Bt7c9bva1Pl4fegEyAbsDGriXjQzfQvO3uLc0rwquqjql/wGClUiHWCuODNN1E+4gBGRJ8C4JcC+FHM/AYRfQ2AzwfwowB8PTO/h4i+BMCXAPjiFx3vuCA1syNWum+ze2gxurwFSN4t6O/MjBF2Z4n//L0UJ2aFOZhWBD/auTFbAvZdF3I9ttGP0S3gVk1G80ZyN6amoNbZN2DYtx37dtZjDOxbdz3ITOFCVbbUUgBrbRENLOVHz2ajXDp7ZH6y9DyhYIBV0sk47sXE76GpfEYGctgxkynuQnV8cZYE1PTVkI04jtT5GFVWWJSCYesddVD3vQMD2MrmALZvZ+ytYt83NBZvJOm6qkg5hFlPt/aEmf2zSO9pmvz5UP1rZlnGmjJjCjNwBH5TyA75Yux7WcwPa4EP3zPRXvRFsOV6k89MCxb2BQezV0uJHj/PPRFtEOb1twH8agCfpZ9/BYD34WUBDDYQzG5Rs5AIeRcXTTiNWiQ4nInQloJa8287yPYBM8/TGOINQIjuio1gMlPUOls899xXiXnBfj+EHe4W92PsBAnA+hCWBQtAtIyZA72zeBf7kBieXUyT8/ms+w6KCVKoYlnuQAW4u3+C02nB3d2dPlas64JlEXOyFB24iOu0yAcbTdTjJkIvS5CdwWlAQNd0uQcO50RmYUUWkBsLyJH4zOYZE9SQnbA1PQwVP68MUHEu1FLQSgUXDdTVy8eQRfznBxmYrVU8WxYQGG98+M7N6LauaLWhLS0EPwMsNfO7L5bWEJcx0LcALwexPUR7C5beu8RnxfuSh95jwRS8jEEVINgS62qFZCG4Wdo3cDfzELruUX4rYc0dgy02zOLgNMVQ0XCkaqnHu4bW3Fb5iAMYM/8tIvotAL4FwBsAvo6Zv46IPoGZ36/feT8R/ZBrvyeidwN4NwC842PfOZmPRGGfZJaUn2VRnzlSQDt91lmHgjJhOpChDJTtMflMbWzC5C+Jr8q/D6ZmgGWLtG2pUIBZrH+UWyL/TMDNOr54vEy/EwDrqKTevqLbx1dCWxraIhuZ2qO1mszIFBqQxBeeXOqzWUMAKijCV/LtsjJNZ5sa46WHyZkefBKaLDU7WBagZIBVF/8tcDU4ry3TkudzzngwozNjr1X1sg3bVrGdH4AhwCHezjJdY+YhbvL58zkkIT+ypjUv8ZnNuizGk1Xeoe/FXdobIbxn8d89ljaRpjsIgR9A7rf6Xd8HWQnBKwA7FCL6QQA+B8A/AuADAP5LIvr5L/t7Zn4vgPcCwCd+8qf6AhgySjRk4SGL3aezDGwKFi8UFXCR7lA95kkbTEYdyMRjH0OWDMeuA8HSSTpX7gw2mo9dwIFLRflu5uCAdmb5knrSFQTy94GtD+y943zesG87Hh4eHLzO5wcM7lib7PlXalHQqsq6Tnjy5AmePLnH6W5Fa8NDGnK4BLMtHI81lVD9Re6x+I5C0MXtpKJ98WOwsziEn0E7gveH8HTm+ouanv7arjqtVv+ym0fMKNR10bK0a1WxfzDLukM9EhFhaw0PrQHMeL01rKeTaGkaRsLN+gNCw0t6gHkf+yaL5Pd+1oDVjtH3iVntPS1l0hCRMczU1NAHdKksBy9bYmXtYHXKknHWNyce+ogKJmZpGzMFGdKR0mSQh0ZR07FmKeAGy1thQv40AH+Tmf8uABDRfwXgnwXwd4jok5R9fRKA73jRgQjSQXOJPVM5RkIWigc0x5UtAk/+L9MSXBBlwE1DcgYl586mltmTcLNSIrTJB7x9bgLs0e09ElMxratbPJABHof3ae8duwrD+1CB2M0KSKLGWrEsDadTZlyRZdYZ2iSis+txnoeKVVY2bYaE7Vbbwg6hYxWSVQdE6oQwk3cf2PeOvg9sm2pCuvTF4rdM67MlQH6ve/falucCMlAdM+pkl8duqw4MCBKbNrbGEry57zvK+YyHhweZ0JqxUsniOtRDF+Crs5YsWJ1iqEKQn0MbzMOcPaMS9mIxh9bv4Nc8aU9p8jANw7/r98OHR2JbRris79LhL8jljsug5dsqbwWAfQuAn0hETyAm5GcD+NMAPgzgXQDeo3+/9oVHIqC28EYZ/ZaGZp/VM4AVDYQsVHQmlnV+tsGNL+fg6ECWpYBJTCWLvzKTySxLuw4iUiA1Kh/gADcPU2dzE4Q9VCLc5GJvmiDfNXGfDNbuD1lZEKsta6toS8N6WnF3d8J6WrGuK5Zl0Z1pDjtLawmzdRZ/Byx4sodZwhVcC1JV6Tb2URf7Lg6Fbes4n8/Yd9GKzDMnZtzAtg9se8e2J12omzd1d5PW0hwVFegZkQpm78JI972ndZDBZqRnaPgDi2607bL7zhtvvIExBqp7ZyvG2CX0JjFha0ZjYCbWJ6o8BaDO8V/JhMx/eQQ4plOkXqOtmkAsdIyLiTeL+5ZRokAmFDKTIYGYtGfspEU3zMLeCg3sTxHR7wXwZwHsAP4cxCR8DcDXENEXQUDuc190LCLCuiw6IerSIIulsnTQFJHnKITRYzZjNqFTV/dpw2Nw5PnzQR5751ERptdNidbOlz2gFl+mV+rXa5/nBIXDfAaTVpSXBqmZ0mWJkMU6RQK6FC9VxFWxrs3Nxddee4LT3YrT3YqlNazrEjnRzURhQHMASrqhNAiYGbvGOonbXkyU0VmDYKsPtkIRgydLmjZsDxvO24Y33ticjfFg8CP3s/dZ3D7GddmicYOm3cTxtKa1d1sbOhL4EQpZJlOZyPbt7PrUrvsjtNakb60n1CoZet2yM80tgVAEPufwh+SI8LTdmirIFrL37WLrNA/nYSASghkry51/Zpfmq7JEmaRtlDUvojCpAUYFO3CVK3pX6Ge3U94SLyQzfxmALzu8/QBhYy9dfHkJM4Z1LOZYTyzxEmJmEml+ZwE4Y2A+66RZzAApZiPdrFQbWyzLHOkVjCXdYzI5j0wssZwrUdQT83EwG9P+hn0aPOzmozEqWx60rivW04rTSTyOrVo0fqwNNVY4XRczjiZPHwIKAKuU0lEGUG1fOWAyR3ko+/Iwj12WO+3h4TWz8iJq3Rin/dUlLgY8RJHbde+xbGd0Dey0xe8sucBA4SwYRH6cPsQ/NxigUrBtG/Z9Q98XSGJLCnbuO4pcYzvJFPQJUhmTsrOL9k3vRR+B9z8nW4mZ2WvpmwFe8R0DrPQapjFG24RQr5/ZMRMjt/5/S+WmIvGJCMvSvDNIMj/zyGiSw0JoVeKbmIChaVIsB5d3UEAaPq9Bg1HrqluuyXeO3jYvLCwKrnPZdcqRStLrhjKuDFJmuu7K0DrHRqP7riET2yasZtuwWQYBuG0AagW1ENppwXK34vTkhCdP73F/f0JbqnxWC0q1yBCa9JYMXJ2DFZpZtm2x5XztFYWKRu7rPaZBMsbA+WHH+bxj23Y8PDMA252B2cqB7bxhO+/Yzjv2rUscm+pm55TquI+OWgr2ffPB759lEMwDj2QJkjW0hOoyMDq2vtsQxxg77u4WnB8aWiWcH04STqGphggyIbLYoC6+WwYITxjJQ/OxdV3k2lUQ1LWKKtyjJxN3eJ7d1J0UoACYAJ/1LZctnI117//Wp9wThACxooCV2bIEMgOtlAnAbq3cFIABiKwEAxAvY9S+zTIhTJIuEUGImzYjjiEdq6TUuomBuJcT0JmRosMFk4cf2s1KvZhsYsK0ptDbLtbIeYBjCPpuquh/YgqZI6KKEFtk04ZlaQJYNXlZk+Ylf0Nd8RsDDrM0TYPGo8chkwSVASbx0BLCKQAIgBkQCcsaziAluHMoMMvaTWE/lyalPXgMiWcaM5vpI/Lke9vlPcMgg9zYtL9nZqgyllgzKdeync/gpt5N27Vdt3uSdutqfnYHLdO7LFGhmZHxwOG1mo1JT80tktvFO5j3K86d7cCc51ANsEUTxrZysibWTEtW5l4SY729clMAZiYkIGvgMASsBuCAlYVJAFN6X/H4EYAOUEfZO4g0KHCXNW2W02mU3AHJ0y6TgVuaFfnQ31zYH5aJKS3OnhgYa/xsgJkNKlsrJwNCHwpe4pXTnWaaaH7racWyNrQl9v8j8sqAqbtkIyddr8/KFAvS87WIo0FzdIGwe/0SWq3JhGRsKuDLI+LV+t7Rt90B7OHhAdsmObu2bcOuLDNnc5CJRsw6MykFhPph8Aowkd700QyaVxnExDKGLAJ/eHhQdr9gaQuWZYT3dkh/6yOWC4VQvyezV6Ps7fjh+QixagIJuebCBNjSNjcFjdVHX3MQs/fsvg28+g5WbdCdGczqsFKZQSczCTexv9eWdt1OuS0Ag8wYDEb1TQvJAcp2ppCZR+O4zFNkAwBuOYrAS0DfN3Sf/SXNbiGNO2Igh1MAMfMZeOWlRYCxmMRmABWYGVuXNNaWsFCuRT7bNUTCHj7Dg2UjjiYzaG0VrRa0VrGukg7n7m7FehKxHsQKfuLUIAuhqHCvVBBMikGR7BkCAZru2kXyIWmb0U0PFBAtlhefWcxBAyz93XbWBfObpLrZ+45nzx5k8fl5w3Y2JiaBugZKQ7OEOjO8YDYzODHk/iSFMnRnKZEV7CdFt40zgO/7hvPDM2AMFABtWXB32nXFwoJaFxDIJxULZelD2GR4k3eRE3TVPmHoPpnaL4jAmks/TMghbeRRwcH4TRDjSSYZDtqeVmfMXlzL0DvFiBlYVc3SQqRMHWitJi3s9kDspgAM6lUZYAzL2glp9sFGzwFpfAUzNc/AIiIPHiANDRhjx+h1XoibkrzFzq5pZE/MK7Q1vcDQ0sjeJw+K9FQ5FqHujEKucaSH5bUfbkICVMVca0tDawXr0nBaFweyRQNZAfhxzcy2VNkWeUHx1EG6cLBHl49dd0qLkXcL8o0gUPPCdgUh9wq6picrCLZtw96P5mP3341pkHIEGBO5eRZm8WUGBXa7TMDB2RhFt8gm8+gd+7YBLHFuveuSGtL9IvU+w1ztCUBM2zOBXk1IBzHMjxyHQ8CU03fuYrlhYGxskiA49ZkUuuFOIluQlqQEyfkmY6gZmFUBsPC+3xaI3RSAEYBaKgozBg2MFKZgf93Dw9pblSZNSz5UKB17x6Bd0/1uGEPWlo3eQUUCOCWvOMPXW+aONIayr0utJT833cY0IWNg5oS3JTLyfe2MKdvq4AGQxLLVQmhLxWldcHdacXd/wtIqVl0i1BZSnUpGCVNVFoK0aDfqc7hmB03dU9wEyeCV09b0h4jurqXK0iV1WGTQMh1sO+84P0gmiGfPnmHvOx4e3vA8XvtZnBPbJns4ZlbBOfuCApjFtS2tOWmxVQEodKU/XOlLCiZ738APA9smCROXZUFoVcGk50XZOslofYxk3qKn0Ao1HW3tqGyLOZuJE6M0cd7DMw79jTs6cpJEi0vbMHwPBFn7WEoAZ4GYj410c9tasDRZTtZaPeilrwDsI1qy5iQsmyIGy3SmPlywtlxMQ7Uc2SlamEytmqVUd7Kpmg++EGkGUgUj3bHVhVSElmFsJRhX+lBfmEIDc+df6CE2Q0OZElKn15AO1gh6/bz4gmg/dHiTzGwiSD1cm+YPl8qIY5BF7OuaSQnRMN1IdncKrUfDE3oXlrlF2IcI+h3nh/CkhlA/ElOLjKWcGtg8nex1LvdUSDPrUtzEHIeXmU/SlpKVLHObhNiMIX7Zfd8Bgqajbqhb80E9bWOXYsDcAWP1oeeyz7xyOXsdoy9NeKF6mBXbMs+sC/O+dzcf53i5XG/WnyxtuK2RNe206VKyWsoEYLem5N8UgDFsIwzJ2oBhaUGKUGYGMBj7WaKtLRxg23bs5106AlXUKubWaV2xrifcrSvulgV3bcGi+lIpEurX1QQc7lzSXPgkubYmI8EALxeCzLyaBcCoPGuCPEaYe7UWNC5gLmijYnQFO91wkcAKXJAsAjUGIYhUqiNQLaJ3VAMB87zaxmzwjpqxjQ28mNBWYTejD7Q2UOserHOBx2EZCO0WeHtOXsUHASxZuyk56j3ifouo+75bcG4M+qr1a9ufjaGsgtKu0zOCSTYSGAgDxbZ/G/sFsxgjTEkmBnHBRgBj4OEcjok+ZPPjogeWLpYF9uER+l6VDlwckcu6woI5TMx87ZEQUzTLPsRxc5m0sGPvkhZo287CIF0eCfPf6qkWUoeEeqqbLDdbVAOTHctfReK/NYWhWUkBA42J+rq4HvFCob2IV4uKxPlUAGtrWJcFd3erPNYV67KIaVJk3Z90CmUyCZxIWVqYkJqH4mA+gnm6RkuNXEqs/R9DF6R7EkJhPYWKhixYLJouTtfAVJl9NWKei2g3hT21SpgENnvDmeUxyDaXUmSGNhNqdMa6GstijDO7+ffs2QPGCD3r4dk54tYeJO2PZczoPeK7JDWQ6l45wNMIo1+fbDgr9RIaFk33EFqShFMo2zGgoSvpvm3R9+jafnPkv9/7kD0XFl2LSaW4BpU3VTmaq6zgFVH34VX2kBg1M22OsgBVz8VmbQcN2dBo/q6xdH3TR7eVGWY6kqYZIn+Y02dZGtZWRS8lEkfD7REvLzcFYAwLtLRBZ+xHGk1ilVJUd+/om6af0X0TJTpC9tZuRWaldV0EuJaGRTMfMEmO9OJmh4nigOWlUujya5COELpcBokjiNl3GJANVznilkxcZRKWZiYihjKQal4/0eDCtOWU3z7pGRyiey4pss0r2EwxY5fLwuAG8CAXiXsR8w+AMC6KpT2SIUNA7PzsQZdDhbY1tY2amjm0gR3WA6i8jS0djLxId0P+vrFQmEXJZoLFMV0bxVGvlL+SXz/iymqt0j61+D7e7oTJsWhHkDxMqINtkbqidNooVCRLBfBELE3PtbxgthVazgJrS8tMtIcCmACU6l6F0JL3umUAu+FyUwAGIC2wJUhaF52hMYSpd8a2nX3hsOkt2IXd1CrAta4NT+7vcP/kDu94co/1dIfTumBpBa2Q7GBcIOKwzeTDDTAZXNAv4VJdMre/OZ6KTquFxXwUxqUfmkUxAK4FmgleQKPod4jBu2pAKXqaMwQlgDSdz/M9IcbMY8tF7HMGsNQKrhVrazJhdIJtQrLX4YJ+a2dsZ9JBtWHbH7BtZ5w3Eezz+s7wlI20G08swh5juHl7DK61QFxZ2pXBTutE7ypnEnNDjWMiYLYpR50OqeW4MyTNTYDsGB21VYyxojaRH1ilAt84mINF2nEthg0cnsOpn0w6n9W/yRDezbxjGIOToNsztu0B235G7xrKoeKAbUrcSnHRflkkceOd7odwWpusVgHgG9o+4ux4u5ebA7BJLLdp1jyEnt0hPGEWpWwTb1PvlS1yvlstS2lFayVETUBnQxOATbFCiKTOtoJNzWxMPjNxlBCMwl3qIHg4iIJiUT1nKAPJOhn8XE5FshKkoBUPN7mQxoTeGx9mX7+iBB4gjQUjzXCQweUgnBiDNCeA6GnKFgujqJYlgAWQ6kYmqNuF0XTMaHdLXAitjwiSIK+/Eq2EY3pkqW1DB8a1IRseZjF59y6xW5a91jrSHAEfLMv5+kFYvw4QAcRm9pq5H/3NvpNN0XlRudWZtXUhmaRrUcalVkVT07G1imr6n/ar24SvmwMwAqEJLYI1pq0xZPFmbRv2s0Vzi5gvzEt0pSd3d3j65A6vPX2Cj33tKZ48fYonT+5R24qq8VW2XMlyjZHGkWXAINiAtg6qnZ/NbT4PcjumaWCUOpAtxeFs9kEHq+4k0zVpHgHYO6RD0qJepiPril22KQ3uC97l5rFXbwJdTDqzXWseQKZZAWJmVV1MbismiEkXg0exJUJ7igPrKbuEePR6GsQzCAc8xaVNLM1ujQOubDdxW7ERE0JUtPw26t8YmJi/0l6DeRowdi9wsDKgMQ3MTD+b+OgCyPJ+pENmYcRay+TYEPNC0/eI6Wiv4UGzhEqSYnttKoksFScV7+81O8nSqp1c6iUxxFsrNwVgBKheROg2Q1m0uJqPFrNkcUi1EKiSzkANp9OKu9Mdntzf4cmTJ3j65Ame3N+DagPVRbacL4S9S/ePmc3CDkNnGCDYrhW2v6DkaRczIkw96Ryq1fvDivWdMPVEiLcNJDwZ4L6DiFELqVhtM27e5zEtzjUmhlhiBb2cqb8mM5MVMBI2hcnGMVHYjjx2nxZqsSzLBGA8eAIeY8XnYtHghG2T1D5ualL8wlLh+F4ITBiwtZiWoTQefj+Uzxoi/nBNySo96YXGrkhYkYQsdElY2WV/SkknrusjE7j48ZLk4AHUSADmvJAd6HSxmaQjT79ljewnDJ9Ew2kQcY3UxQNfWBh4pYJlWXBaxUllAHZaVizKwribWVowCjSB5eWYe7uXmwIw6bSqIYFcBzMT0szHfQ+Bs5CIla1WLG3BaT3h7nTC/d0dntzd4f7+Hvf39+KVoqqzNYEcwBKIJc3FNFcjKh4TBLiAHyK0MYQ5DXU2OSbwUlJPgGfL6Lp/JREwagdX2ZxVTLYyBZO6KZdZSVi2Ovhyrebns1bj84SbVinv1ojIeE9zpABWSlG9j5NKGJ47goSNmPlIpODGDEm1bNeVGa+yWTcUDXIP5rGxNkT7gMjXsfq95ftPP2BjQxguTWx9053EZeMUgIK1ZIeAIr/pUrHPpW0xo64TEZ5UFpC6yeDqAKjv5ZANyWGnK0ZURxQnkGTeqERYahXwWlfc2coNDRNaWsXAjjEIo8iKh+7k8IKnv63LTQEYI2Uq0Q1gfVlOMiVZgyNHHyhNPIt364rT6Q5P75/gtSf3eO3JPZ4+fYInT57g7v4ODGDvjJ0RGTe9K5scfGhcytpEIMIRvPL72QNpLKGSWagElIKOGmYOxwa8to2aHff/397bhlq3ZWeBz5hzrrX3ue97q8okJpRJsCKUHzF0q4iY7oYOpltLiQZBJUGh0EAQAtGmwUqRH+KPgGITusH+oGjTabtjYvCzCGiMUfGP8QtFEmNpJBKrjVYEW6m69+y95hzDH+NjjrXPfu+t6tz3vO+mz7zse867z/6Ya625njnGM8Z4Ri3FXF7SYybCKAWdTIQQMFVZBzEJANu5UVcW7mxl71aXaE3j0MTU0bUg+3zews3yc1AMVNtq6q2Jz1HlU55umblqgLqhEMGYitJhOXoaiP8+OSPM8+nnHvO98c1xPGrZdFY1R2/XlhsT+2rbRRDTPDnxYHo+0/Uf8/UheZQ+1jlNj7wOUeuKxJOveZ9+MdgqRawqgV2DX61gT1D0PLVaFaDWpeF4WHF3POK4Kgd2sKYurVYM0jKqTbQ0nzlP7nbGTQGYLm4r49jVFPoCxeQfwkOwi7osStqb1LLLLXtyn0bzB2j4Dh2mR/rpyaL+93ec7PXXOJ9TVLNKF3p6PhHwrnHlN0TvGi4vVjHQW8fWK6iQWmfm3hYRc90sr8xyqYDZgAMy3YZpSLglkQFMk4ZV214FBM8nzeHydAnPsH8oSeQWkLvZ7/y4OE0x0elqBSZcnNupiEsJoWMLkvQYXjc4whSmBEaX6S/BmWGCoIdqc+5a2hUujmdSDnld5M+cF2FaXJI+31+za4rrCbIyrS8CRVWJB6vWRR8ajVw0OlkLiCsGBKNbh6kXrOLXfdwUgDEL3n7rHiCZjUR7FpjbK5YCWvh8PB7x5ptv4u7uDu//wPvw/M038fzZM7xxPOK4rmitRZ0igGl9JfByI18NMVZHL5JYlQfyReZgkcl6vbu8gzSrq1qAwgUY0OcAAA0QVQzQyhKBjIG+nXF//3ZkwG/b2aJkG9alAVvX3dVC5s2tM2BnDXqUtVCJm2x0vak3T1Rl7UOpNZu57MeSUM9jWoW9T+HDi5wu4oubEIj8r/v7+5Cy6edzyEkLe49Eu69T0Xtu+0WkhLU7va5vhcQuDJPF7kZSawlOD26LSkEFQ1BRUfcFzdYNnKpLlRM8k1Zsjbg1tzvmdLxEALkgIrKKcOKxEoBFRDG562KWlhbTm/Vl0k88rKdkQdTJHtYVx3XBs+MRz493eOPuLtIm1rZoYisRNgCjELCdsQ2CKq/npJLbGDcFYMLa1JUo6UDZRfSF5K6TZ8o35wIOBxzvjkHaHw6rCgC2Es1ASyFU+xhVIzAA810de0kX57QEk0LJTZMeJDYCgIEHoAQ3it+cJS38ZH+IArdbQC4OqNFL/bLRO4iRykQsbF6mReeRvGo9AdSV1ZvLgx5b71F03k0Zwn/OtBTZ6dDPOY0w5Zy7cs6Kxwhrw1+7WWmRlhf1mZDpLJFvAl5nyP75kqoMpkBfmMY0z5smmYp+/gMAY1DxIIAybKUwQHVawdHFadbI+gUPNzEp+kosFV830yp0EMs5bAIHP0/18Y0zkfQZ8AaDRw9RRQn3EzNZ1dImPEH7eFjVAqsFa23W5Jm0vEq0o/kwfjBc4hsaNwVgLILz+YTIQncSWbTAeJriZDs0YakNh2XF3fGAN+7uDMC0X6LXiBUCxORmigAVgtINlAqAIdl3ASwR1cGNAMt5Cu8C8X9nywsSMKmgnMCAtwAMRjX3wPPm9QNounBnFQTsnpIBMouoQoaY21DNAqvRDFaLv02Kp1K41W6qnM0COp/P2EbHZgqlrrB6zbp1UFVd+glexYGFCpqhuZe6BPgxWzLmht7PoeoQHNougdiiZWNEJnuz3DKqFTNRpPoJm66cBRv88zurTJHKFTHI8mQ0675Aqm5aHtnVHqLqolc7Jue+gDk/5af0O6e3qHlv+rrdBd1Z9sGfuYYXEmDFeXe3UVNpnA9jGdacQ+satUi7GAe2WFf2FWtrJqGjIFcAcG8g5gnMNwhewI0BWOZFfJcffTMeQHcoiO3QKFjXiru7I954doc33/cmnr/5HG+++RyHO23yui4FrZGKzhHQBtBNq5y8No0EpQiKMES03lH7QCrQFOPFfE3O9G9nOmy+oqF3l/UVWPFxUamdOEYDFx5Tw9zLWpiBwWo1bZuglmFtwAjnU8coBdwYXAVbGSikIKeSN2pVrKbgikbwVM+zCQq+9da9Cg+OHpbXsGJh71C048nYC5PZSGSn82aenplC6QQNQFQRlwqDigBVQEVQ46oCxARi7SokTBhFLFNesNRqaiI1PreSVzcgcqhC3fXcVQTA3ORhvGOpVXP9NPiMSgWVqj6sHrTWanzlbCoibpFHWyCOv3lDENoBnVp5Xv/qc/QcMEqgO2sJNNLIo2P0M3pXIUj3OmQY90WIuselFpNZWnB3POLucMTdesBhXe36e6IzMDZ116u7yDc6bgrAQK7hre6BkPIbwipS6PVgerGUtFzWBeuqvRKPx+Ps1tOaytGY9xHibxbd2qc8WF5WRAY8t4fDfZnG1STekXZqz2fyW3SflpkOkSh+6n9lgljU5033zz+Ah5L2YwCEgcqas8QgSEHkjjEZbNUJsJooqzxXNz37yyxzyLzJglPzYETUhu5vBILMBhRk6SEkyv1VVb2orRifaAqqUB5pDGsqRYCw5nzp7xJSMK3W5H7ptw+32Jh33bKHaDx5dFclme4gHGDMGqmkFpe6ZSU+W0TCOnbaQHYUw8ORXUvyawbHXYkPc3jUUzU5Mdf+Cn244NjY3GejS3LRdm0RbWxWdVJUeSCArJaCkWpmA3Cv0R6v8bgpAKul4s1nzwAIzqeTNmHgbtX5W+wwd4cjWit49vwO73//+/D+D7wP73//+/Hs+XM8f/M56rqgLovpy4uGpp1fsR2tmRRwqzDLi6OwW/tDqooBy2ydUIiAqkmZANRiCNbM/j9bieOyo3M8J0bplIJaG1pdsS4HHNYjCqlG1WJ8V2tr3GQwY4chAKkUdSECSdHihSLAUkGkDx8FFQUqye1ZV7VUCApKnYDqOU252xL5g6YuWIgRDm+NNoG5Wi1oXYvm7A0X6LN2aGqezCz3sATXyHeqpsNfqUTKS7eaQN7087Ztw+l8Rt863n77pF0FCAECZL0VsCK6mjfrEbksC+rSVLrbekYGP8mzsoApN8qwz+W0gZk15SkKvWuWvXsMl8PVKSRxjls/Y2xnbP2Evp2jCznA6toWQltKdGI/LAes64rDYcFqHdoP6zJVUHSJ4twK6rDGJb5pitwaft0WgFEhHNcVAgF3dZm8zGbbNlWZtJqvdTUz+m4mq969odwXtaoLWLdVBRnRnC+45WUuZC2EUYAiRZVgjeAn5zzEdNeN/CAQmGTusAB2u7Pjl+zTP4ISsZfPZNSKUrWIuLUlPkALi7WBL3luGVJBtL+SCEN45k/Yl1BM2msKU02lW1egB8oWnoAKSvWH9ncWRj9v2heyA5soyV2KfheRgqWIzpmboEWBd3I5LUAgkgCs9wsA0w1DRmqo4eeTJTTKtPvR2QBsbh5EhCIzSbmU1CglHupK+hVke5+nLWjN6pQ+FQcgmRSp+ZMxr5y8PE1oseCFPheiiexWsW7SboX5+z3Q4Cqr3gO02dpwUK61ReDDm9p6rWr2Hm8MuwDcGIAVItzdHQEguth4WsH5fEKhFa0VLGvD8bji+bNneP7sGZ49e4Znb76B58+f4Y1ndxBStQm2m0Zb6U3wUqE3c3EKUIsrd+o8NLdIrR1C0nMyj4TSIyKRu6Jwd10kLWyEO6NvUdalFgWvuixY14PJ3Ghz2Vpn/WMQwjDdLkThE6QMSCFIyZIv09qK30VD+MXAjawRiEoP61IhAtalILseLjDIzNhOJ0tw1YYZXvQDTCFCENnCUxfKraIRChVyxQIbewCzT80WHw+GMCKKup21Q/j9/Un5xprmDUSHK8+7q63Gwwugc+IxLMDioxa1zMWURgReGysWYDKrys65xnNm7eOMaOtzYq8NAt9SVbQ36AQxMZ62Fg2UtFKxVI0+L2ZFquuoVElbWnSc0moEjnUza3KnYsctjfcMwIjoewB8A4DPiMjX2HNfBODPAPgQgH8J4HeJyL+3v30cwLdAfbFvF5Ef/jy+w4TlBM0KV10nqW8bsGr+02FZtFzo7ojj3QHH44rD4YD1sKItLUo2YECEwcbRTN5FwUtQCzQdwaBpsHgvGUQmOCnYOEMSO7upm+rOXUDvsjp0RjR3bKg/OxNx1+k+mb7Tsiih7Yu6CEfdoEaWPIJGtgub9VK8H2BexB5h5NADK1SnNVL19+PBpZYnV1eKNsTIeVDLssAFAUNF1X6KY2f63u1k+WDbDBw43LKBV4C+bRwBYH0qvUYDkt3a0fSVYnr6pVYslszsN7wfY55vPjdIDxKvq5xr851HXN34l32wpWLsI64Byr1H5FEbvLiySkG1QETM3RRVllqwGA+2NBfHVAu5CEOjliUUNnzqvg/e0ngvLbDvBfAnAPyp9Nx3APhREfmjRPQd9u+PEdFXA/gmAL8SwC8C8NeI6JeKijG9cBBgJr1yVbu8maGdhkpBdFdeVsu0t2x7dw1gETUiI0xpLiwq0AgYpZ256FUdjMhaJpoJq2bnOAcbdg38RpV5o8vVRRJMrq2mAoHpNJHVOtbJxxQjl5dW0RYltDWQoSkgTvN7NnxO0CylRhs0167KwJrdWkrzLlSCGFZL0G9as66g5PdYFo1kegSPCK22nb4+aJ5H0BRKxFD3l3vfAUeUHpkF4WcMBmC9+w3PE7ySpVks/aGWgtrUJS8XoBXAlUB2cncTOCeI2QTsfO8uqUx3cE42EfWXI5H2uflMpK5kntQWWPRH8PVQZ9ehYkXylWb/B99MScyNdOFLP0tE0/W9ofGeAZiI/C0i+tDF098I4Ovs9/8TwN8E8DF7/gdE5ATgp4nopwD8OgB/+92+h0jS7oUwiSGaNrDUgsNxwfF4wLPjHQ7rAcvSUKuKGZI3ESWE/HIlD22bYivpiaExi56JNB0MBDSB9sYVS7TU204XLkNTG6iglaYOggCeaMmSQMz9VZTo1k08IDQSnwNokmXDsh7RhMPFWapyfbUUkKUyVJrKE55WElniBByXhlrU1fBjltT/0Xd/AKBatRGwLfS1LTgcVnzgfc9jB58JxerKGw2EQgVj0xtwXVa0VTuHOwhHsTPPOsNhpUoAIrH2fK+pHafTKUQQo2hdYBzRQD/14ELH0IqJVhpoKZCjoDSzuA6qONKsAmM9HnE8HtXCbYvp36cYse2pIlbxIV0reObC17lg1iS6Uzi5rofA4NZkvEpEGytbAEKb/Z71mD1tgtk07PVb1NLSddCK5v6VMpvUhCVpICZk1AjEQLvZQwGcxYvYb2e8bA7sy0TkZwFARH6WiL7Unv9yAD+WXvdpe+7BIKJvBfCtAPDFX/wLdWGx74KeyIogVV3P3QXcVD++WG6Yf6ZbULp3qr686I7EvqvqDlyLKqcKpiVWi7psUnwKYgscENEFooS1fZIBpMi0HECJhSpk3BMwrKRoV4ITu2wFUNCWxSoMtC9kNRVOB2MHMO4jOBU3D2urukuXAua9RaDHicnFmWsTrqhZget6mG6JWQmDNCpb64ZWG0YdaLWBSbCu6rpr5/BlApiBtqpsaMS1U7dLO0Gxb9r8VrtPz/IsdSGndHhUELgFWbSUalkW1NYUwFbVfVvMHV8MyBxYM0cW7l66Fm6d5iqLB0aLEf17KPB1NUHLrbScbe85bLn/pGf8Q6bYJcpskebre2rCTatrb0km1tOP80GQ5rbGqyLxr52nq9AvIp8A8AkA+CUf+rCQLx4Ln+eIkN/kHjrWG8ZcBbvIsOYcBYowYuYQQW8asixwF4jzMhMAkGINcdXU0aPo2rGoj2G0WgGkgEhD1krukjWlERTbvZn0ewhFM+ZtwftCGiKWu4RwG11ra121PORwWHFYViObLZLm7gUAtmRFiZuA7Vy5wKKG4x14w7fDtEjHYDTWVINWF6zLAXfHN8LlctWPrWyopVt3Ij0n67pBBDge77TvwLpiPazadSfxPN0y/9+u9+hlA0StQW/J1s9nnO9P6Ns2u/+4ceM3eVew7b2Ho+Y3dGtNxRaPB6zHA9qillfomBnAucubV2h2YSWARKYXCYRbG5ZWoJMns/KMMsI3hBlNJFvPkWnfN4yeBR+9aYdzq2ojLs5xtRkxbcXFLBGilsWijQTvvcAJ7JLLjKc0isvxb4nog2Z9fRDAZ+z5TwP4yvS6rwDwr9/tw5ToPUXkUUPrtiDIEhFh5rPn9eySEW2XIeO6nKAKwsdlSQCuWho0LLpERbPwicWIVAMjK28h4280FcAWLnmJTQmKS0l6BAkU/EleODSTWEtRaR1voUZQi6ItesO14G+s9tN+Emm+mpCKPxYiiFkkkUJBAqKK2ha0IVgPR9TW0Ee37tR6nmppWlaDYiqretNJEXC3nKXN+kCeGf3M6OeB7aTHv5UOEgJJtZvHO32rJJInz46NMTYBd4A7MDrAXZNaVUlGy5fC+hF3QcUE+szl1rsVhWp0JG+L5Xclkl7JfNL8YO19Z+c/Pv7BUGvFOU8JazXIfAlCwS4zIVJO4FZuqnccpkg7OrZ+BnsD4TFig1GBS1sP1dYGibmPxSKPNTjKy2BEBE48uJQOcJZrTdrilsbLBrBPAvgogD9qP/9Sev5PE9F3Q0n8DwP4u+/2YSKiuzAzuGtHbfECbjLl0WhMW3a7DDla2KAMHkG0G45ZvZhbJQowhMbqeg5DHCLVEItCXFvQCmKwqGT6EpunZ+7Pusk0KPZlA+VqRhGZZtTMAfMO1eFCEOKGJQikFi13KiXE92IBezOUIsqbtIZlWZXkrzVafKmLbeBlFhp3AVUBxlAAY+vCHSDkD01k7bVDP2GglGGWmzVo9f4FXfQ9XSABWKEYE4yBppZNQts5NDAb4PgRWvCjuIWilniOLk6VjuTb07wQuRxI10x2Mc0VtDnEa+z1hZyImARYQXiCcFmc6LLNY7ZLS/pfUyrH1VY9n06iKbMW7tfgvXTDvjxOdxHnovNZa8SXg5m5pfFeplF8P5Sw/xIi+jSAPwwFrh8kom8B8DMAficAiMhPENEPAvgnADqAb3u3CCSg8iif++xnwSI4vX2P7bzBG0Q0anrzWdqB7kDN0gYmcjkvlWDCdkl9iirFXwWkkjRM1lNxoLMAtYSg4rAyFh4ECIMxNNteisFXMfnnipDnSxYYAFNhiBMZrg1RAUqqh7TIqUdUSzG5FwctA7q54q2cwIMFnlrgN4G5THd3d2GdDKuDRCLZtWKhoG+CMzo+V9+Km3t0JZj70FSWz332Lbx9f4/T/QlvvXWyTQdYWseybDgcN5RaIEPMTR6hsnD/1hnnU8d2FowzgzcGjQLiAkJDIbF2dx2w2lev4XftiwD7dB5rqyitmjvvUtEdAlXhbW4lpfflki7/mfPYSEiDHRZU8nUYaa+UDLpioOraXeYuinVyGv2MftZmtXr+N7Al7ro4pHKOgkaAJ/Gu1iZtbRYgWYpa5stMZi0WwSbA5uvFcupJDO76GENThP7/CmAi8s0v+NPXv+D13wXgu76g72DB+XQ2lYQtSGqSqbbgSgiqBT8XohoUbudPAl2griS5V6XUmO2guvgaOYdVARawF+CC0QqpkgX57sbQ1DYBs36D5jlOt82mkI9sR/ITkfV+FLPWbActjkspjwdzJ/XAw97SnAfm5HQcKBDWF6wx7xgNtQ2LiGlUkux8aZKo4HzqaikyTxdyaNb7+dSxnTrO5w3nkzUTlgJZYDWNHkDQnX+AQ2GUN/u8LuBhp1JrquDK/loCNQApEWAQcevCri3R7hyVatUK6eR4R3ORonl9krLhdX3uNz7T9VLxELNE2Te6sGWC/pry4zOiOT/P5s0z6utyQmzNWyR6Per1ipQJmsXbnhY0j3W+ziskvIg/1kJEPmVaXqzWMJt1e0vjpjLxmRlvffZzUevWjQMjkJnSDUtdlK+hGpwYoYQ2FOAcwDTFpZawONzMd7UZFEI10RIqyiuVytg6o3SK1IGtCDppOgdEIOTt4StAbCQ7gDbVKGIhG/cQ+FrUyhLZd8Fxa9LdhF2WuFl2boDp6+3+rx7cT/pZmLpkS6loIljWAzwjfjspH7Odz3B0FSaMLnirn8J148EmuKdk/Ntvn/H22yfc35/w1mfvARHwQdDaQF8GtvtN5+1cEGne1xgD/TTQzww+C6QTZFRACkgKCmlEkwCArSkFe9Nay2k3oJ1NRlpYYOSJtHZzqxKuBU+YtW2caGmQJxMI3G0EKlWwsBc2ApxSLSAw/Z8UXbRVlukJaKa+Zu8PMKtM+NjO6JsGKUbv6NvJqg6c/xJQbaYuIkrcF8LSLI3CpKJjXSR6IfhfyuVMiKBSHwObpaxMF/x2xk0BGERCVvmyrZdbXpGsWfYk5uRw7KPiF48K+Q41rS93tXxRVnMJWMga0EJbVIkuKmZ9iHhul/E1qoOgu7dYqmkmgMHh9kXz1uIgNxeUW0LefTuiS1YC5S+Pm8hdF9ZjE4tQ+B4bzVaDu6MAQ3VflQ+bqRTFfTSdszdU0XCukfQNhRbUwliagnurq7lA6gqSpJuEBdrIl0BoVv3AqFVJ5VFX3YBE50+FwFWt4IGi51MYqpJrqTCW+7R/kLnbsN/9POJhJ3M7J5ft73YRyeDhEoMP7LhDoWJJgByKJzympr13mhp9anz5BqibK1mOX0GxqHaBWBrMtDJbK9GsY1m0cUe2vPL883EoBzl7LjwB2EseIkia5jMnx/dCDwmH9eWLN9wnM+5lD2CE6b759ctWTPydLFXBbmCCCgSK6CJi9oYVqlKhgU2V/AnNKOuo5IEB2FzEeRNbpByux35BKUAnLzA9yDgid40zr0OF4vsg/o2pMYXPYZoeduNXfYJd/czmI+mLU3lBoYZCTQGsHgAArS7zhsucZLr5PWBRiqBWRqvNpHMaBjRPT7RvmMpHgw3YrPwGXT/L0kmK61x5SNZBzAHL/00PH3O9zSTgfOPnn/k1frWmG+rprBwu6uzsNCaYuSXrxL2xVKWo5VeslR5hlrtF8+KSVFjbVOMticTfzT+CB/PhuXSBxzc0bgzAWCV0YMS3kxAEcxtUsaG1Ba36z4ZaliihETPzFcR2+dJq+aQF7DyRb0xNsyFQCzBKwajFbjKVq2l2c503QWfBtvG8RwUAVUB6II+nNTofA+jCBHkqREYomwTBNNB9eklDXh2osBz9GAgE0uZ/Ea1TDkp5j8hjSmDpKQhUASG1d+B1nXG2zPoimL68YGkEHApaWXFoq2JK8V6VBqTQPDevH1UpZ0ZtB4CaWXIFtWyAEPrYlOeSAkIP0pzLgGfIBzFNYukRBaVpCVOtpv1maROlEKjN2kx/PSmRaUCTgOkCyIZo9HuYBHaA/8WIjSG8haHuYj/jfD5prtemP8fWMbZNpXJGDytxad5d2yKaIijWyHapxZrXaoXEuixYD6tWPiQS30cGzt412/+0bTidN9yfe8gf3dK4MQBDlLnMnW/u4LqjGomfc2A8IdVNrLTV+MIjILUfoxmtjHvaXKv0HEH2FphUCLfQa+qY1k24XBjqYhpBFVaZW5L28V6nNoMQ/qVkmKG5QDEo82BKGKt3pxpVFO6vWbLYZ5Unu8MPzkglMkAscO+I5qvCCiSLeNZSIVXnVy3q5675dG49BpZqUEGgUrWUqwmqgUOtDSKCUjsKa3OOIg1EagGLuCabbkhM0yUMC7Tsf6eSkje9JjCRh/m8XAUxFishs3MYZ0Pm/ymdKxFgl2k/gWS6pCYfZDkjfu1dLqfVEgAW25+5kiGd8yC1Zq+2uneDZwlXT4GEWzPBbgrAAHkAYAIXjlFSukYJkScuau2f8i8VUV0Ns3z8oymoff0/KaDN/ZXCA6vQxFIuwMI1SGFXCSU64bx19M1uKwYgekMKhyNgCwkTDeAR1DrdG5ubE8owYDLjK9xOvVucULbzYZUGo2rYH8XbizGGQ4jxXxRncQK72GdRqXru3JJgB0kV1QtYIkGlglY1UVXWEW4T3HWxawfW2j5mTWMhESxtAZeqli0JqBeMwaAIiOjB1jrFE5ktox/mqtmNTaQWDOrku1Bhv09eLHNJscrM1YI4+Dx0Izmr1e5WaLZj2WpcrSJidLO2tugqlAEtWqXBpMDJlCVawfGw6lUJ/X0OSaXWZlnZYgqsnvO2OyaewZa+adVE37QK4nw+RyrILY0bAzDAy2TDL7uyYzywwHw3Rn6rl4O4heCWx1x+0wLypAl9s0CJfK8oQlE9crLPrbWgskaKWDSKpNIrrDlMKilnWEOTZyNXhDWlC3cFzZ30sLzv7kKT/Ie9XzuLJDcYapF4smXwQpmDR7LC/KZ0FYQxwoBUt12sbl2moixr2RFYIGP2kmQZe7LbORggmntMMOBQGx0eZR5W1Dy6dRSawoVxrTzFwHQ4GLLnuZwsDNLQLr8ftz1YZEbqrq06B2DbAGaBfLJHiaKW1t8DMXFGa4nmoMvcJw8Wu1jiA83qaqY40ixYxCYTrqZmfu3UAbt0HXPwYeweI45lr+BxO+PmAGxmQHPcFPrvfPEpsrA9CdRdBLfasvvpOT9+A0+w81we/WZC+kqxdAv7XAUwzUlqhcBFxRCZoQ0pkiSKOICZRlhEi4paNoXm70GDeUTRrEPWzFq4n6IWGqxq6cIq8Gp36OeQ1WqqOypwdWM/JyxTWNBByzv8aNY7DNAUsKKch7W06IXROucfMQFsjBEAthkHNFg5mjE6zl27gGvLN2uH5huXgbFeO5cGEnNpJyk/z8a01yVABskt81fZ9c4g5+DFnNaP4WIp8xgTnjnIqzjjiHSREcDl7dGUcrCv1TVRpsCiixIKs5asiVu9enylptdZ3Wcm8HeEPQ/0eOzJ/Hwf3cq4OQC7HCLz5pwb854byDxAHnnBZhDLHE9+XYiqYho+BNcgUz6mCWGpBGGNDmkBt2DIprV87OR9BdUGKhVLW9RdJY8eOXdBntr9IOpIxdrBx8QeGF5wR9AjoOwvssCiivD4DepAxDOZcjDGuZtF5s2ExevhDdjUMumbWmyjj1km1E0TP5RA5vFw15vJS2iYGWfLRFdJnE1vNgO1zpu6/AbwBC8Rs22GVdqILCWF7BzBiXnLIXO55sLTevF0iZ3Wf6HopLdLPRgDHvzZvd6DL8KxnsQsbh4GzN37N7gIo9Xzmk6+A2klsu7a2h5tXRvWwwIZXWtMx/QcyDmy1qLb/OW69/nnwvncTV1ldObrbmncPIABvrnPE+/u1r52zf6GPTjtPsfdySCX/dUT2ABYDWOqUYPyZbUUcBHVHmPnVwxYgshVXooJkdCpUX57LlxKsxyMnNcb0mZUSPOniOLvcdzJTw5rExIukxuUTvJreoVbcZaS4JaGAYx4X0JLHvXmD2GBDUaPmkZveqs6XRCx8iyzIK1JiINcZ1McHYytq7voXceV3+pg67weET8/7gjO2Ge7TxxPZdR3bs/XTLLc3SW33xPZkNYGwvJif53ss/Xzu2at4+yS5FZnPG8Pr0+chzUz6D0ptdYChitLzABPLn+K0ikDr2t5bbNBsW1Ubj2LPAHYYw8/4er6IfFhevHmhYS5hXsgQrw6PhHAflHG32X/PrfGmtXCafNTFZdjC0e3phEylbixQD/3SHxgFmjy5goqYvebu608b0w3xspclFIneGV3Zh6JzLSvdJDkGmYF/j/ttGPfId7nUMYEEC95cSUI48fG0NKfaXmJ6XKNKC0C5ryJyuSeLJ+v8whC+367V1L5fMJs1DH2J9zPBbkstV2zSoCUKQPh56s4oD3cugSCgWGlZGQWnAOElqIJOe9ktZtQDkzL12B0e7J0Up7Y4CkXNLryesKbkfkj1olvgqApg+Mt0VoKSg0AXDZd16ZrX8sMWIUFVmaKiAeYXMHFCfvz+axa+9Yn06OStzZuDMD2+Tjxb9rzPkT73QkIGit+V6Mo7Jzd3/QzLnfWHPK3//t7yDyA2OlN78l4ESIxl5bQpJoqK83cI29pRrPXn2S4DflqSV/ovSoTsAY629lwsi7VX6obadaLyHSx7HVu6cVdtTM99TOd0FblhJkMKWPfVm2wZnd7FQQRR84rG/c1eOysPRa2DkL73CrPH4MLURKpW0hmC4umlbh1tne355q4uIhGCVwAm0iAVy7x8jfM1JY5x4i0mnaXq9tOfXtVmoDlL4o38YC7sq6mAm3UYcClYpXWuKUQhkdMa40+kK4Fdhm4ymt4Zt6Pnc7YvFYekX6ywF7ykHgEEW/mf5x6u/kiwneFA9s7FjpK3LH2Go/8pfcAHsSTAEUR/T69sYGpFWU9AMGoVe8kJfABoQKqVTPdRQDyDGz/rnQHxlxSXplZH9ODDOdoZor48ybSqN+r56kUjaQG8Wyv8QieHiS5fIdG9NgAyMlgbxxrcjiSrA7NL+oANEJ4efWUvHcSW69l5wEGg4vMYLO5S7Ua/MY1tXM/X6YbkqnmUgLf+XpPS3Fgd0CbFtR0NdUim8/jwVrK7mhE8pycH6kVWjeeb2yWBuGWpSmNFD0+zfogLEVL1NZQHVGgGqJgxVVTghZL22muVpF+0sXcggNLAKZc47xe+VhvZdwcgO2WD0Fdr2JtxoqgFIELznmCqL16GjDuJmTeDGZwWChP730K19M5DxBCN1+c0M/4SB7GR7iOEEYtnhVu1gO0GEZdE81yD4vJFRiMpPX5+e8z802/Q2DRtPBzFXjYunOLmkFwWIS3cSPNgEcxHoasEqAYP1dFC6HtZh6wAmAeERHcjOtyaaPhRdbME5xGzxcOIg5gvAMwtcamCq32vDQFBrdUw6L0c+VWUHrewa2YdW5ARXGRMaVuYl4XN6544+IZRVWvuxiRr+dT4xuZ31LQGtsWxdnns8nkbJtuaCZ0FuVCdozN5L4PrWJZG2or0WWIoODWaoXUCgZm7aPnO9acfT+rTASMIQNDBs79jPv7+3AhT6cNm6nohn7dDY3bA7Dk7sQiJgRokf/NFqakzkH7RUrp/wZq6b9pwVx+uQTZGt9hmzkSiMSub6MUANauXsx86KZu5xruns8wd3kymPPn0nHtjuDi5tu5fPk4krvs1tnO3XL3S4MPwhaZK7CcM/0gz4Vysp+DjLcEVrdIPMfoEhvMGtCAwXz9EN7PEZ7LZXI2dgwCr1yQeC5bYyBJEkbuIqqLGUAGug5euw1P0iNPa+5e/rpJ2s/k2p6ijpHvFYoWxn25FBMpxeCdk8KiIopoKyxQNLzq4UXqq5Zyo/waWdXFtI633iP7fnZyShvBDY2bAzCJxqB5N7aVSNP6mjenBEEO4wXU8LrmROI6cNlwUZrEE5tbZm8rYth5+f2aZqHt0Rpg8jyFGZ0Fsg24mI+23HWLocC1xXTGbh3WmIc/X+2e2pXnOP+X8q+8/EjcChG1AABgVDLxRU3GVYyrIFF9rkwhuQvPoru7huJN0NEBKVgiz4Nz4T+rI0zujX5Wh+fFefmXdpLSk++OkYgWcYOvXCjKDwcswr7sKhysi+sfyA6vUJjzw9VkT7fOPBCx9a6y0NumuWx9A/M263avWP3enGNdFhUoXFes1nHIdczCGi/A0hq4FByWpu9pbVf7SITYNESSZM624f50wlv393j7dMLpfFaQtQoJlya6pXFzAOb7qS9ML2tBSa7DtbddWETurvl6uvyb+C69e3rybHT5Pf71OWhgr1NKqURYHDDdqaE31vDcqqvIaRYAc2Tez8oCin+Le54y5yAyrauAdbO8irlWUlydAyjVrJxSwFZ+VKzcoHqBtKgbLJhigVUqauVp8TgXWNKNb6F64jIDLx4Q8BuHPTXFTdzr19LhPNxlma/3NaBTSekGyXo1/3l++jUjdvfv+QJfA7sE3Vzf6NLanrAaxPhMzdHKD792EiVoqihh+l5+zol2azOT9DVl3ee6RwHivOb8Nc//8k7fW7cIpMyAwq2NGwMwCZcwyn+Mb/LtLEtNvePn4OFuqn+yhe6Ce/7cg0/AzlIr6flQxMTUdPKGo7WZVDQVlCq2+6nOPnvgUo8wuSdWPkNDI3lFQKSaZwieTPma3Y3nKFuKJSoiAF7KtMq8VyWhgC0iWk09VYX7CiAFLKqIOkYFuoOmRKoCs7qBbOVFjr3D0gVyZv4YdT5n+WeFi1kqxl8iWZQCRGQ3sMvCGjv32uxUmtfBX0vhehIs2hJW6OVmBVteuzIzv8aW/OluI5iDsN+2bqKEW2Tb7z6WzJIkB9cSfTqXZcG6NBwPB40wGr+ry2AWeLfWAJHEf00Qiyg9EC6+u7On0wn3pzPeevuE+/OG83nDZmVaYovj+ib6+o4bAzDE1hiMiFkjM9cI8BXPMiCoAVZ6kfa7zUzLsN937gVdBbmQ3HGrJ+YGC9l7OgKZdpNaN07KEmn0kQajsNVZsqAPzQ3zekF1Cd2i6Tp3Uv6jWLTQEhTiRnUCHDCX1xax3qcCT9FgiFpf8BIou2GFUCpAbOxbK0aN1UgVaEsDkQvvaeIumfCh3wxiHYdEBDQUNcXkuEUEtdfgwLwHYh9JqlpIgTsKzvVBcD5Rj11i94JtOGOug739bDhkcwr3GgGAsgMxMqpL4u8+cvmNJ6M6gHVrzNFN015ErF52doSaqhKsQoWWCrGuKw4GYMVIAoJGp8FOSbi0NO2KtommOu8MiEzwUumcM+5P97i/v8fpdNJsfAvG3Kb9dcMApviSXAD/m3EfQu4i5kszicrLzGNVI40PfuHXu/uAhHO5Kcj0JRFh98istrIPr9UEzP2D1TaCMUhA1lyBAes87vyEl5sQQC45XINLyZED/9w4PlZbht2FdJeyEFgGiGdmPou6lpCCWm1PZpkuZK92zswaKqSaYSKoltnNzKCirmMkrAkwnCMj3WBYBkb3+fJMvuW5gYibcmFd6MkmKskSmxG0cBszbRDXFuG+Ij0VF/Ti0ivc7Dc89sewLPudrr3lgJll5l2JyHTKvC+jyeNCJaNNWdVECde2wJOJYcS/E/KaMD1ls6f76FSCnq/odu6pE8bPnVMZUS4l2ung3dC4OQCTIKFlDxjmFqpQDMMpZXkAYumzZL8b2we/C4ZdftbDFxYUNPLwv1lfJkq3NC/5UN6DWVA3bZZRBOikeVCD3f0ChlE8Dr9qN7GWhFtWuHahdrdE5xExPTKgig5DjgeqUlFRlUMsDOECKUAn655dSSNVpmbaugLwGIy+VWxdZYmXVJbCrLyen+NtDHg/seGRyW4WAnuUjrH1Zo1COrqVEvFAWGmRauIVCaUgMvFNg57lWiHQ9TFzg+caQrLIAPKadmzD2scNhnQrBxr7qCNHVrse2z5NhywwYnUYg2Ot5YbM67rieDyCuaP3M1g6RAjMQyWxqQRpv64HFTSs2lJtFm+r9bttynudThvuTxtO92ec7jWNwgn8bTCGPFzVtzJuC8AmGQK/nXVnKhDTRPfnJIXn5WIhXSMs9yBHD0n6F7xeLjkSme6tWwCWWoUpUKeWmLsEjZXfcYtmWFa5iw+qGzUjS5EuYk0fPOo5uR5MnsjmsfOODegEMEULV0Kw7HP3kJnARCiFtQEIEUbVnzxYLYA+ddUzeDG7WyogAzBhBVwRBlsRdRlqjXJl49EGuBDQCcykGfliLqWdkGKRyULuGHtcONcvTCuZrlxJB/o9YRAXF96AVnkkt2ismHun4rDn9vJamDWLVnhNM03HU2nc/b9syEFgcLKiIxeNtNC7lhaNbGf3bXpI3Jul1S1fz7Puh1lec5U/WWCPMGRmkRpHAJg7AnNBjDe5FJwLgNrTGfG34I0uoj7vCGL65okM6guGBWjOkFpKpqLQClCqcmFFYPWQegOXSqiDMVglqZmNS+uAZvX7/eG7t7oZ5KfCXCq3svL8w3qLmyohtJcVVT0eZsbSFHDYebkhWKxQe13VwnDri4dgs/wvBTIYiLF1vmHr/TiU5BcBb7OUpZorsyw1avaWsYFHAw/lysbosTHpZTRH0vk7U/kobj3FfSnpTJgrF262Ph+5f/Yytuuyy3Pz0ptu4oTsxzqLtecas+JqYFrhFx6qzQCuXafdk1ooUHQSVbGwNSaDlUcjlZlel4bjuu66L5VStDmHgdd523A6n3Hy2sfe9WEZ+Dpnn/ITgD3OKLZrWJhrNsnQm1kiu3uYoJ5xCbvykQw4xlEI5iLO5ktc4XlTZLd0X7Zjb5VsFekCJzAKGJVUO3+pFUO0ULgQwOwlI2rB9KGNRgu0eQVBQCNxY3DLjKOBiJ2gwNQITOASmCkVTRl47azIEv7ntDIErSvA1EWVZBcrIdoFIFgBeLiFIoKtT7K7W2qBd1gfQ1VBVW6mhehf7w3MHdybXsthwGHEPyxQINzNO6V5Od2EjBzBKZXkSaFRW5kvnlCyuPRcZ92yvZaXBHhNzXuENUW1JgDz2c1pkXm/BSo64M04HIxEpihhjuDm3g/LsgT4uQSTEvgKuOfz2SKPJ5zOJ+W/utWvirhXr3NK0kC3NN7LztzfA+AbAHxGRL7GnvvjAH4rgDOAfwHg94rI/2t/+ziAb4Fman67iPzwu38JotZNXBUUbmG5C5ktsFw0O12KGEHk7n/ml3kGvHNJuGLV7VM35meRiy4aGrgVVj2x1XIxi2l7EWlya2UrAB+ejiBAtSCD6BfuKgyEIUxWEgSIVFyOLCkEmBCjHx9P8CY/fs+75BI3dakKZlRFuaEhGGapTQCTneoni6B0v9EFxawtPWYFhlKLWmfmVtbeUSqB2RunDMgoEfXTInGB9IGBAnLFCnFLKskhUZI9stQWzxHzRZXVWQO03Qrz4+DpfmkHoWThGxJMC5cAAzCy6+4uf0zLrUEUS2R1PXv9qRvaXlXVgVhlpJs2K0n6/v46txgdxDbL+4raVZ5dvWwRPFgvtzLeSwvsewH8CQB/Kj33IwA+LiKdiP4YgI8D+BgRfTWAbwLwKwH8IgB/jYh+qYR2yruN5D66BWZEZ+ySnOrTMhe2i2ZNEv9hVOpdZmCv90TFHFXMSBHyhaQLuVYXW9TXKq1TwEXBuZr7RVQxirs9UCKeBIWBjd3qzOeCQFL23Fc6Wz7CAqFkhZk1UkrmgczoFI+SwmSiBbWamzUYvavFWMbkv4o9qiVJlmYWizBqb+p+lhKWAhGBawUI2syjEuogMFcMAzLuFN2ryWWrRSOrwgUolg7C+6Om9PAnslKD51fptZx8E1t5jd/wwR+NEd17fD3l861dpcwKFs/dStYuvK5RXddWtAZyWSrW1nBoDetSQVIxakUlrc3wnMJG1sB5WbCuy6Q+jAPTnEJVtz2dz7h3C+x0wnY+mwU2InXCqzRudbxnACYif4uIPnTx3F9N//wxAL/Dfv9GAD8gIicAP01EPwXg1wH42+/2PQ/J9UkW78pS3Ly/IPHT3IxTusKRxS/J7MdD8n/3L5m5W5FZ7rvmzrLzcidEUqN/Ti3zVuNSIBDUAkj1UhvBILH448ygV+4/EAfB98Q5m4g66+qcQp5Hu3Mi8qYsSkT7e8JNzqkKIXNjx2UNRQpM9RSsna+tqLxUPb5SCyqrxVitlEmMRAcKpJj7HN3FBcwUIBRgZG7k1P2iqxZ1HJ5tWhnI5jWcayhyvfL6wvyd/Nr7d4VlW8wCBzz3MK45kVVmGJ3QUkch47n8oU1eZlfxXbPmUq4cx94KCxK/T+kjTwMReOIzPaVRfB7j9wH4M/b7l0MBzcen7bkHg4i+FcC3AsCbz5+HttNsssCWQ0QRbnftJU8qdMngHKW5JPh35nTxLZNQRIJfCfeErtfEqb4VW36TEfkGKC6kQ6L5WwCbKB8ZANkuywQVMjU1V1TrxFxARWsnS9WbeDC0ljJR9Zo/NC2MjMNepJ2tkktrbR5QosUSHnsnJSX25+/CqWg7PtRv6snJuVqHli1ZT24plpyqHJhGxxTUqFUIE4YBwAAspUJzlyaAkl0v/SnOd76Dl7SzvHGZuT4LnQdPDgpXrvsOwHJCtZCJP3YDRw4X8NAW1KrKE3d3R9wdj1pC5ImpRT+jNeXDmCmKvHPtowsW+oVyZQx3H99++22cTiecTmecz5seFzM6z+rbEJuMze92xqMAGBF9J4AO4Pv8qSsvu3rmROQTAD4BAF/2pV8qvpMlQsh2xBKA5ottlzF9BbD8NTZH5WbcrEmWSprL5eR2E3/gpsreRQEmNhZJyYMTLwGzrNAKYl0a0A2QpiN0l0qx4nHJN2ji89JduwetxHdd+JmEnZeddnSJ6JtW/vpu7q+xVA+W4JIux3TdXPFCqwqkcJRcsYhm9Rs14IkiKHp93R2LCCIp8GchmGl3ybyOcCC9iDK71YW8sSGOZYy5ptx6mad4gpefW7GLQWal+mT9nHhPSlVbLVjXFsCl4oQlEp41RSL3dyjxs0Th9rQg85o+R82jlgydt01TKdiFA9yapB3tcP3WfH3HSwcwIvoolNz/epkI8GkAX5le9hUA/vXn94mJvzLCnoTCAnK3MefpZKCKT0muQPzbFu/FEcy7+gWzmZ8xF/UOBXauaLZ8LtwyUsurGso5h+tQ3UCRpe9H753V5nmhaGM/j9WKgMmNIntttlDs6XyU2c3eWSDmt86bPTLw7EMe+PkJuPTbnOz2KgW230EFXBjV1CjIctEkAc+sb5wWwx6HL5NZlQC4BLBwF2FO3g4EZlQ1F0b7Cd1tZhkQgUkKBMhOM9jdwNa0cNtloJWUL5YjSGDLC/PoostHT6J/70Kq9TWvVXYdN4s+5ghkrFkkI/0G3ciXCmBE9BEAHwPwX4vIW+lPnwTwp4nou6Ek/ocB/N3P7zPjs+fug3nhsmSu5+5cWmA+rmNSukk/nwu6A0BON7trPvnu7O1sXUzHjgNIYENINBgYQKkKXLDquOIChWCz0NRNZYE14hWLqrEXDZqbKuZmJwsmAZYA+5t+prHBG0bzmJvB8EYQKWEVsQH4DWznIAFNKTO/zQwwy8tyTs/gxCmCi4c2FxmaFzXYmff4/h2QkQFXXEt93YAlKnvSrb2/9ymPvQ/+7HXaooO7XcBsgV1dMrEHahJuWwqOxwMOy4LnbxxwXBbcrdN99Pmo4gRBUFHLivWwRI9Il5B28BqstaQ9rK6zuY6nyANzHkwPZ947DOc/6QH0v+7jvUyj+H4AXwfgS4jo0wD+MDTqeADwI3bxf0xEfr+I/AQR/SCAfwJ1Lb/t84pAJvBK3zutgAu38bLn3cNxhQcL64XmTvoCd8je+NACi9ekx5Xvdqtn76tOtwh2o1EhrfQJXM1ukFuSyn1pvWNCQTLwY4aUEgBWnSd60XA1CUyACMDytIFkgeHi0yisvf1N7Td5BprIm7Pz/+D6+SZk6QsPaIE457RbG3Fy7URLmBv6S9Q0mpXl1QSXHOk8JoqfF3/ZrxGSeVnteGOGxoEtrWFdGtZ1wWoJrJ5gKzwFB2opEI/Q5nZp6Tgz/xrpK/az73LXJnfrQLUn7/efewvjvYxCfvOVp//kO7z+uwB81xf6PZ4bo3lE+5PtC9sJfC9azVbYnMDeAssg5C6SL/7sJgLJ5AaCY7m22LOVmH/fD9cCS5+FmbldHYuKzzdbnlowLEPQo2O1FmcDpMmUpaCUiuHRKnG8JFhbnzm/fBzDgFymYqyLiXo6hangGejDPWI/vahUpkXlT9orCiwFhSWy9LUoekM/nzE2bWjb+znpym9xbZkHZPSgDeZnW1yXLN033D47Y+wTnRn23ZRJex9h/biShp1xs4x9UygBtGLyQRH3ya77PLt6fksx2ZwVx7s7vHFY8eabz9CI0AAQBkgY26bnAwDWZcHSVMBxTSQ/YGVXF17HKXFf23bGeTtru7qeZXPS2r1Yo7c2bioTf3d63RWKf8/d22vWHroCydoyq8UjlPvnxbofy7Vvxp5j2bEID16lCYsuepOA7eLAyI8Bep9wvN65MHX/KiRl1btFxCZVk6xRez+z5m2pu4HcBCxm6PPPc4jkzHhM68t/n/WOCLDzT8ouVmTGuzvtG4SIRek4yl+GNcDofZiyadcmGf0yE97TG1ypYeKL2HfOSKRgfqVdY9vsuvW0dNWGSyJ/Xh87VwTdXWI9zTQOvXb+JlcMhtEIBPJO24taX8u64nA4oIoGj7TNGhS8RAMWLoBJpEqstdWIUGavI3c+6n2vOLHrOoQEVGbJRUDl6lp/vcdNARiAC7MpPZyE5YuyD34IYsFTJXfB+QR/kC1OEVj5Uh7JhQ1X7Dr5oXk8sgMuB7LdK31NYdJPZJn7AIBCqDaNWjQJFQ5eVn/oAcKILtlx+WlzyR33oii5mu7OBogJIEbG+42/I7YfAFg+N/lGoeke+rHDQCzAd0rSjDEwtq4WmDXHcL0tV64Q6/jtAA4Hqt018Zl4aEEi+jaj1Fa21aeG/bRQyu7z8uaDHXgor6dqsulbhc3SnZtSqwpeS4DXiuPxAOIB6ZvWc5orSySoRVBLRSGNRmp/SP0dFwDGPFTCenhz4Alio8+uQ/vFNsGrGL3wBGCPNF5Eyqubw+YSvMB9fMHnAQjhOMkEjsSPn9d4kYmerbnkBOl00t/nzT9NI89Z6uxWRjG3CqDaABKVcSZRZYlkDV7Oi6ym0qO6el6SwoG5jt7gNgAMMweMqOzOk0gWOZzVEd2US51w1ppI1ZH3bj7DW5PxFAxknlxO6Pvv8St993T/nRrw48ob24P3AQmM5jm6dLVcXYSZJkGZTDf/BI8Yulz0aln0q1lgvJ3BPLBZGdbWNzRrlOzqJeu6hHZ+jj5GsfnWp1x0lA/ZY0zde6W5Jtd1jU++pXGbAHbtJIe1IBc703VZHf+Y3c2G6WLmK5lslN2gK6j2wgVA12+Giz/tnNYAq+To7W4ScVdILTC/YZn95mEzJMa0xOjC9cYlgJG5XzA3SS05r4ecrqNZTwkkducwHY93oI7yrsHRrSdE93pXjbCdqzgJ/Mus+IRIV67B3kq/dAtFZDe/uAgpMJPPy+4cPeA0LUE6LK+YRERXyfK6qhVse0JqtY7bfXRNFfE1OwbEZLxLVquoUzbH+Up9z5j9OHcdh5IXYhLa2W3Mx6HgRVc3gtd53ByAFeM2yEpNZq+gdEE7Y2wd/bzFjXLVAtsBh94QLCaVbMXVnooAPAQ8/YxEfrs3owU0D24AN9d3iydNI8dKc1mPWlwU4MXd3ChfsFtXyRqo5sUwy6hUBlFVVVX/zkvXSPL3FFvAFAKB7tYE7yWqlDEtGz1zQ+Y18OFAlzXx3SreTqYdbxnjvXdsp7O5P2ezuKyXYvBkqQYx82l+jna2bNLpwsWmdkX+xq9NUAgPYP7hIIvw1jr7GSBFEEEmVtgaWis4rCvWw4plWdGWxZ5v4E2/tzt/dT6DloalaRBmWRoOx3VXxwoALB2DN2zd5HIsdeJ8Ps0C7lRC5HOOSCaAWbWhgYobM8BuD8AAPDzLO57mMoXixWFxfeu0HHav89cm7mbu1nurZS7YS8sq1etdey5AJUXxzKqLDkP+vrAg7EY0YFCC1qwVaDGvHrOyaQ5Ic+rXAYxAgMyARgEZaDqAJUBijypOAMtg5qfNI3SZi3QA68Zx9U0B2N3FMawxrLmMMhJZz+m6xAWZ6S5xNi9By63VeA5X18S8drlW4Z2Hl+FoXh6l45fYxDwS7AoSrbVQnfANLYIyIsnSVNUO18wvFpDwz/cglKZO7K2uwZcWmF0bW4uTG7UjLWWe2xsaNwdgD26YWJhZu+pKAuSVBRtWE/BgUYtFJ70bc1hHCYwEZpGDHgDTnicpu+fyiPebAZELj4g0yVMkpzJMNyMWa+8qZwPVRbNSQYgoJ6XqFu4epc9P8stuccz//LUTxKIIOLtkF8Af3Y+AOO+XrqK3+Bq9Yztv0eJeQc2AbHMAM6sLnCVpJ8Qkza/La5stR/8MP4eXQR2/Vv7JDtDvNBQE/L1azzlsN3WD0PO+mllby7JEMw7vpl084dhBaQxws2L3YuoTbdFjTGs5y/1EKoVxolsfZtF1LR8SUcUOzNZs8xBpupbveMSv37g9AIPnFjnVPe1e79HD4lkA8wb0B/lPlBTef7jXvjhv6+Id4drlh3/H9T2cYPldcUz+kFhQbpH50J6mpi3lBZD+nnmkn58LkAwZt/ouHbD5aQ/dZ0+7CvwP75nSpmASR+LulQPh3qrTD5RdYAJG1xQA/cIivnZO5eKoxa+BA5v9nzNQsVszFDbcjFhe4z0l/R+7v7pLazuGcWmye920wPV/qjaBiGpq8qtZuLvOWXkOF0fuyzqlbMSGjil+GfOWpFrhR0n+Vz/q24Kwm5NhzM7CBK99XG2ud7r6oB3opWv4hU3iYtAeGPZ/SRzxhIIwirC/CeXiZ1789OAVsn/R1fnuPym+KYBm/wEXr9qB17VJBnilTxBxQEs/033ub94T7tMiphedx90cH741HhdIc/3ypjw4n2deJ7tNbD6/fx0wrdEXb1jXjuOao6ogZvI7uw304Sdna13yVYpzsXfrYb/PPDDsT+oNDnq39ILXaRDRzwH4HIB/96rnAuBL8DSP12kOwNM83qs5/GIR+YXv5WRe1rgpAAMAIvr7IvJrn+bx+szjdZjD0zxevzk8xrg5F/JpPI2n8TR8PAHY03gaT+Nmxy0C2Cde9QRsPM1jjtdhDsDTPPJ4Hebw0sfNcWBP42k8jafh4xYtsKfxNJ7G0wDwBGBP42k8jRseNwVgRPQRIvoUEf0UEX3HI33nVxLR3yCinySinyCiP2DPfxER/QgR/XP7+QseaT6ViP4hEf3Qq5oHEX2AiP4sEf1TOy9f+9jzIKL/zq7HjxPR9xPR8THmQETfQ0SfIaIfT8+98HuJ6OO2Xj9FRL/pJc/jj9s1+cdE9BeI6AMvex6vetwMgBFRBfA/A/jNAL4awDeTdvh+2aMD+O9F5FcA+PUAvs2+9zsA/KiIfBjAj9q/H2P8AQA/mf79KubxPwH4KyLyywH85zafR5sHEX05gG8H8GtF5GugTSS/6ZHm8L0APnLx3NXvpX0H+o8A+F9sHb+sefwIgK8Rkf8MwD+D9qR42fN4teOyqPV1fQD4WgA/nP79cQAffwXz+EsA/lsAnwLwQXvugwA+9Qjf/RXQG+Q3APghe+5R5wHgfQB+GhYASs8/2jygTZD/FYAvgtbz/hCA3/hYcwDwIQA//m7HfrlGAfwwgK99WfO4+NtvB/B9jzGPV/m4GQsMc9H6eGE375c1iOhDAH41gL8D4MtE5GcBwH5+6SNM4X8E8Iewlw577Hn8EgA/B+D/MFf2fyeiZ485DxH5fwD8DwB+BsDPAvgPIvJXH3MOF+NF3/sq1+zvA/CXX4N5vNRxSwB2reT00XJAiOg5gD8H4A+KyH98rO9N3/8NAD4jIv/gsb/7YjQAvwbA/yoivxpam/pY7jMAwDimbwTwVdC+os+I6Pc85hw+z/FK1iwRfSeU+vi+VzmPxxi3BGA/j27eP79BRAsUvL5PRP68Pf1vieiD9vcPAvjMS57GfwngtxHRvwTwAwB+AxH9369gHp8G8GkR+Tv27z8LBbTHnMd/A+CnReTnRGQD8OcB/BePPIc8XvS9j75mieijAL4BwO8W8xdfxTwea9wSgP09AB8moq8iohVKSn7yZX8pqfbInwTwkyLy3elPnwTwUfv9o1Bu7KUNEfm4iHyFiHwIeux/XUR+zyuYx78B8K+I6JfZU18PbVD8mPP4GQC/nojesOvz9dBAwqOeizRe9L2fBPBNRHQgoq/CF9CB/v/LIKKPAPgYgN8mIm9dzO/R5vGo41WTcF/IA8BvgUZX/gWA73yk7/yvoOb2Pwbwj+zxWwB8MZRQ/+f284se8Tx8HSaJ/+jzAPCrAPx9Oyd/EcAveOx5APgjAP4pgB8H8H9BO8C/9DkA+H4o77ZBLZtveafvBfCdtl4/BeA3v+R5/BSU6/J1+r+97Hm86sdTKdHTeBpP42bHLbmQT+NpPI2nsRtPAPY0nsbTuNnxBGBP42k8jZsdTwD2NJ7G07jZ8QRgT+NpPI2bHU8A9jSextO42fEEYE/jaTyNmx3/CUDTVy9FBVE7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out)\n",
    "plt.title(f'Age :{age}-{img_out_age}, Gender: {gender}-{img_out_gd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-13 11:15:59,561 - onnx-tf - INFO - Unknown op ConstantFill in domain `ai.onnx`.\n",
      "INFO:onnx-tf:Unknown op ConstantFill in domain `ai.onnx`.\n",
      "2021-01-13 11:15:59,563 - onnx-tf - INFO - Unknown op ImageScaler in domain `ai.onnx`.\n",
      "INFO:onnx-tf:Unknown op ImageScaler in domain `ai.onnx`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: ['input']\n",
      "outputs: ['output_age', 'output_gender']\n",
      "tensor_dict:\n",
      "{'838': <tf.Tensor '838:0' shape=(16, 3, 3, 3) dtype=float32>, '839': <tf.Tensor '839:0' shape=(16,) dtype=float32>, '841': <tf.Tensor '841:0' shape=(16, 1, 3, 3) dtype=float32>, '842': <tf.Tensor '842:0' shape=(16,) dtype=float32>, '844': <tf.Tensor '844:0' shape=(16, 16, 1, 1) dtype=float32>, '845': <tf.Tensor '845:0' shape=(16,) dtype=float32>, '847': <tf.Tensor '847:0' shape=(96, 1, 3, 3) dtype=float32>, '848': <tf.Tensor '848:0' shape=(96,) dtype=float32>, '850': <tf.Tensor '850:0' shape=(72, 1, 3, 3) dtype=float32>, '851': <tf.Tensor '851:0' shape=(72,) dtype=float32>, '853': <tf.Tensor '853:0' shape=(144, 24, 1, 1) dtype=float32>, '854': <tf.Tensor '854:0' shape=(144,) dtype=float32>, '856': <tf.Tensor '856:0' shape=(40, 144, 1, 1) dtype=float32>, '857': <tf.Tensor '857:0' shape=(40,) dtype=float32>, '859': <tf.Tensor '859:0' shape=(240, 40, 1, 1) dtype=float32>, '860': <tf.Tensor '860:0' shape=(240,) dtype=float32>, '862': <tf.Tensor '862:0' shape=(480, 80, 1, 1) dtype=float32>, '863': <tf.Tensor '863:0' shape=(480,) dtype=float32>, '865': <tf.Tensor '865:0' shape=(480, 80, 1, 1) dtype=float32>, '866': <tf.Tensor '866:0' shape=(480,) dtype=float32>, '868': <tf.Tensor '868:0' shape=(720, 120, 1, 1) dtype=float32>, '869': <tf.Tensor '869:0' shape=(720,) dtype=float32>, '871': <tf.Tensor '871:0' shape=(200, 720, 1, 1) dtype=float32>, '872': <tf.Tensor '872:0' shape=(200,) dtype=float32>, '874': <tf.Tensor '874:0' shape=(1200, 200, 1, 1) dtype=float32>, '875': <tf.Tensor '875:0' shape=(1200,) dtype=float32>, '877': <tf.Tensor '877:0' shape=(1200, 200, 1, 1) dtype=float32>, '878': <tf.Tensor '878:0' shape=(1200,) dtype=float32>, '880': <tf.Tensor '880:0' shape=(1536, 200, 1, 1) dtype=float32>, '881': <tf.Tensor '881:0' shape=(1536,) dtype=float32>, 'age_head.0.bias': <tf.Tensor 'age_head.0.bias:0' shape=(1,) dtype=float32>, 'age_head.0.weight': <tf.Tensor 'age_head.0.weight:0' shape=(1, 512) dtype=float32>, 'backbone.0.blocks.1.0.bn1.bias': <tf.Tensor 'backbone.0.blocks.1.0.bn1.bias:0' shape=(96,) dtype=float32>, 'backbone.0.blocks.1.0.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.1.0.bn1.running_mean:0' shape=(96,) dtype=float32>, 'backbone.0.blocks.1.0.bn1.running_var': <tf.Tensor 'backbone.0.blocks.1.0.bn1.running_var:0' shape=(96,) dtype=float32>, 'backbone.0.blocks.1.0.bn1.weight': <tf.Tensor 'backbone.0.blocks.1.0.bn1.weight:0' shape=(96,) dtype=float32>, 'backbone.0.blocks.1.0.bn3.bias': <tf.Tensor 'backbone.0.blocks.1.0.bn3.bias:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.0.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.1.0.bn3.running_mean:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.0.bn3.running_var': <tf.Tensor 'backbone.0.blocks.1.0.bn3.running_var:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.0.bn3.weight': <tf.Tensor 'backbone.0.blocks.1.0.bn3.weight:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.0.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.1.0.conv_pw.0.weight:0' shape=(48, 8, 1, 1) dtype=float32>, 'backbone.0.blocks.1.0.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.1.0.conv_pw.1.weight:0' shape=(48, 8, 1, 1) dtype=float32>, 'backbone.0.blocks.1.0.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.1.0.conv_pwl.0.weight:0' shape=(12, 48, 1, 1) dtype=float32>, 'backbone.0.blocks.1.0.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.1.0.conv_pwl.1.weight:0' shape=(12, 48, 1, 1) dtype=float32>, 'backbone.0.blocks.1.1.bn1.bias': <tf.Tensor 'backbone.0.blocks.1.1.bn1.bias:0' shape=(72,) dtype=float32>, 'backbone.0.blocks.1.1.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.1.1.bn1.running_mean:0' shape=(72,) dtype=float32>, 'backbone.0.blocks.1.1.bn1.running_var': <tf.Tensor 'backbone.0.blocks.1.1.bn1.running_var:0' shape=(72,) dtype=float32>, 'backbone.0.blocks.1.1.bn1.weight': <tf.Tensor 'backbone.0.blocks.1.1.bn1.weight:0' shape=(72,) dtype=float32>, 'backbone.0.blocks.1.1.bn3.bias': <tf.Tensor 'backbone.0.blocks.1.1.bn3.bias:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.1.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.1.1.bn3.running_mean:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.1.bn3.running_var': <tf.Tensor 'backbone.0.blocks.1.1.bn3.running_var:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.1.bn3.weight': <tf.Tensor 'backbone.0.blocks.1.1.bn3.weight:0' shape=(24,) dtype=float32>, 'backbone.0.blocks.1.1.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.1.1.conv_pw.0.weight:0' shape=(36, 12, 1, 1) dtype=float32>, 'backbone.0.blocks.1.1.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.1.1.conv_pw.1.weight:0' shape=(36, 12, 1, 1) dtype=float32>, 'backbone.0.blocks.1.1.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.1.1.conv_pwl.0.weight:0' shape=(12, 36, 1, 1) dtype=float32>, 'backbone.0.blocks.1.1.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.1.1.conv_pwl.1.weight:0' shape=(12, 36, 1, 1) dtype=float32>, 'backbone.0.blocks.2.0.bn2.bias': <tf.Tensor 'backbone.0.blocks.2.0.bn2.bias:0' shape=(144,) dtype=float32>, 'backbone.0.blocks.2.0.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.2.0.bn2.running_mean:0' shape=(144,) dtype=float32>, 'backbone.0.blocks.2.0.bn2.running_var': <tf.Tensor 'backbone.0.blocks.2.0.bn2.running_var:0' shape=(144,) dtype=float32>, 'backbone.0.blocks.2.0.bn2.weight': <tf.Tensor 'backbone.0.blocks.2.0.bn2.weight:0' shape=(144,) dtype=float32>, 'backbone.0.blocks.2.0.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.2.0.conv_dw.0.weight:0' shape=(48, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.2.0.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.2.0.conv_dw.1.weight:0' shape=(48, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.2.0.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.2.0.conv_dw.2.weight:0' shape=(48, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.2.0.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.2.0.se.conv_expand.bias:0' shape=(144,) dtype=float32>, 'backbone.0.blocks.2.0.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.2.0.se.conv_expand.weight:0' shape=(144, 12, 1, 1) dtype=float32>, 'backbone.0.blocks.2.0.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.2.0.se.conv_reduce.bias:0' shape=(12,) dtype=float32>, 'backbone.0.blocks.2.0.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.2.0.se.conv_reduce.weight:0' shape=(12, 144, 1, 1) dtype=float32>, 'backbone.0.blocks.2.1.bn1.bias': <tf.Tensor 'backbone.0.blocks.2.1.bn1.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.2.1.bn1.running_mean:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn1.running_var': <tf.Tensor 'backbone.0.blocks.2.1.bn1.running_var:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn1.weight': <tf.Tensor 'backbone.0.blocks.2.1.bn1.weight:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn2.bias': <tf.Tensor 'backbone.0.blocks.2.1.bn2.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.2.1.bn2.running_mean:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn2.running_var': <tf.Tensor 'backbone.0.blocks.2.1.bn2.running_var:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn2.weight': <tf.Tensor 'backbone.0.blocks.2.1.bn2.weight:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.bn3.bias': <tf.Tensor 'backbone.0.blocks.2.1.bn3.bias:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.1.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.2.1.bn3.running_mean:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.1.bn3.running_var': <tf.Tensor 'backbone.0.blocks.2.1.bn3.running_var:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.1.bn3.weight': <tf.Tensor 'backbone.0.blocks.2.1.bn3.weight:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.1.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.2.1.conv_dw.0.weight:0' shape=(120, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.2.1.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.2.1.conv_dw.1.weight:0' shape=(120, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.2.1.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.2.1.conv_pw.0.weight:0' shape=(120, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.1.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.2.1.conv_pw.1.weight:0' shape=(120, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.1.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.2.1.conv_pwl.0.weight:0' shape=(20, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.2.1.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.2.1.conv_pwl.1.weight:0' shape=(20, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.2.1.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.2.1.se.conv_expand.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.1.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.2.1.se.conv_expand.weight:0' shape=(240, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.1.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.2.1.se.conv_reduce.bias:0' shape=(20,) dtype=float32>, 'backbone.0.blocks.2.1.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.2.1.se.conv_reduce.weight:0' shape=(20, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.2.2.bn1.bias': <tf.Tensor 'backbone.0.blocks.2.2.bn1.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.2.2.bn1.running_mean:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn1.running_var': <tf.Tensor 'backbone.0.blocks.2.2.bn1.running_var:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn1.weight': <tf.Tensor 'backbone.0.blocks.2.2.bn1.weight:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn2.bias': <tf.Tensor 'backbone.0.blocks.2.2.bn2.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.2.2.bn2.running_mean:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn2.running_var': <tf.Tensor 'backbone.0.blocks.2.2.bn2.running_var:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn2.weight': <tf.Tensor 'backbone.0.blocks.2.2.bn2.weight:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.bn3.bias': <tf.Tensor 'backbone.0.blocks.2.2.bn3.bias:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.2.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.2.2.bn3.running_mean:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.2.bn3.running_var': <tf.Tensor 'backbone.0.blocks.2.2.bn3.running_var:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.2.bn3.weight': <tf.Tensor 'backbone.0.blocks.2.2.bn3.weight:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.2.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.2.2.conv_dw.0.weight:0' shape=(120, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.2.2.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.2.2.conv_dw.1.weight:0' shape=(120, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.2.2.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.2.2.conv_pw.0.weight:0' shape=(120, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.2.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.2.2.conv_pw.1.weight:0' shape=(120, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.2.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.2.2.conv_pwl.0.weight:0' shape=(20, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.2.2.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.2.2.conv_pwl.1.weight:0' shape=(20, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.2.2.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.2.2.se.conv_expand.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.2.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.2.2.se.conv_expand.weight:0' shape=(240, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.2.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.2.2.se.conv_reduce.bias:0' shape=(20,) dtype=float32>, 'backbone.0.blocks.2.2.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.2.2.se.conv_reduce.weight:0' shape=(20, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.2.3.bn1.bias': <tf.Tensor 'backbone.0.blocks.2.3.bn1.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.2.3.bn1.running_mean:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn1.running_var': <tf.Tensor 'backbone.0.blocks.2.3.bn1.running_var:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn1.weight': <tf.Tensor 'backbone.0.blocks.2.3.bn1.weight:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn2.bias': <tf.Tensor 'backbone.0.blocks.2.3.bn2.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.2.3.bn2.running_mean:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn2.running_var': <tf.Tensor 'backbone.0.blocks.2.3.bn2.running_var:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn2.weight': <tf.Tensor 'backbone.0.blocks.2.3.bn2.weight:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.bn3.bias': <tf.Tensor 'backbone.0.blocks.2.3.bn3.bias:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.3.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.2.3.bn3.running_mean:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.3.bn3.running_var': <tf.Tensor 'backbone.0.blocks.2.3.bn3.running_var:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.3.bn3.weight': <tf.Tensor 'backbone.0.blocks.2.3.bn3.weight:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.2.3.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.2.3.conv_dw.0.weight:0' shape=(120, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.2.3.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.2.3.conv_dw.1.weight:0' shape=(120, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.2.3.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.2.3.conv_pw.0.weight:0' shape=(120, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.3.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.2.3.conv_pw.1.weight:0' shape=(120, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.3.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.2.3.conv_pwl.0.weight:0' shape=(20, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.2.3.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.2.3.conv_pwl.1.weight:0' shape=(20, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.2.3.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.2.3.se.conv_expand.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.2.3.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.2.3.se.conv_expand.weight:0' shape=(240, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.2.3.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.2.3.se.conv_reduce.bias:0' shape=(20,) dtype=float32>, 'backbone.0.blocks.2.3.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.2.3.se.conv_reduce.weight:0' shape=(20, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.3.0.bn2.bias': <tf.Tensor 'backbone.0.blocks.3.0.bn2.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.3.0.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.3.0.bn2.running_mean:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.3.0.bn2.running_var': <tf.Tensor 'backbone.0.blocks.3.0.bn2.running_var:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.3.0.bn2.weight': <tf.Tensor 'backbone.0.blocks.3.0.bn2.weight:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.3.0.bn3.bias': <tf.Tensor 'backbone.0.blocks.3.0.bn3.bias:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.0.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.3.0.bn3.running_mean:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.0.bn3.running_var': <tf.Tensor 'backbone.0.blocks.3.0.bn3.running_var:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.0.bn3.weight': <tf.Tensor 'backbone.0.blocks.3.0.bn3.weight:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.0.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.3.0.conv_dw.0.weight:0' shape=(80, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.3.0.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.3.0.conv_dw.1.weight:0' shape=(80, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.3.0.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.3.0.conv_dw.2.weight:0' shape=(80, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.3.0.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.3.0.conv_pwl.0.weight:0' shape=(40, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.3.0.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.3.0.conv_pwl.1.weight:0' shape=(40, 120, 1, 1) dtype=float32>, 'backbone.0.blocks.3.0.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.3.0.se.conv_expand.bias:0' shape=(240,) dtype=float32>, 'backbone.0.blocks.3.0.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.3.0.se.conv_expand.weight:0' shape=(240, 10, 1, 1) dtype=float32>, 'backbone.0.blocks.3.0.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.3.0.se.conv_reduce.bias:0' shape=(10,) dtype=float32>, 'backbone.0.blocks.3.0.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.3.0.se.conv_reduce.weight:0' shape=(10, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.3.1.bn2.bias': <tf.Tensor 'backbone.0.blocks.3.1.bn2.bias:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.1.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.3.1.bn2.running_mean:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.1.bn2.running_var': <tf.Tensor 'backbone.0.blocks.3.1.bn2.running_var:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.1.bn2.weight': <tf.Tensor 'backbone.0.blocks.3.1.bn2.weight:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.1.bn3.bias': <tf.Tensor 'backbone.0.blocks.3.1.bn3.bias:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.1.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.3.1.bn3.running_mean:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.1.bn3.running_var': <tf.Tensor 'backbone.0.blocks.3.1.bn3.running_var:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.1.bn3.weight': <tf.Tensor 'backbone.0.blocks.3.1.bn3.weight:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.1.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.3.1.conv_dw.0.weight:0' shape=(240, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.3.1.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.3.1.conv_dw.1.weight:0' shape=(240, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.3.1.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.3.1.conv_pwl.0.weight:0' shape=(40, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.3.1.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.3.1.conv_pwl.1.weight:0' shape=(40, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.3.1.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.3.1.se.conv_expand.bias:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.1.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.3.1.se.conv_expand.weight:0' shape=(480, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.3.1.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.3.1.se.conv_reduce.bias:0' shape=(20,) dtype=float32>, 'backbone.0.blocks.3.1.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.3.1.se.conv_reduce.weight:0' shape=(20, 480, 1, 1) dtype=float32>, 'backbone.0.blocks.3.2.bn2.bias': <tf.Tensor 'backbone.0.blocks.3.2.bn2.bias:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.2.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.3.2.bn2.running_mean:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.2.bn2.running_var': <tf.Tensor 'backbone.0.blocks.3.2.bn2.running_var:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.2.bn2.weight': <tf.Tensor 'backbone.0.blocks.3.2.bn2.weight:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.2.bn3.bias': <tf.Tensor 'backbone.0.blocks.3.2.bn3.bias:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.2.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.3.2.bn3.running_mean:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.2.bn3.running_var': <tf.Tensor 'backbone.0.blocks.3.2.bn3.running_var:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.2.bn3.weight': <tf.Tensor 'backbone.0.blocks.3.2.bn3.weight:0' shape=(80,) dtype=float32>, 'backbone.0.blocks.3.2.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.3.2.conv_dw.0.weight:0' shape=(240, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.3.2.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.3.2.conv_dw.1.weight:0' shape=(240, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.3.2.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.3.2.conv_pwl.0.weight:0' shape=(40, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.3.2.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.3.2.conv_pwl.1.weight:0' shape=(40, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.3.2.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.3.2.se.conv_expand.bias:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.3.2.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.3.2.se.conv_expand.weight:0' shape=(480, 20, 1, 1) dtype=float32>, 'backbone.0.blocks.3.2.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.3.2.se.conv_reduce.bias:0' shape=(20,) dtype=float32>, 'backbone.0.blocks.3.2.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.3.2.se.conv_reduce.weight:0' shape=(20, 480, 1, 1) dtype=float32>, 'backbone.0.blocks.4.0.bn1.bias': <tf.Tensor 'backbone.0.blocks.4.0.bn1.bias:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.4.0.bn1.running_mean:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn1.running_var': <tf.Tensor 'backbone.0.blocks.4.0.bn1.running_var:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn1.weight': <tf.Tensor 'backbone.0.blocks.4.0.bn1.weight:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn2.bias': <tf.Tensor 'backbone.0.blocks.4.0.bn2.bias:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.4.0.bn2.running_mean:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn2.running_var': <tf.Tensor 'backbone.0.blocks.4.0.bn2.running_var:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn2.weight': <tf.Tensor 'backbone.0.blocks.4.0.bn2.weight:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.bn3.bias': <tf.Tensor 'backbone.0.blocks.4.0.bn3.bias:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.0.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.4.0.bn3.running_mean:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.0.bn3.running_var': <tf.Tensor 'backbone.0.blocks.4.0.bn3.running_var:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.0.bn3.weight': <tf.Tensor 'backbone.0.blocks.4.0.bn3.weight:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.0.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.4.0.conv_dw.0.weight:0' shape=(160, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.4.0.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.4.0.conv_dw.1.weight:0' shape=(160, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.4.0.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.4.0.conv_dw.2.weight:0' shape=(160, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.4.0.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.4.0.conv_pw.0.weight:0' shape=(240, 40, 1, 1) dtype=float32>, 'backbone.0.blocks.4.0.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.4.0.conv_pw.1.weight:0' shape=(240, 40, 1, 1) dtype=float32>, 'backbone.0.blocks.4.0.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.4.0.conv_pwl.0.weight:0' shape=(60, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.4.0.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.4.0.conv_pwl.1.weight:0' shape=(60, 240, 1, 1) dtype=float32>, 'backbone.0.blocks.4.0.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.4.0.se.conv_expand.bias:0' shape=(480,) dtype=float32>, 'backbone.0.blocks.4.0.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.4.0.se.conv_expand.weight:0' shape=(480, 40, 1, 1) dtype=float32>, 'backbone.0.blocks.4.0.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.4.0.se.conv_reduce.bias:0' shape=(40,) dtype=float32>, 'backbone.0.blocks.4.0.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.4.0.se.conv_reduce.weight:0' shape=(40, 480, 1, 1) dtype=float32>, 'backbone.0.blocks.4.1.bn1.bias': <tf.Tensor 'backbone.0.blocks.4.1.bn1.bias:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.4.1.bn1.running_mean:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn1.running_var': <tf.Tensor 'backbone.0.blocks.4.1.bn1.running_var:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn1.weight': <tf.Tensor 'backbone.0.blocks.4.1.bn1.weight:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn2.bias': <tf.Tensor 'backbone.0.blocks.4.1.bn2.bias:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.4.1.bn2.running_mean:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn2.running_var': <tf.Tensor 'backbone.0.blocks.4.1.bn2.running_var:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn2.weight': <tf.Tensor 'backbone.0.blocks.4.1.bn2.weight:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.bn3.bias': <tf.Tensor 'backbone.0.blocks.4.1.bn3.bias:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.1.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.4.1.bn3.running_mean:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.1.bn3.running_var': <tf.Tensor 'backbone.0.blocks.4.1.bn3.running_var:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.1.bn3.weight': <tf.Tensor 'backbone.0.blocks.4.1.bn3.weight:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.1.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_dw.0.weight:0' shape=(90, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.4.1.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_dw.1.weight:0' shape=(90, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.4.1.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_dw.2.weight:0' shape=(90, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.4.1.conv_dw.3.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_dw.3.weight:0' shape=(90, 1, 9, 9) dtype=float32>, 'backbone.0.blocks.4.1.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_pw.0.weight:0' shape=(180, 60, 1, 1) dtype=float32>, 'backbone.0.blocks.4.1.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_pw.1.weight:0' shape=(180, 60, 1, 1) dtype=float32>, 'backbone.0.blocks.4.1.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_pwl.0.weight:0' shape=(60, 180, 1, 1) dtype=float32>, 'backbone.0.blocks.4.1.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.4.1.conv_pwl.1.weight:0' shape=(60, 180, 1, 1) dtype=float32>, 'backbone.0.blocks.4.1.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.4.1.se.conv_expand.bias:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.1.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.4.1.se.conv_expand.weight:0' shape=(360, 60, 1, 1) dtype=float32>, 'backbone.0.blocks.4.1.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.4.1.se.conv_reduce.bias:0' shape=(60,) dtype=float32>, 'backbone.0.blocks.4.1.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.4.1.se.conv_reduce.weight:0' shape=(60, 360, 1, 1) dtype=float32>, 'backbone.0.blocks.4.2.bn1.bias': <tf.Tensor 'backbone.0.blocks.4.2.bn1.bias:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn1.running_mean': <tf.Tensor 'backbone.0.blocks.4.2.bn1.running_mean:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn1.running_var': <tf.Tensor 'backbone.0.blocks.4.2.bn1.running_var:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn1.weight': <tf.Tensor 'backbone.0.blocks.4.2.bn1.weight:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn2.bias': <tf.Tensor 'backbone.0.blocks.4.2.bn2.bias:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.4.2.bn2.running_mean:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn2.running_var': <tf.Tensor 'backbone.0.blocks.4.2.bn2.running_var:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn2.weight': <tf.Tensor 'backbone.0.blocks.4.2.bn2.weight:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.bn3.bias': <tf.Tensor 'backbone.0.blocks.4.2.bn3.bias:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.2.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.4.2.bn3.running_mean:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.2.bn3.running_var': <tf.Tensor 'backbone.0.blocks.4.2.bn3.running_var:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.2.bn3.weight': <tf.Tensor 'backbone.0.blocks.4.2.bn3.weight:0' shape=(120,) dtype=float32>, 'backbone.0.blocks.4.2.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_dw.0.weight:0' shape=(90, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.4.2.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_dw.1.weight:0' shape=(90, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.4.2.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_dw.2.weight:0' shape=(90, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.4.2.conv_dw.3.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_dw.3.weight:0' shape=(90, 1, 9, 9) dtype=float32>, 'backbone.0.blocks.4.2.conv_pw.0.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_pw.0.weight:0' shape=(180, 60, 1, 1) dtype=float32>, 'backbone.0.blocks.4.2.conv_pw.1.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_pw.1.weight:0' shape=(180, 60, 1, 1) dtype=float32>, 'backbone.0.blocks.4.2.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_pwl.0.weight:0' shape=(60, 180, 1, 1) dtype=float32>, 'backbone.0.blocks.4.2.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.4.2.conv_pwl.1.weight:0' shape=(60, 180, 1, 1) dtype=float32>, 'backbone.0.blocks.4.2.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.4.2.se.conv_expand.bias:0' shape=(360,) dtype=float32>, 'backbone.0.blocks.4.2.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.4.2.se.conv_expand.weight:0' shape=(360, 60, 1, 1) dtype=float32>, 'backbone.0.blocks.4.2.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.4.2.se.conv_reduce.bias:0' shape=(60,) dtype=float32>, 'backbone.0.blocks.4.2.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.4.2.se.conv_reduce.weight:0' shape=(60, 360, 1, 1) dtype=float32>, 'backbone.0.blocks.5.0.bn2.bias': <tf.Tensor 'backbone.0.blocks.5.0.bn2.bias:0' shape=(720,) dtype=float32>, 'backbone.0.blocks.5.0.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.5.0.bn2.running_mean:0' shape=(720,) dtype=float32>, 'backbone.0.blocks.5.0.bn2.running_var': <tf.Tensor 'backbone.0.blocks.5.0.bn2.running_var:0' shape=(720,) dtype=float32>, 'backbone.0.blocks.5.0.bn2.weight': <tf.Tensor 'backbone.0.blocks.5.0.bn2.weight:0' shape=(720,) dtype=float32>, 'backbone.0.blocks.5.0.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.5.0.conv_dw.0.weight:0' shape=(144, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.5.0.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.5.0.conv_dw.1.weight:0' shape=(144, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.5.0.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.5.0.conv_dw.2.weight:0' shape=(144, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.5.0.conv_dw.3.weight': <tf.Tensor 'backbone.0.blocks.5.0.conv_dw.3.weight:0' shape=(144, 1, 9, 9) dtype=float32>, 'backbone.0.blocks.5.0.conv_dw.4.weight': <tf.Tensor 'backbone.0.blocks.5.0.conv_dw.4.weight:0' shape=(144, 1, 11, 11) dtype=float32>, 'backbone.0.blocks.5.0.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.5.0.se.conv_expand.bias:0' shape=(720,) dtype=float32>, 'backbone.0.blocks.5.0.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.5.0.se.conv_expand.weight:0' shape=(720, 60, 1, 1) dtype=float32>, 'backbone.0.blocks.5.0.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.5.0.se.conv_reduce.bias:0' shape=(60,) dtype=float32>, 'backbone.0.blocks.5.0.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.5.0.se.conv_reduce.weight:0' shape=(60, 720, 1, 1) dtype=float32>, 'backbone.0.blocks.5.1.bn2.bias': <tf.Tensor 'backbone.0.blocks.5.1.bn2.bias:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.1.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.5.1.bn2.running_mean:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.1.bn2.running_var': <tf.Tensor 'backbone.0.blocks.5.1.bn2.running_var:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.1.bn2.weight': <tf.Tensor 'backbone.0.blocks.5.1.bn2.weight:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.1.bn3.bias': <tf.Tensor 'backbone.0.blocks.5.1.bn3.bias:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.1.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.5.1.bn3.running_mean:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.1.bn3.running_var': <tf.Tensor 'backbone.0.blocks.5.1.bn3.running_var:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.1.bn3.weight': <tf.Tensor 'backbone.0.blocks.5.1.bn3.weight:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.1.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.5.1.conv_dw.0.weight:0' shape=(300, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.5.1.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.5.1.conv_dw.1.weight:0' shape=(300, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.5.1.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.5.1.conv_dw.2.weight:0' shape=(300, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.5.1.conv_dw.3.weight': <tf.Tensor 'backbone.0.blocks.5.1.conv_dw.3.weight:0' shape=(300, 1, 9, 9) dtype=float32>, 'backbone.0.blocks.5.1.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.5.1.conv_pwl.0.weight:0' shape=(100, 600, 1, 1) dtype=float32>, 'backbone.0.blocks.5.1.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.5.1.conv_pwl.1.weight:0' shape=(100, 600, 1, 1) dtype=float32>, 'backbone.0.blocks.5.1.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.5.1.se.conv_expand.bias:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.1.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.5.1.se.conv_expand.weight:0' shape=(1200, 100, 1, 1) dtype=float32>, 'backbone.0.blocks.5.1.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.5.1.se.conv_reduce.bias:0' shape=(100,) dtype=float32>, 'backbone.0.blocks.5.1.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.5.1.se.conv_reduce.weight:0' shape=(100, 1200, 1, 1) dtype=float32>, 'backbone.0.blocks.5.2.bn2.bias': <tf.Tensor 'backbone.0.blocks.5.2.bn2.bias:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.2.bn2.running_mean': <tf.Tensor 'backbone.0.blocks.5.2.bn2.running_mean:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.2.bn2.running_var': <tf.Tensor 'backbone.0.blocks.5.2.bn2.running_var:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.2.bn2.weight': <tf.Tensor 'backbone.0.blocks.5.2.bn2.weight:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.2.bn3.bias': <tf.Tensor 'backbone.0.blocks.5.2.bn3.bias:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.2.bn3.running_mean': <tf.Tensor 'backbone.0.blocks.5.2.bn3.running_mean:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.2.bn3.running_var': <tf.Tensor 'backbone.0.blocks.5.2.bn3.running_var:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.2.bn3.weight': <tf.Tensor 'backbone.0.blocks.5.2.bn3.weight:0' shape=(200,) dtype=float32>, 'backbone.0.blocks.5.2.conv_dw.0.weight': <tf.Tensor 'backbone.0.blocks.5.2.conv_dw.0.weight:0' shape=(300, 1, 3, 3) dtype=float32>, 'backbone.0.blocks.5.2.conv_dw.1.weight': <tf.Tensor 'backbone.0.blocks.5.2.conv_dw.1.weight:0' shape=(300, 1, 5, 5) dtype=float32>, 'backbone.0.blocks.5.2.conv_dw.2.weight': <tf.Tensor 'backbone.0.blocks.5.2.conv_dw.2.weight:0' shape=(300, 1, 7, 7) dtype=float32>, 'backbone.0.blocks.5.2.conv_dw.3.weight': <tf.Tensor 'backbone.0.blocks.5.2.conv_dw.3.weight:0' shape=(300, 1, 9, 9) dtype=float32>, 'backbone.0.blocks.5.2.conv_pwl.0.weight': <tf.Tensor 'backbone.0.blocks.5.2.conv_pwl.0.weight:0' shape=(100, 600, 1, 1) dtype=float32>, 'backbone.0.blocks.5.2.conv_pwl.1.weight': <tf.Tensor 'backbone.0.blocks.5.2.conv_pwl.1.weight:0' shape=(100, 600, 1, 1) dtype=float32>, 'backbone.0.blocks.5.2.se.conv_expand.bias': <tf.Tensor 'backbone.0.blocks.5.2.se.conv_expand.bias:0' shape=(1200,) dtype=float32>, 'backbone.0.blocks.5.2.se.conv_expand.weight': <tf.Tensor 'backbone.0.blocks.5.2.se.conv_expand.weight:0' shape=(1200, 100, 1, 1) dtype=float32>, 'backbone.0.blocks.5.2.se.conv_reduce.bias': <tf.Tensor 'backbone.0.blocks.5.2.se.conv_reduce.bias:0' shape=(100,) dtype=float32>, 'backbone.0.blocks.5.2.se.conv_reduce.weight': <tf.Tensor 'backbone.0.blocks.5.2.se.conv_reduce.weight:0' shape=(100, 1200, 1, 1) dtype=float32>, 'backbone.0.classifier.bias': <tf.Tensor 'backbone.0.classifier.bias:0' shape=(512,) dtype=float32>, 'backbone.0.classifier.weight': <tf.Tensor 'backbone.0.classifier.weight:0' shape=(512, 1536) dtype=float32>, 'backbone.1.bias': <tf.Tensor 'backbone.1.bias:0' shape=(512,) dtype=float32>, 'backbone.1.running_mean': <tf.Tensor 'backbone.1.running_mean:0' shape=(512,) dtype=float32>, 'backbone.1.running_var': <tf.Tensor 'backbone.1.running_var:0' shape=(512,) dtype=float32>, 'backbone.1.weight': <tf.Tensor 'backbone.1.weight:0' shape=(512,) dtype=float32>, 'gender_head.1.bias': <tf.Tensor 'gender_head.1.bias:0' shape=(1,) dtype=float32>, 'gender_head.1.weight': <tf.Tensor 'gender_head.1.weight:0' shape=(1, 512) dtype=float32>, 'input': <tf.Tensor 'input:0' shape=(1, 3, 224, 224) dtype=float32>, '837': <tf.Tensor 'Add:0' shape=(1, 16, 112, 112) dtype=float32>, '408': <tf.Tensor 'Relu_1:0' shape=(1, 16, 112, 112) dtype=float32>, '840': <tf.Tensor 'Add_1:0' shape=(1, 16, 112, 112) dtype=float32>, '411': <tf.Tensor 'Relu_3:0' shape=(1, 16, 112, 112) dtype=float32>, '843': <tf.Tensor 'Add_2:0' shape=(1, 16, 112, 112) dtype=float32>, '414': <tf.Tensor 'Add_5:0' shape=(1, 16, 112, 112) dtype=float32>, '415': <tf.Tensor 'Split_6:0' shape=(1, 8, 112, 112) dtype=float32>, '416': <tf.Tensor 'Split_6:1' shape=(1, 8, 112, 112) dtype=float32>, '417': <tf.Tensor 'concat_3/concat:0' shape=(1, 48, 112, 112) dtype=float32>, '418': <tf.Tensor 'concat_4/concat:0' shape=(1, 48, 112, 112) dtype=float32>, '419': <tf.Tensor 'Concat_9:0' shape=(1, 96, 112, 112) dtype=float32>, '420': <tf.Tensor 'BatchNormalization_10/add_1:0' shape=(1, 96, 112, 112) dtype=float32>, '421': <tf.Tensor 'Relu_11:0' shape=(1, 96, 112, 112) dtype=float32>, '846': <tf.Tensor 'Add_3:0' shape=(1, 96, 56, 56) dtype=float32>, '424': <tf.Tensor 'Relu_13:0' shape=(1, 96, 56, 56) dtype=float32>, '425': <tf.Tensor 'Split_14:0' shape=(1, 48, 56, 56) dtype=float32>, '426': <tf.Tensor 'Split_14:1' shape=(1, 48, 56, 56) dtype=float32>, '427': <tf.Tensor 'concat_6/concat:0' shape=(1, 12, 56, 56) dtype=float32>, '428': <tf.Tensor 'concat_7/concat:0' shape=(1, 12, 56, 56) dtype=float32>, '429': <tf.Tensor 'Concat_17:0' shape=(1, 24, 56, 56) dtype=float32>, '430': <tf.Tensor 'BatchNormalization_18/add_1:0' shape=(1, 24, 56, 56) dtype=float32>, '431': <tf.Tensor 'Split_19:0' shape=(1, 12, 56, 56) dtype=float32>, '432': <tf.Tensor 'Split_19:1' shape=(1, 12, 56, 56) dtype=float32>, '433': <tf.Tensor 'concat_8/concat:0' shape=(1, 36, 56, 56) dtype=float32>, '434': <tf.Tensor 'concat_10/concat:0' shape=(1, 36, 56, 56) dtype=float32>, '435': <tf.Tensor 'Concat_22:0' shape=(1, 72, 56, 56) dtype=float32>, '436': <tf.Tensor 'BatchNormalization_23/add_1:0' shape=(1, 72, 56, 56) dtype=float32>, '437': <tf.Tensor 'Relu_24:0' shape=(1, 72, 56, 56) dtype=float32>, '849': <tf.Tensor 'Add_4:0' shape=(1, 72, 56, 56) dtype=float32>, '440': <tf.Tensor 'Relu_26:0' shape=(1, 72, 56, 56) dtype=float32>, '441': <tf.Tensor 'Split_27:0' shape=(1, 36, 56, 56) dtype=float32>, '442': <tf.Tensor 'Split_27:1' shape=(1, 36, 56, 56) dtype=float32>, '443': <tf.Tensor 'concat_12/concat:0' shape=(1, 12, 56, 56) dtype=float32>, '444': <tf.Tensor 'concat_13/concat:0' shape=(1, 12, 56, 56) dtype=float32>, '445': <tf.Tensor 'Concat_30:0' shape=(1, 24, 56, 56) dtype=float32>, '446': <tf.Tensor 'BatchNormalization_31/add_1:0' shape=(1, 24, 56, 56) dtype=float32>, '447': <tf.Tensor 'Add_32:0' shape=(1, 24, 56, 56) dtype=float32>, '852': <tf.Tensor 'Add_6:0' shape=(1, 144, 56, 56) dtype=float32>, '450': <tf.Tensor 'Sigmoid_34:0' shape=(1, 144, 56, 56) dtype=float32>, '451': <tf.Tensor 'Mul_35:0' shape=(1, 144, 56, 56) dtype=float32>, '452': <tf.Tensor 'Split_36:0' shape=(1, 48, 56, 56) dtype=float32>, '453': <tf.Tensor 'Split_36:1' shape=(1, 48, 56, 56) dtype=float32>, '454': <tf.Tensor 'Split_36:2' shape=(1, 48, 56, 56) dtype=float32>, '455': <tf.Tensor 'concat_15:0' shape=(1, 48, 28, 28) dtype=float32>, '456': <tf.Tensor 'concat_16:0' shape=(1, 48, 28, 28) dtype=float32>, '457': <tf.Tensor 'concat_18:0' shape=(1, 48, 28, 28) dtype=float32>, '458': <tf.Tensor 'Concat_40:0' shape=(1, 144, 28, 28) dtype=float32>, '459': <tf.Tensor 'BatchNormalization_41/add_1:0' shape=(1, 144, 28, 28) dtype=float32>, '460': <tf.Tensor 'Sigmoid_42:0' shape=(1, 144, 28, 28) dtype=float32>, '461': <tf.Tensor 'Mul_43:0' shape=(1, 144, 28, 28) dtype=float32>, '462': <tf.Tensor 'ReduceMean_44:0' shape=(1, 144, 1, 1) dtype=float32>, '463': <tf.Tensor 'Add_7:0' shape=(1, 12, 1, 1) dtype=float32>, '464': <tf.Tensor 'Sigmoid_46:0' shape=(1, 12, 1, 1) dtype=float32>, '465': <tf.Tensor 'Mul_47:0' shape=(1, 12, 1, 1) dtype=float32>, '466': <tf.Tensor 'Add_8:0' shape=(1, 144, 1, 1) dtype=float32>, '467': <tf.Tensor 'Sigmoid_49:0' shape=(1, 144, 1, 1) dtype=float32>, '468': <tf.Tensor 'Mul_50:0' shape=(1, 144, 28, 28) dtype=float32>, '855': <tf.Tensor 'Add_9:0' shape=(1, 40, 28, 28) dtype=float32>, '471': <tf.Tensor 'Split_52:0' shape=(1, 20, 28, 28) dtype=float32>, '472': <tf.Tensor 'Split_52:1' shape=(1, 20, 28, 28) dtype=float32>, '473': <tf.Tensor 'concat_23/concat:0' shape=(1, 120, 28, 28) dtype=float32>, '474': <tf.Tensor 'concat_24/concat:0' shape=(1, 120, 28, 28) dtype=float32>, '475': <tf.Tensor 'Concat_55:0' shape=(1, 240, 28, 28) dtype=float32>, '476': <tf.Tensor 'BatchNormalization_56/add_1:0' shape=(1, 240, 28, 28) dtype=float32>, '477': <tf.Tensor 'Sigmoid_57:0' shape=(1, 240, 28, 28) dtype=float32>, '478': <tf.Tensor 'Mul_58:0' shape=(1, 240, 28, 28) dtype=float32>, '479': <tf.Tensor 'Split_59:0' shape=(1, 120, 28, 28) dtype=float32>, '480': <tf.Tensor 'Split_59:1' shape=(1, 120, 28, 28) dtype=float32>, '481': <tf.Tensor 'concat_25:0' shape=(1, 120, 28, 28) dtype=float32>, '482': <tf.Tensor 'concat_26:0' shape=(1, 120, 28, 28) dtype=float32>, '483': <tf.Tensor 'Concat_62:0' shape=(1, 240, 28, 28) dtype=float32>, '484': <tf.Tensor 'BatchNormalization_63/add_1:0' shape=(1, 240, 28, 28) dtype=float32>, '485': <tf.Tensor 'Sigmoid_64:0' shape=(1, 240, 28, 28) dtype=float32>, '486': <tf.Tensor 'Mul_65:0' shape=(1, 240, 28, 28) dtype=float32>, '487': <tf.Tensor 'ReduceMean_66:0' shape=(1, 240, 1, 1) dtype=float32>, '488': <tf.Tensor 'Add_10:0' shape=(1, 20, 1, 1) dtype=float32>, '489': <tf.Tensor 'Sigmoid_68:0' shape=(1, 20, 1, 1) dtype=float32>, '490': <tf.Tensor 'Mul_69:0' shape=(1, 20, 1, 1) dtype=float32>, '491': <tf.Tensor 'Add_11:0' shape=(1, 240, 1, 1) dtype=float32>, '492': <tf.Tensor 'Sigmoid_71:0' shape=(1, 240, 1, 1) dtype=float32>, '493': <tf.Tensor 'Mul_72:0' shape=(1, 240, 28, 28) dtype=float32>, '494': <tf.Tensor 'Split_73:0' shape=(1, 120, 28, 28) dtype=float32>, '495': <tf.Tensor 'Split_73:1' shape=(1, 120, 28, 28) dtype=float32>, '496': <tf.Tensor 'concat_29/concat:0' shape=(1, 20, 28, 28) dtype=float32>, '497': <tf.Tensor 'concat_31/concat:0' shape=(1, 20, 28, 28) dtype=float32>, '498': <tf.Tensor 'Concat_76:0' shape=(1, 40, 28, 28) dtype=float32>, '499': <tf.Tensor 'BatchNormalization_77/add_1:0' shape=(1, 40, 28, 28) dtype=float32>, '500': <tf.Tensor 'Add_78:0' shape=(1, 40, 28, 28) dtype=float32>, '501': <tf.Tensor 'Split_79:0' shape=(1, 20, 28, 28) dtype=float32>, '502': <tf.Tensor 'Split_79:1' shape=(1, 20, 28, 28) dtype=float32>, '503': <tf.Tensor 'concat_32/concat:0' shape=(1, 120, 28, 28) dtype=float32>, '504': <tf.Tensor 'concat_33/concat:0' shape=(1, 120, 28, 28) dtype=float32>, '505': <tf.Tensor 'Concat_82:0' shape=(1, 240, 28, 28) dtype=float32>, '506': <tf.Tensor 'BatchNormalization_83/add_1:0' shape=(1, 240, 28, 28) dtype=float32>, '507': <tf.Tensor 'Sigmoid_84:0' shape=(1, 240, 28, 28) dtype=float32>, '508': <tf.Tensor 'Mul_85:0' shape=(1, 240, 28, 28) dtype=float32>, '509': <tf.Tensor 'Split_86:0' shape=(1, 120, 28, 28) dtype=float32>, '510': <tf.Tensor 'Split_86:1' shape=(1, 120, 28, 28) dtype=float32>, '511': <tf.Tensor 'concat_34:0' shape=(1, 120, 28, 28) dtype=float32>, '512': <tf.Tensor 'concat_35:0' shape=(1, 120, 28, 28) dtype=float32>, '513': <tf.Tensor 'Concat_89:0' shape=(1, 240, 28, 28) dtype=float32>, '514': <tf.Tensor 'BatchNormalization_90/add_1:0' shape=(1, 240, 28, 28) dtype=float32>, '515': <tf.Tensor 'Sigmoid_91:0' shape=(1, 240, 28, 28) dtype=float32>, '516': <tf.Tensor 'Mul_92:0' shape=(1, 240, 28, 28) dtype=float32>, '517': <tf.Tensor 'ReduceMean_93:0' shape=(1, 240, 1, 1) dtype=float32>, '518': <tf.Tensor 'Add_12:0' shape=(1, 20, 1, 1) dtype=float32>, '519': <tf.Tensor 'Sigmoid_95:0' shape=(1, 20, 1, 1) dtype=float32>, '520': <tf.Tensor 'Mul_96:0' shape=(1, 20, 1, 1) dtype=float32>, '521': <tf.Tensor 'Add_13:0' shape=(1, 240, 1, 1) dtype=float32>, '522': <tf.Tensor 'Sigmoid_98:0' shape=(1, 240, 1, 1) dtype=float32>, '523': <tf.Tensor 'Mul_99:0' shape=(1, 240, 28, 28) dtype=float32>, '524': <tf.Tensor 'Split_100:0' shape=(1, 120, 28, 28) dtype=float32>, '525': <tf.Tensor 'Split_100:1' shape=(1, 120, 28, 28) dtype=float32>, '526': <tf.Tensor 'concat_38/concat:0' shape=(1, 20, 28, 28) dtype=float32>, '527': <tf.Tensor 'concat_39/concat:0' shape=(1, 20, 28, 28) dtype=float32>, '528': <tf.Tensor 'Concat_103:0' shape=(1, 40, 28, 28) dtype=float32>, '529': <tf.Tensor 'BatchNormalization_104/add_1:0' shape=(1, 40, 28, 28) dtype=float32>, '530': <tf.Tensor 'Add_105:0' shape=(1, 40, 28, 28) dtype=float32>, '531': <tf.Tensor 'Split_106:0' shape=(1, 20, 28, 28) dtype=float32>, '532': <tf.Tensor 'Split_106:1' shape=(1, 20, 28, 28) dtype=float32>, '533': <tf.Tensor 'concat_41/concat:0' shape=(1, 120, 28, 28) dtype=float32>, '534': <tf.Tensor 'concat_42/concat:0' shape=(1, 120, 28, 28) dtype=float32>, '535': <tf.Tensor 'Concat_109:0' shape=(1, 240, 28, 28) dtype=float32>, '536': <tf.Tensor 'BatchNormalization_110/add_1:0' shape=(1, 240, 28, 28) dtype=float32>, '537': <tf.Tensor 'Sigmoid_111:0' shape=(1, 240, 28, 28) dtype=float32>, '538': <tf.Tensor 'Mul_112:0' shape=(1, 240, 28, 28) dtype=float32>, '539': <tf.Tensor 'Split_113:0' shape=(1, 120, 28, 28) dtype=float32>, '540': <tf.Tensor 'Split_113:1' shape=(1, 120, 28, 28) dtype=float32>, '541': <tf.Tensor 'concat_43:0' shape=(1, 120, 28, 28) dtype=float32>, '542': <tf.Tensor 'concat_44:0' shape=(1, 120, 28, 28) dtype=float32>, '543': <tf.Tensor 'Concat_116:0' shape=(1, 240, 28, 28) dtype=float32>, '544': <tf.Tensor 'BatchNormalization_117/add_1:0' shape=(1, 240, 28, 28) dtype=float32>, '545': <tf.Tensor 'Sigmoid_118:0' shape=(1, 240, 28, 28) dtype=float32>, '546': <tf.Tensor 'Mul_119:0' shape=(1, 240, 28, 28) dtype=float32>, '547': <tf.Tensor 'ReduceMean_120:0' shape=(1, 240, 1, 1) dtype=float32>, '548': <tf.Tensor 'Add_14:0' shape=(1, 20, 1, 1) dtype=float32>, '549': <tf.Tensor 'Sigmoid_122:0' shape=(1, 20, 1, 1) dtype=float32>, '550': <tf.Tensor 'Mul_123:0' shape=(1, 20, 1, 1) dtype=float32>, '551': <tf.Tensor 'Add_15:0' shape=(1, 240, 1, 1) dtype=float32>, '552': <tf.Tensor 'Sigmoid_125:0' shape=(1, 240, 1, 1) dtype=float32>, '553': <tf.Tensor 'Mul_126:0' shape=(1, 240, 28, 28) dtype=float32>, '554': <tf.Tensor 'Split_127:0' shape=(1, 120, 28, 28) dtype=float32>, '555': <tf.Tensor 'Split_127:1' shape=(1, 120, 28, 28) dtype=float32>, '556': <tf.Tensor 'concat_47/concat:0' shape=(1, 20, 28, 28) dtype=float32>, '557': <tf.Tensor 'concat_48/concat:0' shape=(1, 20, 28, 28) dtype=float32>, '558': <tf.Tensor 'Concat_130:0' shape=(1, 40, 28, 28) dtype=float32>, '559': <tf.Tensor 'BatchNormalization_131/add_1:0' shape=(1, 40, 28, 28) dtype=float32>, '560': <tf.Tensor 'Add_132:0' shape=(1, 40, 28, 28) dtype=float32>, '858': <tf.Tensor 'Add_16:0' shape=(1, 240, 28, 28) dtype=float32>, '563': <tf.Tensor 'Sigmoid_134:0' shape=(1, 240, 28, 28) dtype=float32>, '564': <tf.Tensor 'Mul_135:0' shape=(1, 240, 28, 28) dtype=float32>, '565': <tf.Tensor 'Split_136:0' shape=(1, 80, 28, 28) dtype=float32>, '566': <tf.Tensor 'Split_136:1' shape=(1, 80, 28, 28) dtype=float32>, '567': <tf.Tensor 'Split_136:2' shape=(1, 80, 28, 28) dtype=float32>, '568': <tf.Tensor 'concat_50:0' shape=(1, 80, 14, 14) dtype=float32>, '569': <tf.Tensor 'concat_51:0' shape=(1, 80, 14, 14) dtype=float32>, '570': <tf.Tensor 'concat_52:0' shape=(1, 80, 14, 14) dtype=float32>, '571': <tf.Tensor 'Concat_140:0' shape=(1, 240, 14, 14) dtype=float32>, '572': <tf.Tensor 'BatchNormalization_141/add_1:0' shape=(1, 240, 14, 14) dtype=float32>, '573': <tf.Tensor 'Sigmoid_142:0' shape=(1, 240, 14, 14) dtype=float32>, '574': <tf.Tensor 'Mul_143:0' shape=(1, 240, 14, 14) dtype=float32>, '575': <tf.Tensor 'ReduceMean_144:0' shape=(1, 240, 1, 1) dtype=float32>, '576': <tf.Tensor 'Add_17:0' shape=(1, 10, 1, 1) dtype=float32>, '577': <tf.Tensor 'Sigmoid_146:0' shape=(1, 10, 1, 1) dtype=float32>, '578': <tf.Tensor 'Mul_147:0' shape=(1, 10, 1, 1) dtype=float32>, '579': <tf.Tensor 'Add_18:0' shape=(1, 240, 1, 1) dtype=float32>, '580': <tf.Tensor 'Sigmoid_149:0' shape=(1, 240, 1, 1) dtype=float32>, '581': <tf.Tensor 'Mul_150:0' shape=(1, 240, 14, 14) dtype=float32>, '582': <tf.Tensor 'Split_151:0' shape=(1, 120, 14, 14) dtype=float32>, '583': <tf.Tensor 'Split_151:1' shape=(1, 120, 14, 14) dtype=float32>, '584': <tf.Tensor 'concat_56/concat:0' shape=(1, 40, 14, 14) dtype=float32>, '585': <tf.Tensor 'concat_57/concat:0' shape=(1, 40, 14, 14) dtype=float32>, '586': <tf.Tensor 'Concat_154:0' shape=(1, 80, 14, 14) dtype=float32>, '587': <tf.Tensor 'BatchNormalization_155/add_1:0' shape=(1, 80, 14, 14) dtype=float32>, '861': <tf.Tensor 'Add_19:0' shape=(1, 480, 14, 14) dtype=float32>, '590': <tf.Tensor 'Sigmoid_157:0' shape=(1, 480, 14, 14) dtype=float32>, '591': <tf.Tensor 'Mul_158:0' shape=(1, 480, 14, 14) dtype=float32>, '592': <tf.Tensor 'Split_159:0' shape=(1, 240, 14, 14) dtype=float32>, '593': <tf.Tensor 'Split_159:1' shape=(1, 240, 14, 14) dtype=float32>, '594': <tf.Tensor 'concat_59:0' shape=(1, 240, 14, 14) dtype=float32>, '595': <tf.Tensor 'concat_60:0' shape=(1, 240, 14, 14) dtype=float32>, '596': <tf.Tensor 'Concat_162:0' shape=(1, 480, 14, 14) dtype=float32>, '597': <tf.Tensor 'BatchNormalization_163/add_1:0' shape=(1, 480, 14, 14) dtype=float32>, '598': <tf.Tensor 'Sigmoid_164:0' shape=(1, 480, 14, 14) dtype=float32>, '599': <tf.Tensor 'Mul_165:0' shape=(1, 480, 14, 14) dtype=float32>, '600': <tf.Tensor 'ReduceMean_166:0' shape=(1, 480, 1, 1) dtype=float32>, '601': <tf.Tensor 'Add_20:0' shape=(1, 20, 1, 1) dtype=float32>, '602': <tf.Tensor 'Sigmoid_168:0' shape=(1, 20, 1, 1) dtype=float32>, '603': <tf.Tensor 'Mul_169:0' shape=(1, 20, 1, 1) dtype=float32>, '604': <tf.Tensor 'Add_21:0' shape=(1, 480, 1, 1) dtype=float32>, '605': <tf.Tensor 'Sigmoid_171:0' shape=(1, 480, 1, 1) dtype=float32>, '606': <tf.Tensor 'Mul_172:0' shape=(1, 480, 14, 14) dtype=float32>, '607': <tf.Tensor 'Split_173:0' shape=(1, 240, 14, 14) dtype=float32>, '608': <tf.Tensor 'Split_173:1' shape=(1, 240, 14, 14) dtype=float32>, '609': <tf.Tensor 'concat_64/concat:0' shape=(1, 40, 14, 14) dtype=float32>, '610': <tf.Tensor 'concat_65/concat:0' shape=(1, 40, 14, 14) dtype=float32>, '611': <tf.Tensor 'Concat_176:0' shape=(1, 80, 14, 14) dtype=float32>, '612': <tf.Tensor 'BatchNormalization_177/add_1:0' shape=(1, 80, 14, 14) dtype=float32>, '613': <tf.Tensor 'Add_178:0' shape=(1, 80, 14, 14) dtype=float32>, '864': <tf.Tensor 'Add_22:0' shape=(1, 480, 14, 14) dtype=float32>, '616': <tf.Tensor 'Sigmoid_180:0' shape=(1, 480, 14, 14) dtype=float32>, '617': <tf.Tensor 'Mul_181:0' shape=(1, 480, 14, 14) dtype=float32>, '618': <tf.Tensor 'Split_182:0' shape=(1, 240, 14, 14) dtype=float32>, '619': <tf.Tensor 'Split_182:1' shape=(1, 240, 14, 14) dtype=float32>, '620': <tf.Tensor 'concat_67:0' shape=(1, 240, 14, 14) dtype=float32>, '621': <tf.Tensor 'concat_68:0' shape=(1, 240, 14, 14) dtype=float32>, '622': <tf.Tensor 'Concat_185:0' shape=(1, 480, 14, 14) dtype=float32>, '623': <tf.Tensor 'BatchNormalization_186/add_1:0' shape=(1, 480, 14, 14) dtype=float32>, '624': <tf.Tensor 'Sigmoid_187:0' shape=(1, 480, 14, 14) dtype=float32>, '625': <tf.Tensor 'Mul_188:0' shape=(1, 480, 14, 14) dtype=float32>, '626': <tf.Tensor 'ReduceMean_189:0' shape=(1, 480, 1, 1) dtype=float32>, '627': <tf.Tensor 'Add_23:0' shape=(1, 20, 1, 1) dtype=float32>, '628': <tf.Tensor 'Sigmoid_191:0' shape=(1, 20, 1, 1) dtype=float32>, '629': <tf.Tensor 'Mul_192:0' shape=(1, 20, 1, 1) dtype=float32>, '630': <tf.Tensor 'Add_24:0' shape=(1, 480, 1, 1) dtype=float32>, '631': <tf.Tensor 'Sigmoid_194:0' shape=(1, 480, 1, 1) dtype=float32>, '632': <tf.Tensor 'Mul_195:0' shape=(1, 480, 14, 14) dtype=float32>, '633': <tf.Tensor 'Split_196:0' shape=(1, 240, 14, 14) dtype=float32>, '634': <tf.Tensor 'Split_196:1' shape=(1, 240, 14, 14) dtype=float32>, '635': <tf.Tensor 'concat_71/concat:0' shape=(1, 40, 14, 14) dtype=float32>, '636': <tf.Tensor 'concat_72/concat:0' shape=(1, 40, 14, 14) dtype=float32>, '637': <tf.Tensor 'Concat_199:0' shape=(1, 80, 14, 14) dtype=float32>, '638': <tf.Tensor 'BatchNormalization_200/add_1:0' shape=(1, 80, 14, 14) dtype=float32>, '639': <tf.Tensor 'Add_201:0' shape=(1, 80, 14, 14) dtype=float32>, '640': <tf.Tensor 'Split_202:0' shape=(1, 40, 14, 14) dtype=float32>, '641': <tf.Tensor 'Split_202:1' shape=(1, 40, 14, 14) dtype=float32>, '642': <tf.Tensor 'concat_73/concat:0' shape=(1, 240, 14, 14) dtype=float32>, '643': <tf.Tensor 'concat_74/concat:0' shape=(1, 240, 14, 14) dtype=float32>, '644': <tf.Tensor 'Concat_205:0' shape=(1, 480, 14, 14) dtype=float32>, '645': <tf.Tensor 'BatchNormalization_206/add_1:0' shape=(1, 480, 14, 14) dtype=float32>, '646': <tf.Tensor 'Sigmoid_207:0' shape=(1, 480, 14, 14) dtype=float32>, '647': <tf.Tensor 'Mul_208:0' shape=(1, 480, 14, 14) dtype=float32>, '648': <tf.Tensor 'Split_209:0' shape=(1, 160, 14, 14) dtype=float32>, '649': <tf.Tensor 'Split_209:1' shape=(1, 160, 14, 14) dtype=float32>, '650': <tf.Tensor 'Split_209:2' shape=(1, 160, 14, 14) dtype=float32>, '651': <tf.Tensor 'concat_75:0' shape=(1, 160, 14, 14) dtype=float32>, '652': <tf.Tensor 'concat_77:0' shape=(1, 160, 14, 14) dtype=float32>, '653': <tf.Tensor 'concat_78:0' shape=(1, 160, 14, 14) dtype=float32>, '654': <tf.Tensor 'Concat_213:0' shape=(1, 480, 14, 14) dtype=float32>, '655': <tf.Tensor 'BatchNormalization_214/add_1:0' shape=(1, 480, 14, 14) dtype=float32>, '656': <tf.Tensor 'Sigmoid_215:0' shape=(1, 480, 14, 14) dtype=float32>, '657': <tf.Tensor 'Mul_216:0' shape=(1, 480, 14, 14) dtype=float32>, '658': <tf.Tensor 'ReduceMean_217:0' shape=(1, 480, 1, 1) dtype=float32>, '659': <tf.Tensor 'Add_25:0' shape=(1, 40, 1, 1) dtype=float32>, '660': <tf.Tensor 'Sigmoid_219:0' shape=(1, 40, 1, 1) dtype=float32>, '661': <tf.Tensor 'Mul_220:0' shape=(1, 40, 1, 1) dtype=float32>, '662': <tf.Tensor 'Add_26:0' shape=(1, 480, 1, 1) dtype=float32>, '663': <tf.Tensor 'Sigmoid_222:0' shape=(1, 480, 1, 1) dtype=float32>, '664': <tf.Tensor 'Mul_223:0' shape=(1, 480, 14, 14) dtype=float32>, '665': <tf.Tensor 'Split_224:0' shape=(1, 240, 14, 14) dtype=float32>, '666': <tf.Tensor 'Split_224:1' shape=(1, 240, 14, 14) dtype=float32>, '667': <tf.Tensor 'concat_81/concat:0' shape=(1, 60, 14, 14) dtype=float32>, '668': <tf.Tensor 'concat_83/concat:0' shape=(1, 60, 14, 14) dtype=float32>, '669': <tf.Tensor 'Concat_227:0' shape=(1, 120, 14, 14) dtype=float32>, '670': <tf.Tensor 'BatchNormalization_228/add_1:0' shape=(1, 120, 14, 14) dtype=float32>, '671': <tf.Tensor 'Split_229:0' shape=(1, 60, 14, 14) dtype=float32>, '672': <tf.Tensor 'Split_229:1' shape=(1, 60, 14, 14) dtype=float32>, '673': <tf.Tensor 'concat_84/concat:0' shape=(1, 180, 14, 14) dtype=float32>, '674': <tf.Tensor 'concat_85/concat:0' shape=(1, 180, 14, 14) dtype=float32>, '675': <tf.Tensor 'Concat_232:0' shape=(1, 360, 14, 14) dtype=float32>, '676': <tf.Tensor 'BatchNormalization_233/add_1:0' shape=(1, 360, 14, 14) dtype=float32>, '677': <tf.Tensor 'Sigmoid_234:0' shape=(1, 360, 14, 14) dtype=float32>, '678': <tf.Tensor 'Mul_235:0' shape=(1, 360, 14, 14) dtype=float32>, '679': <tf.Tensor 'Split_236:0' shape=(1, 90, 14, 14) dtype=float32>, '680': <tf.Tensor 'Split_236:1' shape=(1, 90, 14, 14) dtype=float32>, '681': <tf.Tensor 'Split_236:2' shape=(1, 90, 14, 14) dtype=float32>, '682': <tf.Tensor 'Split_236:3' shape=(1, 90, 14, 14) dtype=float32>, '683': <tf.Tensor 'concat_86:0' shape=(1, 90, 14, 14) dtype=float32>, '684': <tf.Tensor 'concat_87:0' shape=(1, 90, 14, 14) dtype=float32>, '685': <tf.Tensor 'concat_88:0' shape=(1, 90, 14, 14) dtype=float32>, '686': <tf.Tensor 'concat_90:0' shape=(1, 90, 14, 14) dtype=float32>, '687': <tf.Tensor 'Concat_241:0' shape=(1, 360, 14, 14) dtype=float32>, '688': <tf.Tensor 'BatchNormalization_242/add_1:0' shape=(1, 360, 14, 14) dtype=float32>, '689': <tf.Tensor 'Sigmoid_243:0' shape=(1, 360, 14, 14) dtype=float32>, '690': <tf.Tensor 'Mul_244:0' shape=(1, 360, 14, 14) dtype=float32>, '691': <tf.Tensor 'ReduceMean_245:0' shape=(1, 360, 1, 1) dtype=float32>, '692': <tf.Tensor 'Add_27:0' shape=(1, 60, 1, 1) dtype=float32>, '693': <tf.Tensor 'Sigmoid_247:0' shape=(1, 60, 1, 1) dtype=float32>, '694': <tf.Tensor 'Mul_248:0' shape=(1, 60, 1, 1) dtype=float32>, '695': <tf.Tensor 'Add_28:0' shape=(1, 360, 1, 1) dtype=float32>, '696': <tf.Tensor 'Sigmoid_250:0' shape=(1, 360, 1, 1) dtype=float32>, '697': <tf.Tensor 'Mul_251:0' shape=(1, 360, 14, 14) dtype=float32>, '698': <tf.Tensor 'Split_252:0' shape=(1, 180, 14, 14) dtype=float32>, '699': <tf.Tensor 'Split_252:1' shape=(1, 180, 14, 14) dtype=float32>, '700': <tf.Tensor 'concat_93/concat:0' shape=(1, 60, 14, 14) dtype=float32>, '701': <tf.Tensor 'concat_94/concat:0' shape=(1, 60, 14, 14) dtype=float32>, '702': <tf.Tensor 'Concat_255:0' shape=(1, 120, 14, 14) dtype=float32>, '703': <tf.Tensor 'BatchNormalization_256/add_1:0' shape=(1, 120, 14, 14) dtype=float32>, '704': <tf.Tensor 'Add_257:0' shape=(1, 120, 14, 14) dtype=float32>, '705': <tf.Tensor 'Split_258:0' shape=(1, 60, 14, 14) dtype=float32>, '706': <tf.Tensor 'Split_258:1' shape=(1, 60, 14, 14) dtype=float32>, '707': <tf.Tensor 'concat_95/concat:0' shape=(1, 180, 14, 14) dtype=float32>, '708': <tf.Tensor 'concat_96/concat:0' shape=(1, 180, 14, 14) dtype=float32>, '709': <tf.Tensor 'Concat_261:0' shape=(1, 360, 14, 14) dtype=float32>, '710': <tf.Tensor 'BatchNormalization_262/add_1:0' shape=(1, 360, 14, 14) dtype=float32>, '711': <tf.Tensor 'Sigmoid_263:0' shape=(1, 360, 14, 14) dtype=float32>, '712': <tf.Tensor 'Mul_264:0' shape=(1, 360, 14, 14) dtype=float32>, '713': <tf.Tensor 'Split_265:0' shape=(1, 90, 14, 14) dtype=float32>, '714': <tf.Tensor 'Split_265:1' shape=(1, 90, 14, 14) dtype=float32>, '715': <tf.Tensor 'Split_265:2' shape=(1, 90, 14, 14) dtype=float32>, '716': <tf.Tensor 'Split_265:3' shape=(1, 90, 14, 14) dtype=float32>, '717': <tf.Tensor 'concat_97:0' shape=(1, 90, 14, 14) dtype=float32>, '718': <tf.Tensor 'concat_98:0' shape=(1, 90, 14, 14) dtype=float32>, '719': <tf.Tensor 'concat_99:0' shape=(1, 90, 14, 14) dtype=float32>, '720': <tf.Tensor 'concat_100:0' shape=(1, 90, 14, 14) dtype=float32>, '721': <tf.Tensor 'Concat_270:0' shape=(1, 360, 14, 14) dtype=float32>, '722': <tf.Tensor 'BatchNormalization_271/add_1:0' shape=(1, 360, 14, 14) dtype=float32>, '723': <tf.Tensor 'Sigmoid_272:0' shape=(1, 360, 14, 14) dtype=float32>, '724': <tf.Tensor 'Mul_273:0' shape=(1, 360, 14, 14) dtype=float32>, '725': <tf.Tensor 'ReduceMean_274:0' shape=(1, 360, 1, 1) dtype=float32>, '726': <tf.Tensor 'Add_29:0' shape=(1, 60, 1, 1) dtype=float32>, '727': <tf.Tensor 'Sigmoid_276:0' shape=(1, 60, 1, 1) dtype=float32>, '728': <tf.Tensor 'Mul_277:0' shape=(1, 60, 1, 1) dtype=float32>, '729': <tf.Tensor 'Add_30:0' shape=(1, 360, 1, 1) dtype=float32>, '730': <tf.Tensor 'Sigmoid_279:0' shape=(1, 360, 1, 1) dtype=float32>, '731': <tf.Tensor 'Mul_280:0' shape=(1, 360, 14, 14) dtype=float32>, '732': <tf.Tensor 'Split_281:0' shape=(1, 180, 14, 14) dtype=float32>, '733': <tf.Tensor 'Split_281:1' shape=(1, 180, 14, 14) dtype=float32>, '734': <tf.Tensor 'concat_104/concat:0' shape=(1, 60, 14, 14) dtype=float32>, '735': <tf.Tensor 'concat_105/concat:0' shape=(1, 60, 14, 14) dtype=float32>, '736': <tf.Tensor 'Concat_284:0' shape=(1, 120, 14, 14) dtype=float32>, '737': <tf.Tensor 'BatchNormalization_285/add_1:0' shape=(1, 120, 14, 14) dtype=float32>, '738': <tf.Tensor 'Add_286:0' shape=(1, 120, 14, 14) dtype=float32>, '867': <tf.Tensor 'Add_31:0' shape=(1, 720, 14, 14) dtype=float32>, '741': <tf.Tensor 'Sigmoid_288:0' shape=(1, 720, 14, 14) dtype=float32>, '742': <tf.Tensor 'Mul_289:0' shape=(1, 720, 14, 14) dtype=float32>, '743': <tf.Tensor 'Split_290:0' shape=(1, 144, 14, 14) dtype=float32>, '744': <tf.Tensor 'Split_290:1' shape=(1, 144, 14, 14) dtype=float32>, '745': <tf.Tensor 'Split_290:2' shape=(1, 144, 14, 14) dtype=float32>, '746': <tf.Tensor 'Split_290:3' shape=(1, 144, 14, 14) dtype=float32>, '747': <tf.Tensor 'Split_290:4' shape=(1, 144, 14, 14) dtype=float32>, '748': <tf.Tensor 'concat_107:0' shape=(1, 144, 7, 7) dtype=float32>, '749': <tf.Tensor 'concat_108:0' shape=(1, 144, 7, 7) dtype=float32>, '750': <tf.Tensor 'concat_110:0' shape=(1, 144, 7, 7) dtype=float32>, '751': <tf.Tensor 'concat_111:0' shape=(1, 144, 7, 7) dtype=float32>, '752': <tf.Tensor 'concat_112:0' shape=(1, 144, 7, 7) dtype=float32>, '753': <tf.Tensor 'Concat_296:0' shape=(1, 720, 7, 7) dtype=float32>, '754': <tf.Tensor 'BatchNormalization_297/add_1:0' shape=(1, 720, 7, 7) dtype=float32>, '755': <tf.Tensor 'Sigmoid_298:0' shape=(1, 720, 7, 7) dtype=float32>, '756': <tf.Tensor 'Mul_299:0' shape=(1, 720, 7, 7) dtype=float32>, '757': <tf.Tensor 'ReduceMean_300:0' shape=(1, 720, 1, 1) dtype=float32>, '758': <tf.Tensor 'Add_33:0' shape=(1, 60, 1, 1) dtype=float32>, '759': <tf.Tensor 'Sigmoid_302:0' shape=(1, 60, 1, 1) dtype=float32>, '760': <tf.Tensor 'Mul_303:0' shape=(1, 60, 1, 1) dtype=float32>, '761': <tf.Tensor 'Add_34:0' shape=(1, 720, 1, 1) dtype=float32>, '762': <tf.Tensor 'Sigmoid_305:0' shape=(1, 720, 1, 1) dtype=float32>, '763': <tf.Tensor 'Mul_306:0' shape=(1, 720, 7, 7) dtype=float32>, '870': <tf.Tensor 'Add_35:0' shape=(1, 200, 7, 7) dtype=float32>, '873': <tf.Tensor 'Add_36:0' shape=(1, 1200, 7, 7) dtype=float32>, '768': <tf.Tensor 'Sigmoid_309:0' shape=(1, 1200, 7, 7) dtype=float32>, '769': <tf.Tensor 'Mul_310:0' shape=(1, 1200, 7, 7) dtype=float32>, '770': <tf.Tensor 'Split_311:0' shape=(1, 300, 7, 7) dtype=float32>, '771': <tf.Tensor 'Split_311:1' shape=(1, 300, 7, 7) dtype=float32>, '772': <tf.Tensor 'Split_311:2' shape=(1, 300, 7, 7) dtype=float32>, '773': <tf.Tensor 'Split_311:3' shape=(1, 300, 7, 7) dtype=float32>, '774': <tf.Tensor 'concat_118:0' shape=(1, 300, 7, 7) dtype=float32>, '775': <tf.Tensor 'concat_119:0' shape=(1, 300, 7, 7) dtype=float32>, '776': <tf.Tensor 'concat_120:0' shape=(1, 300, 7, 7) dtype=float32>, '777': <tf.Tensor 'concat_121:0' shape=(1, 300, 7, 7) dtype=float32>, '778': <tf.Tensor 'Concat_316:0' shape=(1, 1200, 7, 7) dtype=float32>, '779': <tf.Tensor 'BatchNormalization_317/add_1:0' shape=(1, 1200, 7, 7) dtype=float32>, '780': <tf.Tensor 'Sigmoid_318:0' shape=(1, 1200, 7, 7) dtype=float32>, '781': <tf.Tensor 'Mul_319:0' shape=(1, 1200, 7, 7) dtype=float32>, '782': <tf.Tensor 'ReduceMean_320:0' shape=(1, 1200, 1, 1) dtype=float32>, '783': <tf.Tensor 'Add_37:0' shape=(1, 100, 1, 1) dtype=float32>, '784': <tf.Tensor 'Sigmoid_322:0' shape=(1, 100, 1, 1) dtype=float32>, '785': <tf.Tensor 'Mul_323:0' shape=(1, 100, 1, 1) dtype=float32>, '786': <tf.Tensor 'Add_38:0' shape=(1, 1200, 1, 1) dtype=float32>, '787': <tf.Tensor 'Sigmoid_325:0' shape=(1, 1200, 1, 1) dtype=float32>, '788': <tf.Tensor 'Mul_326:0' shape=(1, 1200, 7, 7) dtype=float32>, '789': <tf.Tensor 'Split_327:0' shape=(1, 600, 7, 7) dtype=float32>, '790': <tf.Tensor 'Split_327:1' shape=(1, 600, 7, 7) dtype=float32>, '791': <tf.Tensor 'concat_124/concat:0' shape=(1, 100, 7, 7) dtype=float32>, '792': <tf.Tensor 'concat_125/concat:0' shape=(1, 100, 7, 7) dtype=float32>, '793': <tf.Tensor 'Concat_330:0' shape=(1, 200, 7, 7) dtype=float32>, '794': <tf.Tensor 'BatchNormalization_331/add_1:0' shape=(1, 200, 7, 7) dtype=float32>, '795': <tf.Tensor 'Add_332:0' shape=(1, 200, 7, 7) dtype=float32>, '876': <tf.Tensor 'Add_39:0' shape=(1, 1200, 7, 7) dtype=float32>, '798': <tf.Tensor 'Sigmoid_334:0' shape=(1, 1200, 7, 7) dtype=float32>, '799': <tf.Tensor 'Mul_335:0' shape=(1, 1200, 7, 7) dtype=float32>, '800': <tf.Tensor 'Split_336:0' shape=(1, 300, 7, 7) dtype=float32>, '801': <tf.Tensor 'Split_336:1' shape=(1, 300, 7, 7) dtype=float32>, '802': <tf.Tensor 'Split_336:2' shape=(1, 300, 7, 7) dtype=float32>, '803': <tf.Tensor 'Split_336:3' shape=(1, 300, 7, 7) dtype=float32>, '804': <tf.Tensor 'concat_127:0' shape=(1, 300, 7, 7) dtype=float32>, '805': <tf.Tensor 'concat_128:0' shape=(1, 300, 7, 7) dtype=float32>, '806': <tf.Tensor 'concat_129:0' shape=(1, 300, 7, 7) dtype=float32>, '807': <tf.Tensor 'concat_131:0' shape=(1, 300, 7, 7) dtype=float32>, '808': <tf.Tensor 'Concat_341:0' shape=(1, 1200, 7, 7) dtype=float32>, '809': <tf.Tensor 'BatchNormalization_342/add_1:0' shape=(1, 1200, 7, 7) dtype=float32>, '810': <tf.Tensor 'Sigmoid_343:0' shape=(1, 1200, 7, 7) dtype=float32>, '811': <tf.Tensor 'Mul_344:0' shape=(1, 1200, 7, 7) dtype=float32>, '812': <tf.Tensor 'ReduceMean_345:0' shape=(1, 1200, 1, 1) dtype=float32>, '813': <tf.Tensor 'Add_40:0' shape=(1, 100, 1, 1) dtype=float32>, '814': <tf.Tensor 'Sigmoid_347:0' shape=(1, 100, 1, 1) dtype=float32>, '815': <tf.Tensor 'Mul_348:0' shape=(1, 100, 1, 1) dtype=float32>, '816': <tf.Tensor 'Add_41:0' shape=(1, 1200, 1, 1) dtype=float32>, '817': <tf.Tensor 'Sigmoid_350:0' shape=(1, 1200, 1, 1) dtype=float32>, '818': <tf.Tensor 'Mul_351:0' shape=(1, 1200, 7, 7) dtype=float32>, '819': <tf.Tensor 'Split_352:0' shape=(1, 600, 7, 7) dtype=float32>, '820': <tf.Tensor 'Split_352:1' shape=(1, 600, 7, 7) dtype=float32>, '821': <tf.Tensor 'concat_134/concat:0' shape=(1, 100, 7, 7) dtype=float32>, '822': <tf.Tensor 'concat_135/concat:0' shape=(1, 100, 7, 7) dtype=float32>, '823': <tf.Tensor 'Concat_355:0' shape=(1, 200, 7, 7) dtype=float32>, '824': <tf.Tensor 'BatchNormalization_356/add_1:0' shape=(1, 200, 7, 7) dtype=float32>, '825': <tf.Tensor 'Add_357:0' shape=(1, 200, 7, 7) dtype=float32>, '879': <tf.Tensor 'Add_42:0' shape=(1, 1536, 7, 7) dtype=float32>, '828': <tf.Tensor 'Relu_359:0' shape=(1, 1536, 7, 7) dtype=float32>, '829': <tf.Tensor 'Mean:0' shape=(1, 1536, 1, 1) dtype=float32>, '830': <tf.Tensor 'Reshape_136:0' shape=(1, 1536) dtype=float32>, '831': <tf.Tensor 'add_43:0' shape=(1, 512) dtype=float32>, '832': <tf.Tensor 'BatchNormalization_363/add_1:0' shape=(1, 512) dtype=float32>, '833': <tf.Tensor 'Relu_364:0' shape=(1, 512) dtype=float32>, '834': <tf.Tensor 'add_44:0' shape=(1, 1) dtype=float32>, 'output_gender': <tf.Tensor 'Sigmoid_366:0' shape=(1, 1) dtype=float32>, 'output_age': <tf.Tensor 'add_45:0' shape=(1, 1) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "import onnx\n",
    "\n",
    "ONNX_PATH = '../models/age_gender_misnet.onnx'\n",
    "\n",
    "onnx_model = onnx.load(ONNX_PATH)\n",
    "\n",
    "tf_rep = prepare(onnx_model, logging_level ='DEBUG')\n",
    "\n",
    "# Input nodes to the model\n",
    "print('inputs:', tf_rep.inputs)\n",
    "\n",
    "# Output nodes from the model\n",
    "print('outputs:', tf_rep.outputs)\n",
    "\n",
    "# All nodes in the model\n",
    "print('tensor_dict:')\n",
    "print(tf_rep.tensor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age [[25.087465]], Gender [[0.3627599]] as \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# prepare input data\n",
    "meanR,meanG,meanB = 0.54568475 ,0.42776844 ,0.3761094\n",
    "stdR,stdG,stdB = 0.21924357, 0.18996198, 0.17315607\n",
    "\n",
    "def get_val_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(mean=(meanR,meanG,meanB), std=(stdR,stdG,stdB)),\n",
    "    ])\n",
    "\n",
    "df = pd.read_csv(\"/home/Data/all/testing.csv\")\n",
    "img = cv2.imread(df['file_name'][500], cv2.IMREAD_COLOR)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype('float32')\n",
    "\n",
    "img /= 255.0\n",
    "\n",
    "img = get_val_transforms()(image = img)['image']\n",
    "\n",
    "output = tf_rep.run(np.transpose(np.asarray(img, dtype=np.float32)[np.newaxis ,: , :, :], (0, 3, 1, 2)))\n",
    "print('Age {}, Gender {} as '.format(output[0], output[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rep.export_graph('../models/agegender.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/agegender.pb -> ../models/age_gender_mixnet_v1.tflite\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "<unknown>:0: error: loc(\"BatchNormalization_363/mul_1\"): 'tfl.fully_connected' op expect 2d filter, got 'tensor<1x512x1536xf32>'\n<unknown>:0: note: loc(\"BatchNormalization_363/mul_1\"): see current operation: %7165 = \"tfl.fully_connected\"(%7164, %cst_6786, %cst_6787) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<1x1536xf32>, tensor<1x512x1536xf32>, tensor<1x512xf32>) -> tensor<1x512xf32>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m       model_str = wrap_toco.wrapped_toco_convert(model_flags_str,\n\u001b[0m\u001b[1;32m    197\u001b[0m                                                  \u001b[0mtoco_flags_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py\u001b[0m in \u001b[0;36mwrapped_toco_convert\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;34m\"\"\"Wraps TocoConvert with lazy loader.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   return _pywrap_toco_api.TocoConvert(\n\u001b[0m\u001b[1;32m     33\u001b[0m       \u001b[0mmodel_flags_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: <unknown>:0: error: loc(\"BatchNormalization_363/mul_1\"): 'tfl.fully_connected' op expect 2d filter, got 'tensor<1x512x1536xf32>'\n<unknown>:0: note: loc(\"BatchNormalization_363/mul_1\"): see current operation: %7165 = \"tfl.fully_connected\"(%7164, %cst_6786, %cst_6787) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<1x1536xf32>, tensor<1x512x1536xf32>, tensor<1x512xf32>) -> tensor<1x512xf32>\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2b05dd419890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtflite_quant_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFLITE_PATH_V1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mNone\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m     \"\"\"\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_tf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lite.TocoConverter\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[0;31m# Converts model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m       result = _toco_convert_impl(\n\u001b[0m\u001b[1;32m   1327\u001b[0m           \u001b[0minput_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimized_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m           \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m       input_tensors, output_tensors, *args, **kwargs)\n\u001b[1;32m    568\u001b[0m   \u001b[0mdebug_info_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdebug_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdebug_info\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m   data = toco_convert_protos(\n\u001b[0m\u001b[1;32m    570\u001b[0m       \u001b[0mmodel_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m       \u001b[0mtoco_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/age_gender/lib/python3.8/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_toco_from_proto_bin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConverterError\u001b[0m: <unknown>:0: error: loc(\"BatchNormalization_363/mul_1\"): 'tfl.fully_connected' op expect 2d filter, got 'tensor<1x512x1536xf32>'\n<unknown>:0: note: loc(\"BatchNormalization_363/mul_1\"): see current operation: %7165 = \"tfl.fully_connected\"(%7164, %cst_6786, %cst_6787) {fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"} : (tensor<1x1536xf32>, tensor<1x512x1536xf32>, tensor<1x512xf32>) -> tensor<1x512xf32>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "TFLITE_PATH_V1 = '../models/age_gender_mixnet_v1.tflite'\n",
    "\n",
    "graph_file = '../models/agegender.pb'\n",
    "\n",
    "input_array = ['input'] # you need to change it based on your model\n",
    "output_array = ['output_age', 'output_gender'] # you need to change it based on your model\n",
    "print(\"{} -> {}\".format(graph_file, TFLITE_PATH_V1))\n",
    "\n",
    "converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(\n",
    "      graph_def_file=graph_file,\n",
    "      input_arrays=input_array,\n",
    "      output_arrays=output_array,\n",
    "#       input_shapes={'input' : [1, 3, 224, 224]}\n",
    ")\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.allow_custom_ops=True\n",
    "converter.experimental_new_converter =True\n",
    "\n",
    "# I had to explicitly state the ops\n",
    "converter.target_spec.supported_ops = [tf.compat.v1.lite.OpsSet.TFLITE_BUILTINS,\n",
    "                                       tf.compat.v1.lite.OpsSet.SELECT_TF_OPS]\n",
    "\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "with open(TFLITE_PATH_V1, 'wb') as f:\n",
    "    f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_PATH_V2 = '../models/age_gender_mixnet_v2.tflite'\n",
    "TFLITE_PATH_V3 = '../models/age_gender_mixnet_v3.tflite'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "age_gender",
   "language": "python",
   "name": "age_gender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
